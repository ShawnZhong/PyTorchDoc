
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>PyTorch master documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/katex-math.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"></script>
    <script type="text/javascript" src="_static/katex_autorenderer.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="pytorch-documentation">
<h1>PyTorch documentation<a class="headerlink" href="#pytorch-documentation" title="Permalink to this headline">¶</a></h1>
<p>PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.</p>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
<span id="document-torch"></span><div class="section" id="module-torch">
<span id="torch"></span><h2>torch<a class="headerlink" href="#module-torch" title="Permalink to this headline">¶</a></h2>
<p>The torch package contains data structures for multi-dimensional
tensors and mathematical operations over these are defined.
Additionally, it provides many utilities for efficient serializing of
Tensors and arbitrary types, and other useful utilities.</p>
<p>It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability &gt;= 3.0.</p>
<div class="section" id="tensors">
<h3>Tensors<a class="headerlink" href="#tensors" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.is_tensor">
<code class="descclassname">torch.</code><code class="descname">is_tensor</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.is_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if <cite>obj</cite> is a PyTorch tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>obj</strong> (<em>Object</em>) – Object to test</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.is_storage">
<code class="descclassname">torch.</code><code class="descname">is_storage</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.is_storage" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if <cite>obj</cite> is a PyTorch storage object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>obj</strong> (<em>Object</em>) – Object to test</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.is_floating_point">
<code class="descclassname">torch.</code><code class="descname">is_floating_point</code><span class="sig-paren">(</span><em>tensor) -&gt; (bool</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.is_floating_point" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the data type of <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> is a floating point data type i.e.,
one of <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the PyTorch tensor to test</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.set_default_dtype">
<code class="descclassname">torch.</code><code class="descname">set_default_dtype</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_default_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the default floating point dtype to <code class="xref py py-attr docutils literal notranslate"><span class="pre">d</span></code>. This type will be
used as default floating point type for type inference in
<a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a>.</p>
<p>The default floating point dtype is initially <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>d</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>) – the floating point dtype to make the default</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span>           <span class="c1"># initial default for floating point is torch.float32</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span>           <span class="c1"># a new floating point tensor</span>
<span class="go">torch.float64</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.get_default_dtype">
<code class="descclassname">torch.</code><code class="descname">get_default_dtype</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.dtype<a class="headerlink" href="#torch.get_default_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the current default floating point <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>  <span class="c1"># initial default for floating point is torch.float32</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>  <span class="c1"># default is now changed to torch.float64</span>
<span class="go">torch.float64</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_tensor_type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>  <span class="c1"># setting tensor type also affects this</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>  <span class="c1"># changed to torch.float32, the dtype for torch.FloatTensor</span>
<span class="go">torch.float32</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.set_default_tensor_type">
<code class="descclassname">torch.</code><code class="descname">set_default_tensor_type</code><span class="sig-paren">(</span><em>t</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_default_tensor_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the default <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> type to floating point tensor type
<a class="reference internal" href="#torch.t" title="torch.t"><code class="xref py py-attr docutils literal notranslate"><span class="pre">t</span></code></a>. This type will also be used as default floating point type for
type inference in <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a>.</p>
<p>The default floating point tensor type is initially <code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a><em> or </em><em>string</em>) – the floating point tensor type or its name</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span>    <span class="c1"># initial default for floating point is torch.float32</span>
<span class="go">torch.float32</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_tensor_type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">dtype</span>    <span class="c1"># a new floating point tensor</span>
<span class="go">torch.float64</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.numel">
<code class="descclassname">torch.</code><code class="descname">numel</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.numel" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the total number of elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">120</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">16</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.set_printoptions">
<code class="descclassname">torch.</code><code class="descname">set_printoptions</code><span class="sig-paren">(</span><em>precision=None</em>, <em>threshold=None</em>, <em>edgeitems=None</em>, <em>linewidth=None</em>, <em>profile=None</em>, <em>sci_mode=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_printoptions" title="Permalink to this definition">¶</a></dt>
<dd><p>Set options for printing. Items shamelessly taken from NumPy</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>precision</strong> – Number of digits of precision for floating point output
(default = 4).</p></li>
<li><p><strong>threshold</strong> – Total number of array elements which trigger summarization
rather than full <cite>repr</cite> (default = 1000).</p></li>
<li><p><strong>edgeitems</strong> – Number of array items in summary at beginning and end of
each dimension (default = 3).</p></li>
<li><p><strong>linewidth</strong> – The number of characters per line for the purpose of
inserting line breaks (default = 80). Thresholded matrices will
ignore this parameter.</p></li>
<li><p><strong>profile</strong> – Sane defaults for pretty printing. Can override with any of
the above options. (any one of <cite>default</cite>, <cite>short</cite>, <cite>full</cite>)</p></li>
<li><p><strong>sci_mode</strong> – Enable (True) or disable (False) scientific notation. If
None (default) is specified, the value is defined by <cite>_Formatter</cite></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.set_flush_denormal">
<code class="descclassname">torch.</code><code class="descname">set_flush_denormal</code><span class="sig-paren">(</span><em>mode</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.set_flush_denormal" title="Permalink to this definition">¶</a></dt>
<dd><p>Disables denormal floating numbers on CPU.</p>
<p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if your system supports flushing denormal numbers and it
successfully configures flush denormal mode.  <a class="reference internal" href="#torch.set_flush_denormal" title="torch.set_flush_denormal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_flush_denormal()</span></code></a>
is only supported on x86 architectures supporting SSE3.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – Controls whether to enable flush denormal mode or not</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_flush_denormal</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1e-323</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([ 0.], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_flush_denormal</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1e-323</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor(9.88131e-324 *</span>
<span class="go">       [ 1.0000], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<div class="section" id="creation-ops">
<span id="tensor-creation-ops"></span><h4>Creation Ops<a class="headerlink" href="#creation-ops" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Random sampling creation ops are listed under <a class="reference internal" href="#random-sampling"><span class="std std-ref">Random sampling</span></a> and
include:
<a class="reference internal" href="#torch.rand" title="torch.rand"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rand()</span></code></a>
<a class="reference internal" href="#torch.rand_like" title="torch.rand_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rand_like()</span></code></a>
<a class="reference internal" href="#torch.randn" title="torch.randn"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randn()</span></code></a>
<a class="reference internal" href="#torch.randn_like" title="torch.randn_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randn_like()</span></code></a>
<a class="reference internal" href="#torch.randint" title="torch.randint"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randint()</span></code></a>
<a class="reference internal" href="#torch.randint_like" title="torch.randint_like"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randint_like()</span></code></a>
<a class="reference internal" href="#torch.randperm" title="torch.randperm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randperm()</span></code></a>
You may also use <a class="reference internal" href="#torch.empty" title="torch.empty"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.empty()</span></code></a> with the <a class="reference internal" href="#inplace-random-sampling"><span class="std std-ref">In-place random sampling</span></a>
methods to create <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> s with values sampled from a broader
range of distributions.</p>
</div>
<dl class="function">
<dt id="torch.tensor">
<code class="descclassname">torch.</code><code class="descname">tensor</code><span class="sig-paren">(</span><em>data</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="docutils literal notranslate"><span class="pre">data</span></code> and want to avoid a copy, use <a class="reference internal" href="index.html#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code></a>
or <a class="reference internal" href="index.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.detach()</span></code></a>.
If you have a NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> and want to avoid a copy, use
<a class="reference internal" href="#torch.as_tensor" title="torch.as_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_tensor()</span></code></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When data is a tensor <cite>x</cite>, <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> reads out ‘the data’ from whatever it is passed,
and constructs a leaf variable. Therefore <code class="docutils literal notranslate"><span class="pre">torch.tensor(x)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach()</span></code>
and <code class="docutils literal notranslate"><span class="pre">torch.tensor(x,</span> <span class="pre">requires_grad=True)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach().requires_grad_(True)</span></code>.
The equivalents using <code class="docutils literal notranslate"><span class="pre">clone()</span></code> and <code class="docutils literal notranslate"><span class="pre">detach()</span></code> are recommended.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>array_like</em>) – Initial data for the tensor. Can be a list, tuple,
NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, scalar, and other types.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, infers data type from <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">5.2</span><span class="p">]])</span>
<span class="go">tensor([[ 0.1000,  1.2000],</span>
<span class="go">        [ 2.2000,  3.1000],</span>
<span class="go">        [ 4.9000,  5.2000]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Type inference on data</span>
<span class="go">tensor([ 0,  1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.11111</span><span class="p">,</span> <span class="mf">0.222222</span><span class="p">,</span> <span class="mf">0.3333333</span><span class="p">]],</span>
<span class="go">                 dtype=torch.float64,</span>
<span class="go">                 device=torch.device(&#39;cuda:0&#39;))  # creates a torch.cuda.DoubleTensor</span>
<span class="go">tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.14159</span><span class="p">)</span>  <span class="c1"># Create a scalar (zero-dimensional tensor)</span>
<span class="go">tensor(3.1416)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>  <span class="c1"># Create an empty tensor (of size (0,))</span>
<span class="go">tensor([])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sparse_coo_tensor">
<code class="descclassname">torch.</code><code class="descname">sparse_coo_tensor</code><span class="sig-paren">(</span><em>indices</em>, <em>values</em>, <em>size=None</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.sparse_coo_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code>
with the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code>. A sparse tensor can be <cite>uncoalesced</cite>, in that case, there are duplicate
coordinates in the indices, and the value at that index is the sum of all duplicate value entries:
<a class="reference external" href="https://pytorch.org/docs/stable/sparse.html">torch.sparse</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> (<em>array_like</em>) – Initial data for the tensor. Can be a list, tuple,
NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, scalar, and other types. Will be cast to a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>
internally. The indices are the coordinates of the non-zero values in the matrix, and thus
should be two-dimensional where the first dimension is the number of tensor dimensions and
the second dimension is the number of non-zero values.</p></li>
<li><p><strong>values</strong> (<em>array_like</em>) – Initial values for the tensor. Can be a list, tuple,
NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, scalar, and other types.</p></li>
<li><p><strong>size</strong> (list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code>, optional) – Size of the sparse tensor. If not
provided the size will be inferred as the minimum size big enough to hold all non-zero
elements.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if None, infers data type from <code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">                      [2, 0, 2]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="go">tensor(indices=tensor([[0, 1, 1],</span>
<span class="go">                       [2, 0, 2]]),</span>
<span class="go">       values=tensor([3., 4., 5.]),</span>
<span class="go">       size=(2, 4), nnz=3, layout=torch.sparse_coo)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># Shape inference</span>
<span class="go">tensor(indices=tensor([[0, 1, 1],</span>
<span class="go">                       [2, 0, 2]]),</span>
<span class="go">       values=tensor([3., 4., 5.]),</span>
<span class="go">       size=(2, 3), nnz=3, layout=torch.sparse_coo)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="go">                            dtype=torch.float64,</span>
<span class="go">                            device=torch.device(&#39;cuda:0&#39;))</span>
<span class="go">tensor(indices=tensor([[0, 1, 1],</span>
<span class="go">                       [2, 0, 2]]),</span>
<span class="go">       values=tensor([3., 4., 5.]),</span>
<span class="go">       device=&#39;cuda:0&#39;, size=(2, 4), nnz=3, dtype=torch.float64,</span>
<span class="go">       layout=torch.sparse_coo)</span>

<span class="go"># Create an empty sparse tensor with the following invariants:</span>
<span class="go">#   1. sparse_dim + dense_dim = len(SparseTensor.shape)</span>
<span class="go">#   2. SparseTensor._indices().shape = (sparse_dim, nnz)</span>
<span class="go">#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])</span>
<span class="go">#</span>
<span class="go"># For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and</span>
<span class="go"># sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="p">[],</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="go">tensor(indices=tensor([], size=(1, 0)),</span>
<span class="go">       values=tensor([], size=(0,)),</span>
<span class="go">       size=(1,), nnz=0, layout=torch.sparse_coo)</span>

<span class="go"># and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and</span>
<span class="go"># sparse_dim = 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">tensor(indices=tensor([], size=(1, 0)),</span>
<span class="go">       values=tensor([], size=(0, 2)),</span>
<span class="go">       size=(1, 2), nnz=0, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.as_tensor">
<code class="descclassname">torch.</code><code class="descname">as_tensor</code><span class="sig-paren">(</span><em>data</em>, <em>dtype=None</em>, <em>device=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.as_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the data into a <cite>torch.Tensor</cite>. If the data is already a <cite>Tensor</cite> with the same <cite>dtype</cite> and <cite>device</cite>,
no copy will be performed, otherwise a new <cite>Tensor</cite> will be returned with computational graph retained if data
<cite>Tensor</cite> has <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>. Similarly, if the data is an <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> of the corresponding <cite>dtype</cite> and
the <cite>device</cite> is the cpu, no copy will be performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>array_like</em>) – Initial data for the tensor. Can be a list, tuple,
NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, scalar, and other types.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, infers data type from <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span>
<span class="go">tensor([ 1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">array([-1,  2,  3])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span>
<span class="go">tensor([ 1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">array([1,  2,  3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.from_numpy">
<code class="descclassname">torch.</code><code class="descname">from_numpy</code><span class="sig-paren">(</span><em>ndarray</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.from_numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> from a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.16)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.</p>
<p>The returned tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">ndarray</span></code> share the same memory. Modifications to
the tensor will be reflected in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ndarray</span></code> and vice versa. The returned
tensor is not resizable.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span>
<span class="go">tensor([ 1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">array([-1,  2,  3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.zeros">
<code class="descclassname">torch.</code><code class="descname">zeros</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with the scalar value <cite>0</cite>, with the shape defined
by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sizes</strong> (<em>int...</em>) – a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 0.,  0.,  0.,  0.,  0.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.zeros_like">
<code class="descclassname">torch.</code><code class="descname">zeros_like</code><span class="sig-paren">(</span><em>input</em>, <em>dtype=None</em>, <em>layout=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.zeros_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with the scalar value <cite>0</cite>, with the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. <code class="docutils literal notranslate"><span class="pre">torch.zeros_like(input)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.zeros(input.size(),</span> <span class="pre">dtype=input.dtype,</span> <span class="pre">layout=input.layout,</span> <span class="pre">device=input.device)</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As of 0.4, this function does not support an <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> keyword. As an alternative,
the old <code class="docutils literal notranslate"><span class="pre">torch.zeros_like(input,</span> <span class="pre">out=output)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.zeros(input.size(),</span> <span class="pre">out=output)</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will determine size of the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned Tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the dtype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the device of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ones">
<code class="descclassname">torch.</code><code class="descname">ones</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the shape defined
by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sizes</strong> (<em>int...</em>) – a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  1.,  1.],</span>
<span class="go">        [ 1.,  1.,  1.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 1.,  1.,  1.,  1.,  1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ones_like">
<code class="descclassname">torch.</code><code class="descname">ones_like</code><span class="sig-paren">(</span><em>input</em>, <em>dtype=None</em>, <em>layout=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.ones_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. <code class="docutils literal notranslate"><span class="pre">torch.ones_like(input)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.ones(input.size(),</span> <span class="pre">dtype=input.dtype,</span> <span class="pre">layout=input.layout,</span> <span class="pre">device=input.device)</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>As of 0.4, this function does not support an <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> keyword. As an alternative,
the old <code class="docutils literal notranslate"><span class="pre">torch.ones_like(input,</span> <span class="pre">out=output)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.ones(input.size(),</span> <span class="pre">out=output)</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will determine size of the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned Tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the dtype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the device of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  1.,  1.],</span>
<span class="go">        [ 1.,  1.,  1.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.arange">
<code class="descclassname">torch.</code><code class="descname">arange</code><span class="sig-paren">(</span><em>start=0</em>, <em>end</em>, <em>step=1</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.arange" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a 1-D tensor of size <span class="math">\(\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor\)</span>
with values from the interval <code class="docutils literal notranslate"><span class="pre">[start,</span> <span class="pre">end)</span></code> taken with common difference
<code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code> beginning from <cite>start</cite>.</p>
<p>Note that non-integer <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code> is subject to floating point rounding errors when
comparing against <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code>; to avoid inconsistency, we advise adding a small epsilon to <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code>
in such cases.</p>
<div class="math">
\[\text{out}_{{i+1}} = \text{out}_{i} + \text{step}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<em>Number</em>) – the starting value for the set of points. Default: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>end</strong> (<em>Number</em>) – the ending value for the set of points</p></li>
<li><p><strong>step</strong> (<em>Number</em>) – the gap between each pair of adjacent points. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). If <cite>dtype</cite> is not given, infer the data type from the other input
arguments. If any of <cite>start</cite>, <cite>end</cite>, or <cite>stop</cite> are floating-point, the
<cite>dtype</cite> is inferred to be the default dtype, see
<a class="reference internal" href="#torch.get_default_dtype" title="torch.get_default_dtype"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_default_dtype()</span></code></a>. Otherwise, the <cite>dtype</cite> is inferred to
be <cite>torch.int64</cite>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 0,  1,  2,  3,  4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([ 1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="go">tensor([ 1.0000,  1.5000,  2.0000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.range">
<code class="descclassname">torch.</code><code class="descname">range</code><span class="sig-paren">(</span><em>start=0</em>, <em>end</em>, <em>step=1</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.range" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a 1-D tensor of size <span class="math">\(\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1\)</span>
with values from <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code> with step <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code>. Step is
the gap between two values in the tensor.</p>
<div class="math">
\[\text{out}_{i+1} = \text{out}_i + \text{step}.

\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is deprecated in favor of <a class="reference internal" href="#torch.arange" title="torch.arange"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.arange()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the starting value for the set of points. Default: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>end</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the ending value for the set of points</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the gap between each pair of adjacent points. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="go">tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.linspace">
<code class="descclassname">torch.</code><code class="descname">linspace</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>steps=100</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.linspace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a one-dimensional tensor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">steps</span></code>
equally spaced points between <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code>.</p>
<p>The output tensor is 1-D of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">steps</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the starting value for the set of points</p></li>
<li><p><strong>end</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the ending value for the set of points</p></li>
<li><p><strong>steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of points to sample between <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([-10.,  -5.,   0.,   5.,  10.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([-10.,  -5.,   0.,   5.,  10.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([-10.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.logspace">
<code class="descclassname">torch.</code><code class="descname">logspace</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>steps=100</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.logspace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a one-dimensional tensor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">steps</span></code> points
logarithmically spaced between <span class="math">\(10^{\text{start}}\)</span> and <span class="math">\(10^{\text{end}}\)</span>.</p>
<p>The output tensor is 1-D of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">steps</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the starting value for the set of points</p></li>
<li><p><strong>end</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the ending value for the set of points</p></li>
<li><p><strong>steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of points to sample between <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([1.2589])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.eye">
<code class="descclassname">torch.</code><code class="descname">eye</code><span class="sig-paren">(</span><em>n</em>, <em>m=None</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.eye" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the number of rows</p></li>
<li><p><strong>m</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the number of columns with default being <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code></p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 2-D tensor with ones on the diagonal and zeros elsewhere</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  0.,  0.],</span>
<span class="go">        [ 0.,  1.,  0.],</span>
<span class="go">        [ 0.,  0.,  1.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.empty">
<code class="descclassname">torch.</code><code class="descname">empty</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with uninitialized data. The shape of the tensor is
defined by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sizes</strong> (<em>int...</em>) – a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor(1.00000e-08 *</span>
<span class="go">       [[ 6.3984,  0.0000,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.empty_like">
<code class="descclassname">torch.</code><code class="descname">empty_like</code><span class="sig-paren">(</span><em>input</em>, <em>dtype=None</em>, <em>layout=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.empty_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an uninitialized tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.
<code class="docutils literal notranslate"><span class="pre">torch.empty_like(input)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.empty(input.size(),</span> <span class="pre">dtype=input.dtype,</span> <span class="pre">layout=input.layout,</span> <span class="pre">device=input.device)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will determine size of the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned Tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the dtype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the device of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="go">tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],</span>
<span class="go">        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.full">
<code class="descclassname">torch.</code><code class="descname">full</code><span class="sig-paren">(</span><em>size</em>, <em>fill_value</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.full" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int...</em>) – a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the
shape of the output tensor.</p></li>
<li><p><strong>fill_value</strong> – the number to fill the output tensor with.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mf">3.141592</span><span class="p">)</span>
<span class="go">tensor([[ 3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.full_like">
<code class="descclassname">torch.</code><code class="descname">full_like</code><span class="sig-paren">(</span><em>input</em>, <em>fill_value</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.full_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.
<code class="docutils literal notranslate"><span class="pre">torch.full_like(input,</span> <span class="pre">fill_value)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.full(input.size(),</span> <span class="pre">fill_value,</span> <span class="pre">dtype=input.dtype,</span> <span class="pre">layout=input.layout,</span> <span class="pre">device=input.device)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will determine size of the output tensor</p></li>
<li><p><strong>fill_value</strong> – the number to fill the output tensor with.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned Tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the dtype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the device of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="indexing-slicing-joining-mutating-ops">
<h4>Indexing, Slicing, Joining, Mutating Ops<a class="headerlink" href="#indexing-slicing-joining-mutating-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.cat">
<code class="descclassname">torch.</code><code class="descname">cat</code><span class="sig-paren">(</span><em>tensors</em>, <em>dim=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.cat" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates the given sequence of <code class="xref py py-attr docutils literal notranslate"><span class="pre">seq</span></code> tensors in the given dimension.
All tensors must either have the same shape (except in the concatenating
dimension) or be empty.</p>
<p><a class="reference internal" href="#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cat()</span></code></a> can be seen as an inverse operation for <a class="reference internal" href="#torch.split" title="torch.split"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></a>
and <a class="reference internal" href="#torch.chunk" title="torch.chunk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.chunk()</span></code></a>.</p>
<p><a class="reference internal" href="#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cat()</span></code></a> can be best understood via examples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> (<em>sequence of Tensors</em>) – any python sequence of tensors of the same type.
Non-empty tensors provided must have the same shape, except in the
cat dimension.</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the dimension over which the tensors are concatenated</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.6580, -1.0969, -0.4614],</span>
<span class="go">        [-0.1034, -0.5790,  0.1497]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[ 0.6580, -1.0969, -0.4614],</span>
<span class="go">        [-0.1034, -0.5790,  0.1497],</span>
<span class="go">        [ 0.6580, -1.0969, -0.4614],</span>
<span class="go">        [-0.1034, -0.5790,  0.1497],</span>
<span class="go">        [ 0.6580, -1.0969, -0.4614],</span>
<span class="go">        [-0.1034, -0.5790,  0.1497]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,</span>
<span class="go">         -1.0969, -0.4614],</span>
<span class="go">        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,</span>
<span class="go">         -0.5790,  0.1497]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.chunk">
<code class="descclassname">torch.</code><code class="descname">chunk</code><span class="sig-paren">(</span><em>tensor</em>, <em>chunks</em>, <em>dim=0</em><span class="sig-paren">)</span> &#x2192; List of Tensors<a class="headerlink" href="#torch.chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits a tensor into a specific number of chunks.</p>
<p>Last chunk will be smaller if the tensor size along the given dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is not divisible by <code class="xref py py-attr docutils literal notranslate"><span class="pre">chunks</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to split</p></li>
<li><p><strong>chunks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of chunks to return</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension along which to split the tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.gather">
<code class="descclassname">torch.</code><code class="descname">gather</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>index</em>, <em>out=None</em>, <em>sparse_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers values along an axis specified by <cite>dim</cite>.</p>
<p>For a 3-D tensor the output is specified by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is an n-dimensional tensor with size
<span class="math">\((x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})\)</span>
and <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">=</span> <span class="pre">i</span></code>, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> must be an <span class="math">\(n\)</span>-dimensional tensor with
size <span class="math">\((x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})\)</span> where <span class="math">\(y \geq 1\)</span>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will have the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to gather</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the destination tensor</p></li>
<li><p><strong>sparse_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be a sparse tensor.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  1],</span>
<span class="go">        [ 4,  3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.index_select">
<code class="descclassname">torch.</code><code class="descname">index_select</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>index</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.index_select" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor which indexes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> using the entries in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> which is a <cite>LongTensor</cite>.</p>
<p>The returned tensor has the same number of dimensions as the original tensor
(<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>).  The <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>th dimension has the same size as the length
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>; other dimensions have the same size as in the original tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned tensor does <strong>not</strong> use the same storage as the original
tensor.  If <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> has a different shape than expected, we
silently change it to the correct shape, reallocating the underlying
storage if necessary.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension in which we index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the 1-D tensor containing the indices to index</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],</span>
<span class="go">        [-0.4664,  0.2647, -0.1228, -1.1068],</span>
<span class="go">        [-1.1734, -0.6571,  0.7230, -0.6004]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],</span>
<span class="go">        [-1.1734, -0.6571,  0.7230, -0.6004]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[ 0.1427, -0.5414],</span>
<span class="go">        [-0.4664, -0.1228],</span>
<span class="go">        [-1.1734,  0.7230]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.masked_select">
<code class="descclassname">torch.</code><code class="descname">masked_select</code><span class="sig-paren">(</span><em>input</em>, <em>mask</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.masked_select" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new 1-D tensor which indexes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor according to
the binary mask <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> which is a <cite>ByteTensor</cite>.</p>
<p>The shapes of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> tensor and the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor don’t need
to match, but they must be <span class="xref std std-ref">broadcastable</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned tensor does <strong>not</strong> use the same storage
as the original tensor</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input data</p></li>
<li><p><strong>mask</strong> (<a class="reference internal" href="index.html#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – the tensor containing the binary mask to index with</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],</span>
<span class="go">        [-1.2035,  1.2252,  0.5002,  0.6248],</span>
<span class="go">        [ 0.1307, -2.0608,  0.1244,  2.0139]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span>
<span class="go">tensor([[ 0,  0,  0,  0],</span>
<span class="go">        [ 0,  1,  1,  1],</span>
<span class="go">        [ 0,  0,  0,  1]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="go">tensor([ 1.2252,  0.5002,  0.6248,  2.0139])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.narrow">
<code class="descclassname">torch.</code><code class="descname">narrow</code><span class="sig-paren">(</span><em>input</em>, <em>dimension</em>, <em>start</em>, <em>length</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.narrow" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor that is a narrowed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor. The
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is input from <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span> <span class="pre">+</span> <span class="pre">length</span></code>. The
returned tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor share the same underlying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to narrow</p></li>
<li><p><strong>dimension</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension along which to narrow</p></li>
<li><p><strong>start</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the starting dimension</p></li>
<li><p><strong>length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the distance to the ending dimension</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 2,  3],</span>
<span class="go">        [ 5,  6],</span>
<span class="go">        [ 8,  9]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nonzero">
<code class="descclassname">torch.</code><code class="descname">nonzero</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.nonzero" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor containing the indices of all non-zero elements of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.  Each row in the result contains the indices of a non-zero
element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> has <cite>n</cite> dimensions, then the resulting indices tensor
<code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is of size <span class="math">\((z \times n)\)</span>, where <span class="math">\(z\)</span> is the total number of
non-zero elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) – the output tensor containing indices</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">tensor([[ 0],</span>
<span class="go">        [ 1],</span>
<span class="go">        [ 2],</span>
<span class="go">        [ 4]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
<span class="go">                                [0.0, 0.4, 0.0, 0.0],</span>
<span class="go">                                [0.0, 0.0, 1.2, 0.0],</span>
<span class="go">                                [0.0, 0.0, 0.0,-0.4]]))</span>
<span class="go">tensor([[ 0,  0],</span>
<span class="go">        [ 1,  1],</span>
<span class="go">        [ 2,  2],</span>
<span class="go">        [ 3,  3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.reshape">
<code class="descclassname">torch.</code><code class="descname">reshape</code><span class="sig-paren">(</span><em>input</em>, <em>shape</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>,
but with the specified shape. When possible, the returned tensor will be a view
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. Otherwise, it will be a copy. Contiguous inputs and inputs
with compatible strides can be reshaped without copying, but you should not
depend on the copying vs. viewing behavior.</p>
<p>See <a class="reference internal" href="index.html#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code></a> on when it is possible to return a view.</p>
<p>A single dimension may be -1, in which case it’s inferred from the remaining
dimensions and the number of elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be reshaped</p></li>
<li><p><strong>shape</strong> (<em>tuple of python:ints</em>) – the new shape</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">4.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">tensor([[ 0.,  1.],</span>
<span class="go">        [ 2.,  3.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
<span class="go">tensor([ 0,  1,  2,  3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.split">
<code class="descclassname">torch.</code><code class="descname">split</code><span class="sig-paren">(</span><em>tensor</em>, <em>split_size_or_sections</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits the tensor into chunks.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">split_size_or_sections</span></code> is an integer type, then <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> will
be split into equally sized chunks (if possible). Last chunk will be smaller if
the tensor size along the given dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is not divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">split_size</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">split_size_or_sections</span></code> is a list, then <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> will be split
into <code class="docutils literal notranslate"><span class="pre">len(split_size_or_sections)</span></code> chunks with sizes in <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> according
to <code class="xref py py-attr docutils literal notranslate"><span class="pre">split_size_or_sections</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor to split.</p></li>
<li><p><strong>split_size_or_sections</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>) or </em><em>(</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>)</em>) – size of a single chunk or
list of sizes for each chunk</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension along which to split the tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.squeeze">
<code class="descclassname">torch.</code><code class="descname">squeeze</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with all the dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of size <cite>1</cite> removed.</p>
<p>For example, if <cite>input</cite> is of shape:
<span class="math">\((A \times 1 \times B \times C \times 1 \times D)\)</span> then the <cite>out</cite> tensor
will be of shape: <span class="math">\((A \times B \times C \times D)\)</span>.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is given, a squeeze operation is done only in the given
dimension. If <cite>input</cite> is of shape: <span class="math">\((A \times 1 \times B)\)</span>,
<code class="docutils literal notranslate"><span class="pre">squeeze(input,</span> <span class="pre">0)</span></code> leaves the tensor unchanged, but <code class="docutils literal notranslate"><span class="pre">squeeze(input,</span> <span class="pre">1)</span></code>
will squeeze the tensor to the shape <span class="math">\((A \times B)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned tensor shares the storage with the input tensor,
so changing the contents of one will change the contents of the other.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – if given, the input will be squeezed only in
this dimension</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 1, 2, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 2, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 1, 2, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 2, 1, 2])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.stack">
<code class="descclassname">torch.</code><code class="descname">stack</code><span class="sig-paren">(</span><em>seq</em>, <em>dim=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates sequence of tensors along a new dimension.</p>
<p>All tensors need to be of the same size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seq</strong> (<em>sequence of Tensors</em>) – sequence of tensors to concatenate</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension to insert. Has to be between 0 and the number
of dimensions of concatenated tensors (inclusive)</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.t">
<code class="descclassname">torch.</code><code class="descname">t</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.t" title="Permalink to this definition">¶</a></dt>
<dd><p>Expects <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to be &lt;= 2-D tensor and transposes dimensions 0
and 1.</p>
<p>0-D and 1-D tensors are returned as it is and
2-D tensor can be seen as a short-hand function for <code class="docutils literal notranslate"><span class="pre">transpose(input,</span> <span class="pre">0,</span> <span class="pre">1)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor(0.1995)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor(0.1995)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([ 2.4320, -0.4608,  0.7702])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([.2.4320,.-0.4608,..0.7702])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.4875,  0.9158, -0.5872],</span>
<span class="go">        [ 0.3938, -0.6929,  0.6932]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 0.4875,  0.3938],</span>
<span class="go">        [ 0.9158, -0.6929],</span>
<span class="go">        [-0.5872,  0.6932]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.take">
<code class="descclassname">torch.</code><code class="descname">take</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.take" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> at the given indices.
The input tensor is treated as if it were viewed as a 1-D tensor. The result
takes the same shape as the indices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>indices</strong> (<em>LongTensor</em>) – the indices into tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="go">                        [6, 7, 8]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="go">tensor([ 4,  5,  8])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.transpose">
<code class="descclassname">torch.</code><code class="descname">transpose</code><span class="sig-paren">(</span><em>input</em>, <em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor that is a transposed version of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.
The given dimensions <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim0</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> are swapped.</p>
<p>The resulting <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> tensor shares it’s underlying storage with the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor, so changing the content of one would change the content
of the other.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim0</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the first dimension to be transposed</p></li>
<li><p><strong>dim1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the second dimension to be transposed</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1.0028, -0.9893,  0.5809],</span>
<span class="go">        [-0.1669,  0.7299,  0.4942]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1.0028, -0.1669],</span>
<span class="go">        [-0.9893,  0.7299],</span>
<span class="go">        [ 0.5809,  0.4942]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.unbind">
<code class="descclassname">torch.</code><code class="descname">unbind</code><span class="sig-paren">(</span><em>tensor</em>, <em>dim=0</em><span class="sig-paren">)</span> &#x2192; seq<a class="headerlink" href="#torch.unbind" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes a tensor dimension.</p>
<p>Returns a tuple of all slices along a given dimension, already without it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to unbind</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension to remove</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                           <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                           <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]))</span>
<span class="go">(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.unsqueeze">
<code class="descclassname">torch.</code><code class="descname">unsqueeze</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with a dimension of size one inserted at the
specified position.</p>
<p>The returned tensor shares the same underlying data with this tensor.</p>
<p>A <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> value within the range <code class="docutils literal notranslate"><span class="pre">[-input.dim()</span> <span class="pre">-</span> <span class="pre">1,</span> <span class="pre">input.dim()</span> <span class="pre">+</span> <span class="pre">1)</span></code>
can be used. Negative <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> will correspond to <a class="reference internal" href="#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a>
applied at <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> = <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">+</span> <span class="pre">input.dim()</span> <span class="pre">+</span> <span class="pre">1</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the index at which to insert the singleton dimension</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3,  4]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1],</span>
<span class="go">        [ 2],</span>
<span class="go">        [ 3],</span>
<span class="go">        [ 4]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.where">
<code class="descclassname">torch.</code><code class="descname">where</code><span class="sig-paren">(</span><em>condition</em>, <em>x</em>, <em>y</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.where" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a tensor of elements selected from either <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> or <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code>, depending on <code class="xref py py-attr docutils literal notranslate"><span class="pre">condition</span></code>.</p>
<p>The operation is defined as:</p>
<div class="math">
\[out_i = \begin{cases}
    x_i & \text{if } \text{condition}_i \\
    y_i & \text{otherwise} \\
\end{cases}

\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The tensors <code class="xref py py-attr docutils literal notranslate"><span class="pre">condition</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code> must be <span class="xref std std-ref">broadcastable</span>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>condition</strong> (<a class="reference internal" href="index.html#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – When True (nonzero), yield x, otherwise yield y</p></li>
<li><p><strong>x</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – values selected at indices where <code class="xref py py-attr docutils literal notranslate"><span class="pre">condition</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>y</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – values selected at indices where <code class="xref py py-attr docutils literal notranslate"><span class="pre">condition</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape equal to the broadcasted shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">condition</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[-0.4620,  0.3139],</span>
<span class="go">        [ 0.3898, -0.7197],</span>
<span class="go">        [ 0.0478, -0.1657]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">tensor([[ 1.0000,  0.3139],</span>
<span class="go">        [ 0.3898,  1.0000],</span>
<span class="go">        [ 0.0478,  1.0000]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="random-sampling">
<span id="id1"></span><h3>Random sampling<a class="headerlink" href="#random-sampling" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.manual_seed">
<code class="descclassname">torch.</code><code class="descname">manual_seed</code><span class="sig-paren">(</span><em>seed</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.manual_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers. Returns a
<cite>torch._C.Generator</cite> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The desired seed.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.initial_seed">
<code class="descclassname">torch.</code><code class="descname">initial_seed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.initial_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the initial seed for generating random numbers as a
Python <cite>long</cite>.</p>
</dd></dl>

<dl class="function">
<dt id="torch.get_rng_state">
<code class="descclassname">torch.</code><code class="descname">get_rng_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.get_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the random number generator state as a <cite>torch.ByteTensor</cite>.</p>
</dd></dl>

<dl class="function">
<dt id="torch.set_rng_state">
<code class="descclassname">torch.</code><code class="descname">set_rng_state</code><span class="sig-paren">(</span><em>new_state</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_state</strong> (<a class="reference internal" href="index.html#torch.ByteTensor" title="torch.ByteTensor"><em>torch.ByteTensor</em></a>) – The desired state</p>
</dd>
</dl>
</dd></dl>

<dl class="data">
<dt id="torch.default_generator">
<code class="descclassname">torch.</code><code class="descname">default_generator</code><em class="property"> = &lt;torch._C.Generator object&gt;</em><a class="headerlink" href="#torch.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.bernoulli">
<code class="descclassname">torch.</code><code class="descname">bernoulli</code><span class="sig-paren">(</span><em>input</em>, <em>*</em>, <em>generator=None</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.bernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor should be a tensor containing probabilities
to be used for drawing the binary random number.
Hence, all values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> have to be in the range:
<span class="math">\(0 \leq \text{input}_i \leq 1\)</span>.</p>
<p>The <span class="math">\(\text{i}^{th}\)</span> element of the output tensor will draw a
value <span class="math">\(1\)</span> according to the <span class="math">\(\text{i}^{th}\)</span> probability value given
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})

\]</div>
<p>The returned <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> tensor only has values 0 or 1 and is of the same
shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> can have integral <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, but <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> must have floating
point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of probability values for the Bernoulli distribution</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># generate a uniform random matrix with range [0, 1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.1737,  0.0950,  0.3609],</span>
<span class="go">        [ 0.7148,  0.0289,  0.2676],</span>
<span class="go">        [ 0.9456,  0.8937,  0.7202]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 1.,  1.,  1.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># probability of drawing &quot;1&quot; is 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  1.,  1.],</span>
<span class="go">        [ 1.,  1.,  1.],</span>
<span class="go">        [ 1.,  1.,  1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># probability of drawing &quot;1&quot; is 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.multinomial">
<code class="descclassname">torch.</code><code class="descname">multinomial</code><span class="sig-paren">(</span><em>input</em>, <em>num_samples</em>, <em>replacement=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor where each row contains <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_samples</span></code> indices sampled
from the multinomial probability distribution located in the corresponding row
of tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The rows of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> do not need to sum to one (in which case we use
the values as weights), but must be non-negative, finite and have
a non-zero sum.</p>
</div>
<p>Indices are ordered from left to right according to when each was sampled
(first samples are placed in first column).</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is a vector of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_samples</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a matrix with <cite>m</cite> rows, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is an matrix of shape
<span class="math">\((m \times \text{num\_samples})\)</span>.</p>
<p>If replacement is <code class="docutils literal notranslate"><span class="pre">True</span></code>, samples are drawn with replacement.</p>
<p>If not, they are drawn without replacement, which means that when a
sample index is drawn for a row, it cannot be drawn again for that row.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When drawn without replacement, <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_samples</span></code> must be lower than
number of non-zero elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (or the min number of non-zero
elements in each row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> if it is a matrix).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor containing probabilities</p></li>
<li><p><strong>num_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of samples to draw</p></li>
<li><p><strong>replacement</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to draw with replacement or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="c1"># create a tensor of weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="c1"># ERROR!</span>
<span class="go">RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,</span>
<span class="go">not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([ 2,  1,  1,  1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.normal">
<code class="descclassname">torch.</code><code class="descname">normal</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.normal" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>mean</em>, <em>std</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns a tensor of random numbers drawn from separate normal distributions
whose mean and standard deviation are given.</p>
<p>The <a class="reference internal" href="#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> is a tensor with the mean of
each output element’s normal distribution</p>
<p>The <a class="reference internal" href="#torch.std" title="torch.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a> is a tensor with the standard deviation of
each output element’s normal distribution</p>
<p>The shapes of <a class="reference internal" href="#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="#torch.std" title="torch.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a> don’t need to match, but the
total number of elements in each tensor need to be the same.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the shapes do not match, the shape of <a class="reference internal" href="#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a>
is used as the shape for the returned output tensor</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mean</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor of per-element means</p></li>
<li><p><strong>std</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor of per-element standard deviations</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">),</span> <span class="n">std</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">))</span>
<span class="go">tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,</span>
<span class="go">          8.0505,   8.1408,   9.0563,  10.0566])</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>mean=0.0</em>, <em>std</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Similar to the function above, but the means are shared among all drawn
elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mean</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – the mean for all distributions</p></li>
<li><p><strong>std</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor of per-element standard deviations</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">))</span>
<span class="go">tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>mean</em>, <em>std=1.0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Similar to the function above, but the standard-deviations are shared among
all drawn elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mean</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor of per-element means</p></li>
<li><p><strong>std</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – the standard deviation for all distributions</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">))</span>
<span class="go">tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.rand">
<code class="descclassname">torch.</code><code class="descname">rand</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.rand" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with random numbers from a uniform distribution
on the interval <span class="math">\([0, 1)\)</span></p>
<p>The shape of the tensor is defined by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sizes</strong> (<em>int...</em>) – a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([ 0.5204,  0.2503,  0.3525,  0.5673])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 0.8237,  0.5781,  0.6879],</span>
<span class="go">        [ 0.3816,  0.7249,  0.0998]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.rand_like">
<code class="descclassname">torch.</code><code class="descname">rand_like</code><span class="sig-paren">(</span><em>input</em>, <em>dtype=None</em>, <em>layout=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.rand_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> that is filled with
random numbers from a uniform distribution on the interval <span class="math">\([0, 1)\)</span>.
<code class="docutils literal notranslate"><span class="pre">torch.rand_like(input)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.rand(input.size(),</span> <span class="pre">dtype=input.dtype,</span> <span class="pre">layout=input.layout,</span> <span class="pre">device=input.device)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will determine size of the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned Tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the dtype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the device of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.randint">
<code class="descclassname">torch.</code><code class="descname">randint</code><span class="sig-paren">(</span><em>low=0</em>, <em>high</em>, <em>size</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.randint" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with random integers generated uniformly
between <code class="xref py py-attr docutils literal notranslate"><span class="pre">low</span></code> (inclusive) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">high</span></code> (exclusive).</p>
<p>The shape of the tensor is defined by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>low</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Lowest integer to be drawn from the distribution. Default: 0.</p></li>
<li><p><strong>high</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – One above the highest integer to be drawn from the distribution.</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – a tuple defining the shape of the output tensor.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="go">tensor([4, 3, 4])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">tensor([[0, 2],</span>
<span class="go">        [5, 5]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">tensor([[4, 5],</span>
<span class="go">        [6, 7]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.randint_like">
<code class="descclassname">torch.</code><code class="descname">randint_like</code><span class="sig-paren">(</span><em>input</em>, <em>low=0</em>, <em>high</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.randint_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with the same shape as Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> filled with
random integers generated uniformly between <code class="xref py py-attr docutils literal notranslate"><span class="pre">low</span></code> (inclusive) and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">high</span></code> (exclusive).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will determine size of the output tensor</p></li>
<li><p><strong>low</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Lowest integer to be drawn from the distribution. Default: 0.</p></li>
<li><p><strong>high</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – One above the highest integer to be drawn from the distribution.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned Tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the dtype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the device of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.randn">
<code class="descclassname">torch.</code><code class="descname">randn</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.randn" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with random numbers from a normal distribution
with mean <cite>0</cite> and variance <cite>1</cite> (also called the standard normal
distribution).</p>
<div class="math">
\[\text{out}_{i} \sim \mathcal{N}(0, 1)

\]</div>
<p>The shape of the tensor is defined by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sizes</strong> (<em>int...</em>) – a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>).</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([-2.1436,  0.9966,  2.3426, -0.6366])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 1.5954,  2.8929, -1.0923],</span>
<span class="go">        [ 1.1719, -0.4709, -0.1996]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.randn_like">
<code class="descclassname">torch.</code><code class="descname">randn_like</code><span class="sig-paren">(</span><em>input</em>, <em>dtype=None</em>, <em>layout=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.randn_like" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> that is filled with
random numbers from a normal distribution with mean 0 and variance 1.
<code class="docutils literal notranslate"><span class="pre">torch.randn_like(input)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.randn(input.size(),</span> <span class="pre">dtype=input.dtype,</span> <span class="pre">layout=input.layout,</span> <span class="pre">device=input.device)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will determine size of the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned Tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the dtype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the layout of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, defaults to the device of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.randperm">
<code class="descclassname">torch.</code><code class="descname">randperm</code><span class="sig-paren">(</span><em>n</em>, <em>out=None</em>, <em>dtype=torch.int64</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.randperm" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a random permutation of integers from <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">-</span> <span class="pre">1</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the upper bound (exclusive)</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.int64</span></code>.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned Tensor.
Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([2, 1, 0, 3])</span>
</pre></div>
</div>
</dd></dl>

<div class="section" id="in-place-random-sampling">
<span id="inplace-random-sampling"></span><h4>In-place random sampling<a class="headerlink" href="#in-place-random-sampling" title="Permalink to this headline">¶</a></h4>
<p>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</p>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.bernoulli_()</span></code></a> - in-place version of <a class="reference internal" href="#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p></li>
<li><p><a class="reference internal" href="index.html#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.cauchy_()</span></code></a> - numbers drawn from the Cauchy distribution</p></li>
<li><p><a class="reference internal" href="index.html#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.exponential_()</span></code></a> - numbers drawn from the exponential distribution</p></li>
<li><p><a class="reference internal" href="index.html#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.geometric_()</span></code></a> - elements drawn from the geometric distribution</p></li>
<li><p><a class="reference internal" href="index.html#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.log_normal_()</span></code></a> - samples from the log-normal distribution</p></li>
<li><p><a class="reference internal" href="index.html#torch.Tensor.normal_" title="torch.Tensor.normal_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.normal_()</span></code></a> - in-place version of <a class="reference internal" href="#torch.normal" title="torch.normal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.normal()</span></code></a></p></li>
<li><p><a class="reference internal" href="index.html#torch.Tensor.random_" title="torch.Tensor.random_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.random_()</span></code></a> - numbers sampled from the discrete uniform distribution</p></li>
<li><p><a class="reference internal" href="index.html#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.uniform_()</span></code></a> - numbers sampled from the continuous uniform distribution</p></li>
</ul>
</div>
</div>
<div class="section" id="serialization">
<h3>Serialization<a class="headerlink" href="#serialization" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.save">
<code class="descclassname">torch.</code><code class="descname">save</code><span class="sig-paren">(</span><em>obj</em>, <em>f</em>, <em>pickle_module=&lt;module 'pickle' from '/Users/Shawn/SDKs/miniconda3/lib/python3.7/pickle.py'&gt;</em>, <em>pickle_protocol=2</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves an object to a disk file.</p>
<p>See also: <span class="xref std std-ref">recommend-saving-models</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>obj</strong> – saved object</p></li>
<li><p><strong>f</strong> – a file-like object (has to implement write and flush) or a string
containing a file name</p></li>
<li><p><strong>pickle_module</strong> – module used for pickling metadata and objects</p></li>
<li><p><strong>pickle_protocol</strong> – can be specified to override the default protocol</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you are using Python 2, torch.save does NOT support StringIO.StringIO
as a valid file-like object. This is because the write method should return
the number of bytes written; StringIO.write() does not do this.</p>
<p>Please use something like io.BytesIO instead.</p>
</div>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Save to file</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;tensor.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Save to io.BytesIO buffer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">buffer</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.load">
<code class="descclassname">torch.</code><code class="descname">load</code><span class="sig-paren">(</span><em>f</em>, <em>map_location=None</em>, <em>pickle_module=&lt;module 'pickle' from '/Users/Shawn/SDKs/miniconda3/lib/python3.7/pickle.py'&gt;</em>, <em>**pickle_load_args</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads an object saved with <a class="reference internal" href="#torch.save" title="torch.save"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.save()</span></code></a> from a file.</p>
<p><a class="reference internal" href="#torch.load" title="torch.load"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.load()</span></code></a> uses Python’s unpickling facilities but treats storages,
which underlie tensors, specially. They are first deserialized on the
CPU and are then moved to the device they were saved from. If this fails
(e.g. because the run time system doesn’t have certain devices), an exception
is raised. However, storages can be dynamically remapped to an alternative
set of devices using the <cite>map_location</cite> argument.</p>
<p>If <cite>map_location</cite> is a callable, it will be called once for each serialized
storage with two arguments: storage and location. The storage argument
will be the initial deserialization of the storage, residing on the CPU.
Each serialized storage has a location tag associated with it which
identifies the device it was saved from, and this tag is the second
argument passed to map_location. The builtin location tags are <cite>‘cpu’</cite> for
CPU tensors and <cite>‘cuda:device_id’</cite> (e.g. <cite>‘cuda:2’</cite>) for CUDA tensors.
<cite>map_location</cite> should return either None or a storage. If <cite>map_location</cite> returns
a storage, it will be used as the final deserialized object, already moved to
the right device. Otherwise, <span class="math">\(torch.load\)</span> will fall back to the default
behavior, as if <cite>map_location</cite> wasn’t specified.</p>
<p>If <cite>map_location</cite> is a string, it should be a device tag, where all tensors
should be loaded.</p>
<p>Otherwise, if <cite>map_location</cite> is a dict, it will be used to remap location tags
appearing in the file (keys), to ones that specify where to put the
storages (values).</p>
<p>User extensions can register their own location tags and tagging and
deserialization methods using <cite>register_package</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>f</strong> – a file-like object (has to implement read, readline, tell, and seek),
or a string containing a file name</p></li>
<li><p><strong>map_location</strong> – a function, torch.device, string or a dict specifying how to remap storage
locations</p></li>
<li><p><strong>pickle_module</strong> – module used for unpickling metadata and objects (has to
match the pickle_module used to serialize file)</p></li>
<li><p><strong>pickle_load_args</strong> – optional keyword arguments passed over to
<code class="docutils literal notranslate"><span class="pre">pickle_module.load</span></code> and <code class="docutils literal notranslate"><span class="pre">pickle_module.Unpickler</span></code>, e.g.,
<code class="docutils literal notranslate"><span class="pre">encoding=...</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When you call <a class="reference internal" href="#torch.load" title="torch.load"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.load()</span></code></a> on a file which contains GPU tensors, those tensors
will be loaded to GPU by default. You can call <cite>torch.load(.., map_location=’cpu’)</cite>
and then <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code> to avoid GPU RAM surge when loading a model checkpoint.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In Python 3, when loading files saved by Python 2, you may encounter
<code class="docutils literal notranslate"><span class="pre">UnicodeDecodeError:</span> <span class="pre">'ascii'</span> <span class="pre">codec</span> <span class="pre">can't</span> <span class="pre">decode</span> <span class="pre">byte</span> <span class="pre">0x...</span></code>. This is
caused by the difference of handling in byte strings in Python2 and
Python 3. You may use extra <code class="docutils literal notranslate"><span class="pre">encoding</span></code> keyword argument to specify how
these objects should be loaded, e.g., <code class="docutils literal notranslate"><span class="pre">encoding='latin1'</span></code> decodes them
to strings using <code class="docutils literal notranslate"><span class="pre">latin1</span></code> encoding, and <code class="docutils literal notranslate"><span class="pre">encoding='bytes'</span></code> keeps them
as byte arrays which can be decoded later with <code class="docutils literal notranslate"><span class="pre">byte_array.decode(...)</span></code>.</p>
</div>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>
<span class="go"># Load all tensors onto the CPU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span>
<span class="go"># Load all tensors onto the CPU, using a function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="k">lambda</span> <span class="n">storage</span><span class="p">,</span> <span class="n">loc</span><span class="p">:</span> <span class="n">storage</span><span class="p">)</span>
<span class="go"># Load all tensors onto GPU 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="k">lambda</span> <span class="n">storage</span><span class="p">,</span> <span class="n">loc</span><span class="p">:</span> <span class="n">storage</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="go"># Map tensors from GPU 1 to GPU 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">})</span>
<span class="go"># Load tensor from io.BytesIO object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;tensor.pt&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="go">        buffer = io.BytesIO(f.read())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="parallelism">
<h3>Parallelism<a class="headerlink" href="#parallelism" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.get_num_threads">
<code class="descclassname">torch.</code><code class="descname">get_num_threads</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.get_num_threads" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the number of OpenMP threads used for parallelizing CPU operations</p>
</dd></dl>

<dl class="function">
<dt id="torch.set_num_threads">
<code class="descclassname">torch.</code><code class="descname">set_num_threads</code><span class="sig-paren">(</span><em>int</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_num_threads" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the number of OpenMP threads used for parallelizing CPU operations</p>
</dd></dl>

</div>
<div class="section" id="locally-disabling-gradient-computation">
<h3>Locally disabling gradient computation<a class="headerlink" href="#locally-disabling-gradient-computation" title="Permalink to this headline">¶</a></h3>
<p>The context managers <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.no_grad()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.enable_grad()</span></code>, and
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_grad_enabled()</span></code> are helpful for locally disabling and enabling
gradient computation. See <a class="reference internal" href="index.html#locally-disable-grad"><span class="std std-ref">Locally disabling gradient computation</span></a> for more details on
their usage.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># this can also be used as a function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
</pre></div>
</div>
</div>
<div class="section" id="math-operations">
<h3>Math operations<a class="headerlink" href="#math-operations" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pointwise-ops">
<h4>Pointwise Ops<a class="headerlink" href="#pointwise-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.abs">
<code class="descclassname">torch.</code><code class="descname">abs</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the element-wise absolute value of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<div class="math">
\[\text{out}_{i} = |\text{input}_{i}|

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">tensor([ 1,  2,  3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.acos">
<code class="descclassname">torch.</code><code class="descname">acos</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the arccosine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \cos^{-1}(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.3348, -0.5889,  0.2005, -0.1584])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">acos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 1.2294,  2.2004,  1.3690,  1.7298])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.add">
<code class="descclassname">torch.</code><code class="descname">add</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.add" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">add</code><span class="sig-paren">(</span><em>input</em>, <em>value</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Adds the scalar <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> to each element of the input <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
and returns a new resulting tensor.</p>
<div class="math">
\[\text{out} = \text{input} + \text{value}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is of type FloatTensor or DoubleTensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> must be
a real number, otherwise it should be an integer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>value</strong> (<em>Number</em>) – the number to be added to each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments</dt>
<dd class="field-even"><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.0202,  1.0985,  1.3506, -0.6056])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="go">tensor([ 20.0202,  21.0985,  21.3506,  19.3944])</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">add</code><span class="sig-paren">(</span><em>input</em>, <em>value=1</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Each element of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is multiplied by the scalar
<code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and added to each element of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.
The resulting tensor is returned.</p>
<p>The shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<span class="xref std std-ref">broadcastable</span>.</p>
<div class="math">
\[\text{out} = \text{input} + \text{value} \times \text{other}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is of type FloatTensor or DoubleTensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> must be
a real number, otherwise it should be an integer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first input tensor</p></li>
<li><p><strong>value</strong> (<em>Number</em>) – the scalar multiplier for <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code></p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments</dt>
<dd class="field-even"><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.9732, -0.3497,  0.6245,  0.4022])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([[ 0.3743],</span>
<span class="go">        [-1.7724],</span>
<span class="go">        [-0.5811],</span>
<span class="go">        [-0.8017]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">tensor([[  2.7695,   3.3930,   4.3672,   4.1450],</span>
<span class="go">        [-18.6971, -18.0736, -17.0994, -17.3216],</span>
<span class="go">        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],</span>
<span class="go">        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addcdiv">
<code class="descclassname">torch.</code><code class="descname">addcdiv</code><span class="sig-paren">(</span><em>tensor</em>, <em>value=1</em>, <em>tensor1</em>, <em>tensor2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.addcdiv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the element-wise division of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code>,
multiply the result by the scalar <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and add it to <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a>.</p>
<div class="math">
\[\text{out}_i = \text{tensor}_i + \text{value} \times \frac{\text{tensor1}_i}{\text{tensor2}_i}

\]</div>
<p>The shapes of <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code>, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code> must be
<span class="xref std std-ref">broadcastable</span>.</p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> must be
a real number, otherwise an integer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be added</p></li>
<li><p><strong>value</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <span class="math">\(\text{tensor1} / \text{tensor2}\)</span></p></li>
<li><p><strong>tensor1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the numerator tensor</p></li>
<li><p><strong>tensor2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the denominator tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span>
<span class="go">tensor([[-0.2312, -3.6496,  0.1312],</span>
<span class="go">        [-1.0428,  3.4292, -0.1030],</span>
<span class="go">        [-0.5369, -0.9829,  0.0430]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addcmul">
<code class="descclassname">torch.</code><code class="descname">addcmul</code><span class="sig-paren">(</span><em>tensor</em>, <em>value=1</em>, <em>tensor1</em>, <em>tensor2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.addcmul" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the element-wise multiplication of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code>
by <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code>, multiply the result by the scalar <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code>
and add it to <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a>.</p>
<div class="math">
\[\text{out}_i = \text{tensor}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i

\]</div>
<p>The shapes of <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code>, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code> must be
<span class="xref std std-ref">broadcastable</span>.</p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> must be
a real number, otherwise an integer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be added</p></li>
<li><p><strong>value</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <span class="math">\(tensor1 .* tensor2\)</span></p></li>
<li><p><strong>tensor1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be multiplied</p></li>
<li><p><strong>tensor2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span>
<span class="go">tensor([[-0.8635, -0.6391,  1.6174],</span>
<span class="go">        [-0.7617, -0.5879,  1.7388],</span>
<span class="go">        [-0.8353, -0.6249,  1.6511]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.asin">
<code class="descclassname">torch.</code><code class="descname">asin</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the arcsine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \sin^{-1}(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.5962,  1.4985, -0.4396,  1.4525])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">asin</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-0.6387,     nan, -0.4552,     nan])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.atan">
<code class="descclassname">torch.</code><code class="descname">atan</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the arctangent  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \tan^{-1}(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.2341,  0.2539, -0.6256, -0.6448])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 0.2299,  0.2487, -0.5591, -0.5727])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.atan2">
<code class="descclassname">torch.</code><code class="descname">atan2</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the arctangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input1</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">input2</span></code>.</p>
<p>The shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">input2</span></code> must be
<span class="xref std std-ref">broadcastable</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first input tensor</p></li>
<li><p><strong>input2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.9041,  0.0196, -0.3108, -2.4423])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">atan2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="go">tensor([ 0.9833,  0.0811, -1.9743, -1.4151])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ceil">
<code class="descclassname">torch.</code><code class="descname">ceil</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the ceil of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>,
the smallest integer greater than or equal to each element.</p>
<div class="math">
\[\text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil = \left\lfloor \text{input}_{i} \right\rfloor + 1

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.6341, -1.4208, -1.0900,  0.5826])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-0., -1., -1.,  1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.clamp">
<code class="descclassname">torch.</code><code class="descname">clamp</code><span class="sig-paren">(</span><em>input</em>, <em>min</em>, <em>max</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.clamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Clamp all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into the range <cite>[</cite> <a class="reference internal" href="#torch.min" title="torch.min"><code class="xref py py-attr docutils literal notranslate"><span class="pre">min</span></code></a>, <a class="reference internal" href="#torch.max" title="torch.max"><code class="xref py py-attr docutils literal notranslate"><span class="pre">max</span></code></a> <cite>]</cite> and return
a resulting tensor:</p>
<div class="math">
\[y_i = \begin{cases}
    \text{min} & \text{if } x_i < \text{min} \\
    x_i & \text{if } \text{min} \leq x_i \leq \text{max} \\
    \text{max} & \text{if } x_i > \text{max}
\end{cases}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, args <a class="reference internal" href="#torch.min" title="torch.min"><code class="xref py py-attr docutils literal notranslate"><span class="pre">min</span></code></a>
and <a class="reference internal" href="#torch.max" title="torch.max"><code class="xref py py-attr docutils literal notranslate"><span class="pre">max</span></code></a> must be real numbers, otherwise they should be integers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>min</strong> (<em>Number</em>) – lower-bound of the range to be clamped to</p></li>
<li><p><strong>max</strong> (<em>Number</em>) – upper-bound of the range to be clamped to</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-1.7120,  0.1734, -0.0478, -0.0922])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">tensor([-0.5000,  0.1734, -0.0478, -0.0922])</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">clamp</code><span class="sig-paren">(</span><em>input</em>, <em>*</em>, <em>min</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Clamps all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to be larger or equal <a class="reference internal" href="#torch.min" title="torch.min"><code class="xref py py-attr docutils literal notranslate"><span class="pre">min</span></code></a>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code>
should be a real number, otherwise it should be an integer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>value</strong> (<em>Number</em>) – minimal value of each element in the output</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.0299, -2.3184,  2.1593, -0.8883])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">tensor([ 0.5000,  0.5000,  2.1593,  0.5000])</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">clamp</code><span class="sig-paren">(</span><em>input</em>, <em>*</em>, <em>max</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Clamps all elements in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> to be smaller or equal <a class="reference internal" href="#torch.max" title="torch.max"><code class="xref py py-attr docutils literal notranslate"><span class="pre">max</span></code></a>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code>
should be a real number, otherwise it should be an integer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>value</strong> (<em>Number</em>) – maximal value of each element in the output</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.7753, -0.4702, -0.4599,  1.1899])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">tensor([ 0.5000, -0.4702, -0.4599,  0.5000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cos">
<code class="descclassname">torch.</code><code class="descname">cos</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the cosine  of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \cos(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 1.4309,  1.2706, -0.8562,  0.9796])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 0.1395,  0.2957,  0.6553,  0.5574])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cosh">
<code class="descclassname">torch.</code><code class="descname">cosh</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the hyperbolic cosine  of the elements of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \cosh(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.1632,  1.1835, -0.6979, -0.7325])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 1.0133,  1.7860,  1.2536,  1.2805])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.div">
<code class="descclassname">torch.</code><code class="descname">div</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.div" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">div</code><span class="sig-paren">(</span><em>input</em>, <em>value</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Divides each element of the input <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the scalar <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code>
and returns a new resulting tensor.</p>
<div class="math">
\[\text{out}_i = \frac{\text{input}_i}{\text{value}}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code>
should be a real number, otherwise it should be an integer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>value</strong> (<em>Number</em>) – the number to be divided to each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="go">tensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">div</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Each element of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is divided by each element
of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>. The resulting tensor is returned. The shapes of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<span class="xref std std-ref">broadcastable</span>.</p>
<div class="math">
\[\text{out}_i = \frac{\text{input}_i}{\text{other}_i}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the numerator tensor</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the denominator tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.3711, -1.9353, -0.4605, -0.2917],</span>
<span class="go">        [ 0.1815, -1.0111,  0.9805, -1.5923],</span>
<span class="go">        [ 0.1062,  1.4581,  0.7759, -1.2344],</span>
<span class="go">        [-0.1830, -0.0313,  1.1908, -1.4757]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([ 0.8032,  0.2930, -0.8113, -0.2308])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">tensor([[-0.4620, -6.6051,  0.5676,  1.2637],</span>
<span class="go">        [ 0.2260, -3.4507, -1.2086,  6.8988],</span>
<span class="go">        [ 0.1322,  4.9764, -0.9564,  5.3480],</span>
<span class="go">        [-0.2278, -0.1068, -1.4678,  6.3936]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.digamma">
<code class="descclassname">torch.</code><code class="descname">digamma</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.digamma" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the logarithmic derivative of the gamma function on <cite>input</cite>.</p>
<div class="math">
\[\psi(x) = \frac{d}{dx} \ln\left(\Gamma\left(x\right)\right) = \frac{\Gamma'(x)}{\Gamma(x)}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compute the digamma function on</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">digamma</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-0.5772, -1.9635])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.erf">
<code class="descclassname">torch.</code><code class="descname">erf</code><span class="sig-paren">(</span><em>tensor</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.erf" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the error function of each element. The error function is defined as follows:</p>
<div class="math">
\[\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">]))</span>
<span class="go">tensor([ 0.0000, -0.8427,  1.0000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.erfc">
<code class="descclassname">torch.</code><code class="descname">erfc</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.erfc" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the complementary error function of each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.
The complementary error function is defined as follows:</p>
<div class="math">
\[\mathrm{erfc}(x) = 1 - \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">erfc</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">]))</span>
<span class="go">tensor([ 1.0000, 1.8427,  0.0000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.erfinv">
<code class="descclassname">torch.</code><code class="descname">erfinv</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.erfinv" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the inverse error function of each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.
The inverse error function is defined in the range <span class="math">\((-1, 1)\)</span> as:</p>
<div class="math">
\[\mathrm{erfinv}(\mathrm{erf}(x)) = x

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">erfinv</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]))</span>
<span class="go">tensor([ 0.0000,  0.4769,    -inf])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.exp">
<code class="descclassname">torch.</code><code class="descname">exp</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the exponential of the elements
of the input tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[y_{i} = e^{x_{i}}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">2.</span><span class="p">)]))</span>
<span class="go">tensor([ 1.,  2.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.expm1">
<code class="descclassname">torch.</code><code class="descname">expm1</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.expm1" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the exponential of the elements minus 1
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[y_{i} = e^{x_{i}} - 1

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">2.</span><span class="p">)]))</span>
<span class="go">tensor([ 0.,  1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.floor">
<code class="descclassname">torch.</code><code class="descname">floor</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the floor of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>,
the largest integer less than or equal to each element.</p>
<div class="math">
\[\text{out}_{i} = \left\lfloor \text{input}_{i} \right\rfloor

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.8166,  1.5308, -0.2530, -0.2091])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-1.,  1., -1., -1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.fmod">
<code class="descclassname">torch.</code><code class="descname">fmod</code><span class="sig-paren">(</span><em>input</em>, <em>divisor</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.fmod" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the element-wise remainder of division.</p>
<p>The dividend and divisor may contain both for integer and floating point
numbers. The remainder has the same sign as the dividend <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">divisor</span></code> is a tensor, the shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">divisor</span></code> must be <span class="xref std std-ref">broadcastable</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the dividend</p></li>
<li><p><strong>divisor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the divisor, which may be either a number or a tensor of the same shape as the dividend</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([-1., -0., -1.,  1.,  0.,  1.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="go">tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.frac">
<code class="descclassname">torch.</code><code class="descname">frac</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.frac" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the fractional portion of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \text{input}_{i} - \left\lfloor \text{input}_{i} \right\rfloor

\]</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">frac</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]))</span>
<span class="go">tensor([ 0.0000,  0.5000, -0.2000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.lerp">
<code class="descclassname">torch.</code><code class="descname">lerp</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>weight</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.lerp" title="Permalink to this definition">¶</a></dt>
<dd><p>Does a linear interpolation of two tensors <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code> based
on a scalar or tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> and returns the resulting <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> tensor.</p>
<div class="math">
\[\text{out}_i = \text{start}_i + \text{weight}_i \times (\text{end}_i - \text{start}_i)

\]</div>
<p>The shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code> must be
<span class="xref std std-ref">broadcastable</span>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> is a tensor, then
the shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">start</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">end</span></code> must be <span class="xref std std-ref">broadcastable</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor with the starting points</p></li>
<li><p><strong>end</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor with the ending points</p></li>
<li><p><strong>weight</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>tensor</em>) – the weight for the interpolation formula</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">start</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">end</span>
<span class="go">tensor([ 10.,  10.,  10.,  10.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="go">tensor([ 5.5000,  6.0000,  6.5000,  7.0000])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="go">tensor([ 5.5000,  6.0000,  6.5000,  7.0000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.log">
<code class="descclassname">torch.</code><code class="descname">log</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the natural logarithm of the elements
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[y_{i} = \log_{e} (x_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ nan,  nan,  nan,  nan,  nan])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.log10">
<code class="descclassname">torch.</code><code class="descname">log10</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.log10" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the logarithm to the base 10 of the elements
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[y_{i} = \log_{10} (x_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.log1p">
<code class="descclassname">torch.</code><code class="descname">log1p</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the natural logarithm of (1 + <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>).</p>
<div class="math">
\[y_i = \log_{e} (x_i + 1)

\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is more accurate than <a class="reference internal" href="#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a> for small
values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.log2">
<code class="descclassname">torch.</code><code class="descname">log2</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.log2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the logarithm to the base 2 of the elements
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[y_{i} = \log_{2} (x_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mul">
<code class="descclassname">torch.</code><code class="descname">mul</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.mul" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">mul</code><span class="sig-paren">(</span><em>input</em>, <em>value</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Multiplies each element of the input <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the scalar
<code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and returns a new resulting tensor.</p>
<div class="math">
\[\text{out}_i = \text{value} \times \text{input}_i

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code>
should be a real number, otherwise it should be an integer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>value</strong> (<em>Number</em>) – the number to be multiplied to each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.2015, -0.4255,  2.6087])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="go">tensor([  20.1494,  -42.5491,  260.8663])</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">mul</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Each element of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is multiplied by the corresponding
element of the Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>. The resulting tensor is returned.</p>
<p>The shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<span class="xref std std-ref">broadcastable</span>.</p>
<div class="math">
\[\text{out}_i = \text{input}_i \times \text{other}_i

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first multiplicand tensor</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second multiplicand tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 1.1207],</span>
<span class="go">        [-0.3137],</span>
<span class="go">        [ 0.0700],</span>
<span class="go">        [ 0.8378]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],</span>
<span class="go">        [-0.1614, -0.0382,  0.1645, -0.7021],</span>
<span class="go">        [ 0.0360,  0.0085, -0.0367,  0.1567],</span>
<span class="go">        [ 0.4312,  0.1019, -0.4394,  1.8753]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mvlgamma">
<code class="descclassname">torch.</code><code class="descname">mvlgamma</code><span class="sig-paren">(</span><em>input</em>, <em>p</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.mvlgamma" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the multivariate log-gamma function (<a class="reference external" href="https://en.wikipedia.org/wiki/Multivariate_gamma_function">[reference]</a>) with dimension <span class="math">\(p\)</span> element-wise, given by</p>
<div class="math">
\[\log(\Gamma_{p}(a)) = C + \displaystyle \sum_{i=1}^{p} \log\left(\Gamma\left(a - \frac{i - 1}{2}\right)\right)

\]</div>
<p>where <span class="math">\(C = \log(\pi) \times \frac{p (p - 1)}{4}\)</span> and <span class="math">\(\Gamma(\cdot)\)</span> is the Gamma function.</p>
<p>If any of the elements are less than or equal to <span class="math">\(\frac{p - 1}{2}\)</span>, then an error
is thrown.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compute the multivariate log-gamma function</p></li>
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the number of dimensions</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[1.6835, 1.8474, 1.1929],</span>
<span class="go">        [1.0475, 1.7162, 1.4180]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mvlgamma</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[0.3928, 0.4007, 0.7586],</span>
<span class="go">        [1.0311, 0.3901, 0.5049]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.neg">
<code class="descclassname">torch.</code><code class="descname">neg</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.neg" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the negative of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out} = -1 \times \text{input}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.pow">
<code class="descclassname">torch.</code><code class="descname">pow</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.pow" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">pow</code><span class="sig-paren">(</span><em>input</em>, <em>exponent</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Takes the power of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> and
returns a tensor with the result.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> can be either a single <code class="docutils literal notranslate"><span class="pre">float</span></code> number or a <cite>Tensor</cite>
with the same number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> is a scalar value, the operation applied is:</p>
<div class="math">
\[\text{out}_i = x_i ^ \text{exponent}

\]</div>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> is a tensor, the operation applied is:</p>
<div class="math">
\[\text{out}_i = x_i ^ {\text{exponent}_i}

\]</div>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> is a tensor, the shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">exponent</span></code> must be <span class="xref std std-ref">broadcastable</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>exponent</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>tensor</em>) – the exponent value</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.4331,  1.2475,  0.6834, -0.2791])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([ 0.1875,  1.5561,  0.4670,  0.0779])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">exp</span><span class="p">)</span>
<span class="go">tensor([   1.,    4.,   27.,  256.])</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">pow</code><span class="sig-paren">(</span><em>base</em>, <em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">base</span></code> is a scalar <code class="docutils literal notranslate"><span class="pre">float</span></code> value, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a tensor.
The returned tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is of the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p>
<p>The operation applied is:</p>
<div class="math">
\[out_i = base ^ {input_i}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the scalar base value for the power operation</p></li>
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the exponent tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">base</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">exp</span><span class="p">)</span>
<span class="go">tensor([  2.,   4.,   8.,  16.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.reciprocal">
<code class="descclassname">torch.</code><code class="descname">reciprocal</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.reciprocal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the reciprocal of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code></p>
<div class="math">
\[\text{out}_{i} = \frac{1}{\text{input}_{i}}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.4595, -2.1219, -1.4314,  0.7298])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-2.1763, -0.4713, -0.6986,  1.3702])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.remainder">
<code class="descclassname">torch.</code><code class="descname">remainder</code><span class="sig-paren">(</span><em>input</em>, <em>divisor</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.remainder" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the element-wise remainder of division.</p>
<p>The divisor and dividend may contain both for integer and floating point
numbers. The remainder has the same sign as the divisor.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">divisor</span></code> is a tensor, the shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">divisor</span></code> must be <span class="xref std std-ref">broadcastable</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the dividend</p></li>
<li><p><strong>divisor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the divisor that may be either a number or a
Tensor of the same shape as the dividend</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([ 1.,  0.,  1.,  1.,  0.,  1.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="go">tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#torch.fmod" title="torch.fmod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmod()</span></code></a>, which computes the element-wise remainder of
division equivalently to the C library function <code class="docutils literal notranslate"><span class="pre">fmod()</span></code>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.round">
<code class="descclassname">torch.</code><code class="descname">round</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.round" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with each of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> rounded
to the closest integer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.9920,  0.6077,  0.9734, -1.0362])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 1.,  1.,  1., -1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.rsqrt">
<code class="descclassname">torch.</code><code class="descname">rsqrt</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.rsqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the reciprocal of the square-root of each of
the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \frac{1}{\sqrt{\text{input}_{i}}}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.0370,  0.2970,  1.5420, -0.9105])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([    nan,  1.8351,  0.8053,     nan])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sigmoid">
<code class="descclassname">torch.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the sigmoid of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \frac{1}{1 + e^{-\text{input}_{i}}}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.9213,  1.0887, -0.8858, -1.7683])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 0.7153,  0.7481,  0.2920,  0.1458])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sign">
<code class="descclassname">torch.</code><code class="descname">sign</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.sign" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the sign of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.7000, -1.2000,  0.0000,  2.3000])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 1., -1.,  0.,  1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sin">
<code class="descclassname">torch.</code><code class="descname">sin</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the sine of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \sin(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.5461,  0.1347, -2.7266, -0.2746])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-0.5194,  0.1343, -0.4032, -0.2711])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sinh">
<code class="descclassname">torch.</code><code class="descname">sinh</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the hyperbolic sine of the elements of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \sinh(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.5380, -0.8632, -0.1265,  0.9399])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 0.5644, -0.9744, -0.1268,  1.0845])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sqrt">
<code class="descclassname">torch.</code><code class="descname">sqrt</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the square-root of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \sqrt{\text{input}_{i}}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-2.0755,  1.0226,  0.0831,  0.4806])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([    nan,  1.0112,  0.2883,  0.6933])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.tan">
<code class="descclassname">torch.</code><code class="descname">tan</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.tan" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the tangent of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \tan(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-1.2027, -1.7687,  0.4412, -1.3856])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([-2.5930,  4.9859,  0.4722, -5.3366])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.tanh">
<code class="descclassname">torch.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the hyperbolic tangent of the elements
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="math">
\[\text{out}_{i} = \tanh(\text{input}_{i})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.8986, -0.7279,  1.1745,  0.2611])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 0.7156, -0.6218,  0.8257,  0.2553])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.trunc">
<code class="descclassname">torch.</code><code class="descname">trunc</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the truncated integer values of
the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 3.4742,  0.5466, -0.8008, -0.9079])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">trunc</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([ 3.,  0., -0., -0.])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="reduction-ops">
<h4>Reduction Ops<a class="headerlink" href="#reduction-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.argmax">
<code class="descclassname">torch.</code><code class="descname">argmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the indices of the maximum values of a tensor across a dimension.</p>
<p>This is the second value returned by <a class="reference internal" href="#torch.max" title="torch.max"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.max()</span></code></a>. See its
documentation for the exact semantics of this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the argmax of the
flattened input is returned.</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensors have <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>
retained or not. Ignored if <code class="docutils literal notranslate"><span class="pre">dim=None</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],</span>
<span class="go">        [-0.7401, -0.8805, -0.3402, -1.1936],</span>
<span class="go">        [ 0.4907, -1.3948, -1.0691, -0.3132],</span>
<span class="go">        [-1.6092,  0.5419, -0.2993,  0.3195]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 0,  2,  0,  1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.argmin">
<code class="descclassname">torch.</code><code class="descname">argmin</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.argmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the indices of the minimum values of a tensor across a dimension.</p>
<p>This is the second value returned by <a class="reference internal" href="#torch.min" title="torch.min"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.min()</span></code></a>. See its
documentation for the exact semantics of this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the argmin of the
flattened input is returned.</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensors have <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>
retained or not. Ignored if <code class="docutils literal notranslate"><span class="pre">dim=None</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],</span>
<span class="go">        [ 1.0100, -1.1975, -0.0102, -0.4732],</span>
<span class="go">        [-0.9240,  0.1207, -0.7506, -1.0213],</span>
<span class="go">        [ 1.7809, -1.2960,  0.9384,  0.1438]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 2,  1,  3,  1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cumprod">
<code class="descclassname">torch.</code><code class="descname">cumprod</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.cumprod" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the cumulative product of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p>
<p>For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector of size N, the result will also be
a vector of size N, with elements.</p>
<div class="math">
\[y_i = x_1 \times x_2\times x_3\times \dots \times x_i

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to do the operation over</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,</span>
<span class="go">        -0.2129, -0.4206,  0.1968])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,</span>
<span class="go">         0.0014, -0.0006, -0.0001])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,</span>
<span class="go">         0.0000, -0.0000, -0.0000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cumsum">
<code class="descclassname">torch.</code><code class="descname">cumsum</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.cumsum" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the cumulative sum of elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p>
<p>For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector of size N, the result will also be
a vector of size N, with elements.</p>
<div class="math">
\[y_i = x_1 + x_2 + x_3 + \dots + x_i

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to do the operation over</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,</span>
<span class="go">         0.1850, -1.1571, -0.4243])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,</span>
<span class="go">        -1.8209, -2.9780, -3.4022])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.dist">
<code class="descclassname">torch.</code><code class="descname">dist</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>p=2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the p-norm of (<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> - <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>)</p>
<p>The shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<span class="xref std std-ref">broadcastable</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the Right-hand-side input tensor</p></li>
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – the norm to be computed</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([-1.5393, -0.8675,  0.5916,  1.6321])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">tensor([ 0.0967, -1.0511,  0.6295,  0.8360])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="go">tensor(1.6727)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor(1.6973)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor(inf)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(2.6537)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.logsumexp">
<code class="descclassname">torch.</code><code class="descname">logsumexp</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.logsumexp" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the log of summed exponentials of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
tensor in the given dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. The computation is numerically
stabilized.</p>
<p>For summation index <span class="math">\(j\)</span> given by <cite>dim</cite> and other indices <span class="math">\(i\)</span>, the result is</p>
<blockquote>
<div><div class="math">
\[\text{logsumexp}(x)_{i} = \log \sum_j \exp(x_{ij})

\]</div>
</div></blockquote>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in the
output tensor having 1 (or <code class="docutils literal notranslate"><span class="pre">len(dim)</span></code>) fewer dimension(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of python:ints</em>) – the dimension or dimensions to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 0.8442,  1.4322,  0.8711])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.mean">
<code class="descclassname">torch.</code><code class="descname">mean</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.mean" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">mean</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the mean value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.2294, -0.5481,  1.3288]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(0.3367)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">mean</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the mean value of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is a list of dimensions,
reduce over all of them.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in the
output tensor having 1 (or <code class="docutils literal notranslate"><span class="pre">len(dim)</span></code>) fewer dimension(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of python:ints</em>) – the dimension or dimensions to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.3841,  0.6320,  0.4254, -0.7384],</span>
<span class="go">        [-0.9644,  1.0131, -0.6549, -1.4279],</span>
<span class="go">        [-0.2951, -1.3350, -0.7694,  0.5600],</span>
<span class="go">        [ 1.0842, -0.9580,  0.3623,  0.2343]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([-0.0163, -0.5085, -0.4599,  0.1807])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[-0.0163],</span>
<span class="go">        [-0.5085],</span>
<span class="go">        [-0.4599],</span>
<span class="go">        [ 0.1807]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.median">
<code class="descclassname">torch.</code><code class="descname">median</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.median" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">median</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the median value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 1.5219, -1.5212,  0.2202]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(0.2202)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">median</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em>, <em>keepdim=False</em>, <em>values=None</em>, <em>indices=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the median
value of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. And <code class="docutils literal notranslate"><span class="pre">indices</span></code> is the index location of each median value found.</p>
<p>By default, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is the last dimension of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensors are of the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where they are of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in
the outputs tensor having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensors have <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>values</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>indices</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output index tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],</span>
<span class="go">        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],</span>
<span class="go">        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],</span>
<span class="go">        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mode">
<code class="descclassname">torch.</code><code class="descname">mode</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em>, <em>keepdim=False</em>, <em>values=None</em>, <em>indices=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the mode
value of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>, i.e. a value which appears most often
in that row, and <code class="docutils literal notranslate"><span class="pre">indices</span></code> is the index location of each mode value found.</p>
<p>By default, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is the last dimension of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensors are of the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where they are of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting
in the output tensors having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is not defined for <code class="docutils literal notranslate"><span class="pre">torch.cuda.Tensor</span></code> yet.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensors have <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>values</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
<li><p><strong>indices</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output index tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([6, 5, 1, 0, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.norm">
<code class="descclassname">torch.</code><code class="descname">norm</code><span class="sig-paren">(</span><em>input</em>, <em>p='fro'</em>, <em>dim=None</em>, <em>keepdim=False</em>, <em>out=None</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the matrix norm or vector norm of a given tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>inf</em><em>, </em><em>-inf</em><em>, </em><em>'fro'</em><em>, </em><em>'nuc'</em><em>, </em><em>optional</em>) – <p>the order of norm. Default: <code class="docutils literal notranslate"><span class="pre">'fro'</span></code>
The following norms can be calculated:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 8%" />
<col style="width: 47%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>ord</p></th>
<th class="head"><p>matrix norm</p></th>
<th class="head"><p>vector norm</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>None</p></td>
<td><p>Frobenius norm</p></td>
<td><p>2-norm</p></td>
</tr>
<tr class="row-odd"><td><p>’fro’</p></td>
<td><p>Frobenius norm</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-even"><td><p>‘nuc’</p></td>
<td><p>nuclear norm</p></td>
<td><p>–</p></td>
</tr>
<tr class="row-odd"><td><p>Other</p></td>
<td><p>as vec norm when dim is None</p></td>
<td><p>sum(abs(x)**ord)**(1./ord)</p></td>
</tr>
</tbody>
</table>
</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>2-tuple of python:ints</em><em>, </em><em>2-list of python:ints</em><em>, </em><em>optional</em>) – If it is an int,
vector norm will be calculated, if it is 2-tuple of ints, matrix norm
will be calculated. If the value is None, matrix norm will be calculated
when the input tensor only has two dimensions, vector norm will be
calculated when the input tensor only has one dimension. If the input
tensor has more than two dimensions, the vector norm will be applied to
last dimension.</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether the output tensors have <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>
retained or not. Ignored if <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> = <code class="docutils literal notranslate"><span class="pre">None</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> = <code class="docutils literal notranslate"><span class="pre">None</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor. Ignored if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> = <code class="docutils literal notranslate"><span class="pre">None</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> = <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of
returned tensor. If specified, the input tensor is casted to
:attr:’dtype’ while performing the operation. Default: None.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="o">-</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(7.7460)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="go">tensor(7.7460)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
<span class="go">tensor(4.)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
<span class="go">tensor(4.)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span> <span class="p">,</span> <span class="n">dtype</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([1.4142, 2.2361, 5.0000])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([3.7417, 4.2426])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([6., 6.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="go">tensor([ 3.7417, 11.2250])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
<span class="go">(tensor(3.7417), tensor(11.2250))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.prod">
<code class="descclassname">torch.</code><code class="descname">prod</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.prod" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">prod</code><span class="sig-paren">(</span><em>input</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the product of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.8020,  0.5428, -1.5854]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(0.6902)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">prod</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the product of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in
the output tensor having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.5261, -0.3837],</span>
<span class="go">        [ 1.1857, -0.2498],</span>
<span class="go">        [-1.1646,  0.0705],</span>
<span class="go">        [ 1.1131, -1.0629]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([-0.2018, -0.2962, -0.0821, -1.1831])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.std">
<code class="descclassname">torch.</code><code class="descname">std</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.std" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">std</code><span class="sig-paren">(</span><em>input</em>, <em>unbiased=True</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the standard-deviation of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">unbiased</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the standard-deviation will be calculated
via the biased estimator. Otherwise, Bessel’s correction will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>unbiased</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to use the unbiased estimation or not</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.8166, -1.3802, -0.3560]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(0.5130)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">std</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>unbiased=True</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the standard-deviation of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is a list of dimensions,
reduce over all of them.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in the
output tensor having 1 (or <code class="docutils literal notranslate"><span class="pre">len(dim)</span></code>) fewer dimension(s).</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">unbiased</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the standard-deviation will be calculated
via the biased estimator. Otherwise, Bessel’s correction will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of python:ints</em>) – the dimension or dimensions to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>unbiased</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to use the unbiased estimation or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.2035,  1.2959,  1.8101, -0.4644],</span>
<span class="go">        [ 1.5027, -0.3270,  0.5905,  0.6538],</span>
<span class="go">        [-1.5745,  1.3330, -0.5596, -0.6548],</span>
<span class="go">        [ 0.1264, -0.5080,  1.6420,  0.1992]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 1.0311,  0.7477,  1.2204,  0.9087])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sum">
<code class="descclassname">torch.</code><code class="descname">sum</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sum" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">sum</code><span class="sig-paren">(</span><em>input</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the sum of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.1133, -0.9567,  0.2958]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(-0.5475)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">sum</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the sum of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is a list of dimensions,
reduce over all of them.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in the
output tensor having 1 (or <code class="docutils literal notranslate"><span class="pre">len(dim)</span></code>) fewer dimension(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of python:ints</em>) – the dimension or dimensions to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],</span>
<span class="go">        [-0.2993,  0.9138,  0.9337, -1.6864],</span>
<span class="go">        [ 0.1132,  0.7892, -0.1003,  0.5688],</span>
<span class="go">        [ 0.3637, -0.9906, -0.4752, -1.5197]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([-0.4598, -0.1381,  1.3708, -2.6217])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">tensor([  435.,  1335.,  2235.,  3135.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.unique">
<code class="descclassname">torch.</code><code class="descname">unique</code><span class="sig-paren">(</span><em>input</em>, <em>sorted=True</em>, <em>return_inverse=False</em>, <em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.unique" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the unique scalar elements of the input tensor as a 1-D tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>sorted</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – Whether to sort the unique elements in ascending order
before returning as output.</p></li>
<li><p><strong>return_inverse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – Whether to also return the indices for where
elements in the original input ended up in the returned unique list.</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to apply unique. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the unique of the
flattened input is returned. default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A tensor or a tuple of tensors containing</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>output</strong> (<em>Tensor</em>): the output list of unique scalar elements.</p></li>
<li><p><strong>inverse_indices</strong> (<em>Tensor</em>): (optional) if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">return_inverse</span></code> is True, there will be a
2nd returned tensor (same shape as input) representing the indices
for where elements in the original input map to in the output;
otherwise, this function will only return a single tensor.</p></li>
</ul>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a> (optional))</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([ 2,  3,  1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">inverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span>
<span class="go">        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([ 1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inverse_indices</span>
<span class="go">tensor([ 0,  2,  1,  2])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">inverse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span>
<span class="go">        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">tensor([ 1,  2,  3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inverse_indices</span>
<span class="go">tensor([[ 0,  2],</span>
<span class="go">        [ 1,  2]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.var">
<code class="descclassname">torch.</code><code class="descname">var</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.var" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">var</code><span class="sig-paren">(</span><em>input</em>, <em>unbiased=True</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the variance of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">unbiased</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the variance will be calculated via the
biased estimator. Otherwise, Bessel’s correction will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>unbiased</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to use the unbiased estimation or not</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.3425, -1.2636, -0.4864]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(0.2455)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">var</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>unbiased=True</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the variance of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension(s) <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in the
output tensor having 1 (or <code class="docutils literal notranslate"><span class="pre">len(dim)</span></code>) fewer dimension(s).</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">unbiased</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the variance will be calculated via the
biased estimator. Otherwise, Bessel’s correction will be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of python:ints</em>) – the dimension or dimensions to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>unbiased</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to use the unbiased estimation or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.3567,  1.7385, -1.3042,  0.7423],</span>
<span class="go">        [ 1.3436, -0.1015, -0.9834, -0.8438],</span>
<span class="go">        [ 0.6056,  0.1089, -0.3112, -1.4085],</span>
<span class="go">        [-0.7700,  0.6074, -0.1469,  0.7777]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 1.7444,  1.1363,  0.7356,  0.5112])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="comparison-ops">
<h4>Comparison Ops<a class="headerlink" href="#comparison-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.allclose">
<code class="descclassname">torch.</code><code class="descname">allclose</code><span class="sig-paren">(</span><em>self</em>, <em>other</em>, <em>rtol=1e-05</em>, <em>atol=1e-08</em>, <em>equal_nan=False</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.allclose" title="Permalink to this definition">¶</a></dt>
<dd><p>This function checks if all <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> satisfy the condition:</p>
<div class="math">
\[\lvert \text{self} - \text{other} \rvert \leq \texttt{atol} + \texttt{rtol} \times \lvert \text{other} \rvert

\]</div>
<p>elementwise, for all elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>. The behaviour of this function is analogous to
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html">numpy.allclose</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – first tensor to compare</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – second tensor to compare</p></li>
<li><p><strong>atol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – absolute tolerance. Default: 1e-08</p></li>
<li><p><strong>rtol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – relative tolerance. Default: 1e-05</p></li>
<li><p><strong>equal_nan</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, then two <code class="docutils literal notranslate"><span class="pre">NaN</span></code> s will be compared as equal. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10000.</span><span class="p">,</span> <span class="mf">1e-07</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10000.1</span><span class="p">,</span> <span class="mf">1e-08</span><span class="p">]))</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10000.</span><span class="p">,</span> <span class="mf">1e-08</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10000.1</span><span class="p">,</span> <span class="mf">1e-09</span><span class="p">]))</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)]))</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)]),</span> <span class="n">equal_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.argsort">
<code class="descclassname">torch.</code><code class="descname">argsort</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em>, <em>descending=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.argsort" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the indices that sort a tensor along a given dimension in ascending
order by value.</p>
<p>This is the second value returned by <a class="reference internal" href="#torch.sort" title="torch.sort"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.sort()</span></code></a>.  See its documentation
for the exact semantics of this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the dimension to sort along</p></li>
<li><p><strong>descending</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls the sorting order (ascending or descending)</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],</span>
<span class="go">        [ 0.1598,  0.0788, -0.0745, -1.2700],</span>
<span class="go">        [ 1.2208,  1.0722, -0.7064,  1.2564],</span>
<span class="go">        [ 0.0669, -0.2318, -0.8229, -0.9280]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[2, 0, 3, 1],</span>
<span class="go">        [3, 2, 1, 0],</span>
<span class="go">        [2, 1, 0, 3],</span>
<span class="go">        [3, 2, 1, 0]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.eq">
<code class="descclassname">torch.</code><code class="descname">eq</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.eq" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes element-wise equality</p>
<p>The second argument can be a number or a tensor whose shape is
<span class="xref std std-ref">broadcastable</span> with the first argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the tensor or value to compare</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor. Must be a <cite>ByteTensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  0],</span>
<span class="go">        [ 0,  1]], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.equal">
<code class="descclassname">torch.</code><code class="descname">equal</code><span class="sig-paren">(</span><em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.equal" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">True</span></code> if two tensors have the same size and elements, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ge">
<code class="descclassname">torch.</code><code class="descname">ge</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.ge" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <span class="math">\(\text{input} \geq \text{other}\)</span> element-wise.</p>
<p>The second argument can be a number or a tensor whose shape is
<span class="xref std std-ref">broadcastable</span> with the first argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the tensor or value to compare</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor that must be a <cite>ByteTensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  1],</span>
<span class="go">        [ 0,  1]], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.gt">
<code class="descclassname">torch.</code><code class="descname">gt</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.gt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <span class="math">\(\text{input} &gt; \text{other}\)</span> element-wise.</p>
<p>The second argument can be a number or a tensor whose shape is
<span class="xref std std-ref">broadcastable</span> with the first argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the tensor or value to compare</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor that must be a <cite>ByteTensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go">tensor([[ 0,  1],</span>
<span class="go">        [ 0,  0]], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.isfinite">
<code class="descclassname">torch.</code><code class="descname">isfinite</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.isfinite" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with boolean elements representing if each element is <cite>Finite</cite> or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – A tensor to check</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location of finite elements and 0 otherwise</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)]))</span>
<span class="go">tensor([ 1,  0,  1,  0,  0], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.isinf">
<code class="descclassname">torch.</code><code class="descname">isinf</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.isinf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with boolean elements representing if each element is <cite>+/-INF</cite> or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – A tensor to check</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location of <cite>+/-INF</cite> elements and 0 otherwise</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">)]))</span>
<span class="go">tensor([ 0,  1,  0,  1,  0], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.isnan">
<code class="descclassname">torch.</code><code class="descname">isnan</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.isnan" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with boolean elements representing if each element is <cite>NaN</cite> or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – A tensor to check</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location of <cite>NaN</cite> elements.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">),</span> <span class="mi">2</span><span class="p">]))</span>
<span class="go">tensor([ 0,  1,  0], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.kthvalue">
<code class="descclassname">torch.</code><code class="descname">kthvalue</code><span class="sig-paren">(</span><em>input</em>, <em>k</em>, <em>dim=None</em>, <em>keepdim=False</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.kthvalue" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the <code class="xref py py-attr docutils literal notranslate"><span class="pre">k</span></code> th
smallest element of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. And <code class="docutils literal notranslate"><span class="pre">indices</span></code> is the index location of each element found.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is not given, the last dimension of the <cite>input</cite> is chosen.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, both the <code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code> tensors
are the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where
they are of size 1. Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed
(see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in both the <code class="xref py py-attr docutils literal notranslate"><span class="pre">values</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code> tensors having 1 fewer dimension than the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>k</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – k for the k-th smallest element</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the dimension to find the kth value along</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensors have <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the output tuple of (Tensor, LongTensor)
can be optionally given to be used as output buffers</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.,  5.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">kthvalue</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span><span class="mf">7.</span><span class="p">)</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1.,  2.,  3.],</span>
<span class="go">        [ 4.,  5.,  6.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">kthvalue</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="go">torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.le">
<code class="descclassname">torch.</code><code class="descname">le</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.le" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <span class="math">\(\text{input} \leq \text{other}\)</span> element-wise.</p>
<p>The second argument can be a number or a tensor whose shape is
<span class="xref std std-ref">broadcastable</span> with the first argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the tensor or value to compare</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor that must be a <cite>ByteTensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  0],</span>
<span class="go">        [ 1,  1]], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.lt">
<code class="descclassname">torch.</code><code class="descname">lt</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.lt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <span class="math">\(\text{input} &lt; \text{other}\)</span> element-wise.</p>
<p>The second argument can be a number or a tensor whose shape is
<span class="xref std std-ref">broadcastable</span> with the first argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the tensor or value to compare</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor that must be a <cite>ByteTensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>torch.ByteTensor</cite> containing a 1 at each location where comparison is true</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go">tensor([[ 0,  0],</span>
<span class="go">        [ 1,  0]], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.max">
<code class="descclassname">torch.</code><code class="descname">max</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.max" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">max</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the maximum value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.6763,  0.7445, -2.2369]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(0.7445)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">max</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the maximum
value of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. And <code class="docutils literal notranslate"><span class="pre">indices</span></code> is the index location of each maximum value found
(argmax).</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensors are of the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where they are of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting
in the output tensors having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether the output tensors have <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the result tuple of two output tensors (max, max_indices)</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-1.2360, -0.2942, -0.1222,  0.8475],</span>
<span class="go">        [ 1.1949, -1.1127, -2.2379, -0.6702],</span>
<span class="go">        [ 1.5717, -0.9207,  0.1297, -1.8768],</span>
<span class="go">        [-0.6172,  1.0036, -0.6060, -0.2432]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">max</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Each element of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is compared with the corresponding
element of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> and an element-wise maximum is taken.</p>
<p>The shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> don’t need to match,
but they must be <span class="xref std std-ref">broadcastable</span>.</p>
<div class="math">
\[\text{out}_i = \max(\text{tensor}_i, \text{other}_i)

\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the shapes do not match, the shape of the returned output tensor
follows the <span class="xref std std-ref">broadcasting rules</span>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.2942, -0.7416,  0.2653, -0.1584])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([ 0.8722, -1.7421, -0.4141, -0.5055])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">tensor([ 0.8722, -0.7416,  0.2653, -0.1584])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.min">
<code class="descclassname">torch.</code><code class="descname">min</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.min" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">min</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns the minimum value of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.6750,  1.0857,  1.7197]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(0.6750)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">min</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>keepdim=False</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(values,</span> <span class="pre">indices)</span></code> where <code class="docutils literal notranslate"><span class="pre">values</span></code> is the minimum
value of each row of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor in the given dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. And <code class="docutils literal notranslate"><span class="pre">indices</span></code> is the index location of each minimum value found
(argmin).</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensors are of the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where they are of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting in
the output tensors having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensors have <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the tuple of two output tensors (min, min_indices)</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.6248,  1.1334, -1.1899, -0.2803],</span>
<span class="go">        [-1.4644, -0.2635, -0.3651,  0.6134],</span>
<span class="go">        [ 0.2457,  0.0384,  1.0128,  0.7015],</span>
<span class="go">        [-0.1153,  2.9849,  2.1458,  0.5788]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">min</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Each element of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is compared with the corresponding
element of the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> and an element-wise minimum is taken.
The resulting tensor is returned.</p>
<p>The shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> don’t need to match,
but they must be <span class="xref std std-ref">broadcastable</span>.</p>
<div class="math">
\[\text{out}_i = \min(\text{tensor}_i, \text{other}_i)

\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the shapes do not match, the shape of the returned output tensor
follows the <span class="xref std std-ref">broadcasting rules</span>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.8137, -1.1740, -0.6460,  0.6308])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([-0.1369,  0.1555,  0.4019, -0.1929])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">tensor([-0.1369, -1.1740, -0.6460, -0.1929])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ne">
<code class="descclassname">torch.</code><code class="descname">ne</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.ne" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <span class="math">\(input \neq other\)</span> element-wise.</p>
<p>The second argument can be a number or a tensor whose shape is
<span class="xref std std-ref">broadcastable</span> with the first argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to compare</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the tensor or value to compare</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor that must be a <cite>ByteTensor</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go">tensor([[ 0,  1],</span>
<span class="go">        [ 1,  0]], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sort">
<code class="descclassname">torch.</code><code class="descname">sort</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em>, <em>descending=False</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Sorts the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along a given dimension
in ascending order by value.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is not given, the last dimension of the <cite>input</cite> is chosen.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">descending</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> then the elements are sorted in descending
order by value.</p>
<p>A tuple of (sorted_tensor, sorted_indices) is returned, where the
sorted_indices are the indices of the elements in the original <cite>input</cite> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the dimension to sort along</p></li>
<li><p><strong>descending</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls the sorting order (ascending or descending)</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the output tuple of (<cite>Tensor</cite>, <cite>LongTensor</cite>) that can
be optionally given to be used as output buffers</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span>
<span class="go">tensor([[-0.2162,  0.0608,  0.6719,  2.3332],</span>
<span class="go">        [-0.5793,  0.0061,  0.6058,  0.9497],</span>
<span class="go">        [-0.5071,  0.3343,  0.9553,  1.0960]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="go">tensor([[ 1,  0,  2,  3],</span>
<span class="go">        [ 3,  1,  0,  2],</span>
<span class="go">        [ 0,  3,  1,  2]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span>
<span class="go">tensor([[-0.5071, -0.2162,  0.6719, -0.5793],</span>
<span class="go">        [ 0.0608,  0.0061,  0.9497,  0.3343],</span>
<span class="go">        [ 0.6058,  0.9553,  1.0960,  2.3332]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="go">tensor([[ 2,  0,  0,  1],</span>
<span class="go">        [ 0,  1,  1,  2],</span>
<span class="go">        [ 1,  2,  2,  0]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.topk">
<code class="descclassname">torch.</code><code class="descname">topk</code><span class="sig-paren">(</span><em>input</em>, <em>k</em>, <em>dim=None</em>, <em>largest=True</em>, <em>sorted=True</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.topk" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <code class="xref py py-attr docutils literal notranslate"><span class="pre">k</span></code> largest elements of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor along
a given dimension.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is not given, the last dimension of the <cite>input</cite> is chosen.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">largest</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code> then the <cite>k</cite> smallest elements are returned.</p>
<p>A tuple of <cite>(values, indices)</cite> is returned, where the <cite>indices</cite> are the indices
of the elements in the original <cite>input</cite> tensor.</p>
<p>The boolean option <code class="xref py py-attr docutils literal notranslate"><span class="pre">sorted</span></code> if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will make sure that the returned
<cite>k</cite> elements are themselves sorted</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>k</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the k in “top-k”</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the dimension to sort along</p></li>
<li><p><strong>largest</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return largest or
smallest elements</p></li>
<li><p><strong>sorted</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return the elements
in sorted order</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the output tuple of (Tensor, LongTensor) that can be
optionally given to be used as output buffers</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.,  5.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">(tensor([ 5.,  4.,  3.]), tensor([ 4,  3,  2]))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="spectral-ops">
<h4>Spectral Ops<a class="headerlink" href="#spectral-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.fft">
<code class="descclassname">torch.</code><code class="descname">fft</code><span class="sig-paren">(</span><em>input</em>, <em>signal_ndim</em>, <em>normalized=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.fft" title="Permalink to this definition">¶</a></dt>
<dd><p>Complex-to-complex Discrete Fourier Transform</p>
<p>This method computes the complex-to-complex discrete Fourier transform.
Ignoring the batch dimensions, it computes the following expression:</p>
<div class="math">
\[X[\omega_1, \dots, \omega_d] =
    \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]
     e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},

\]</div>
<p>where <span class="math">\(d\)</span> = <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> is number of dimensions for the
signal, and <span class="math">\(N_i\)</span> is the size of signal dimension <span class="math">\(i\)</span>.</p>
<p>This method supports 1D, 2D and 3D complex-to-complex transforms, indicated
by <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> must be a tensor with last dimension
of size 2, representing the real and imaginary components of complex
numbers, and should have at least <code class="docutils literal notranslate"><span class="pre">signal_ndim</span> <span class="pre">+</span> <span class="pre">1</span></code> dimensions with optionally
arbitrary number of leading batch dimensions. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>, this normalizes the result by dividing it with
<span class="math">\(\sqrt{\prod_{i=1}^K N_i}\)</span> so that the operator is unitary.</p>
<p>Returns the real and the imaginary parts together as one tensor of the same
shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>The inverse of this function is <a class="reference internal" href="#torch.ifft" title="torch.ifft"><code class="xref py py-func docutils literal notranslate"><span class="pre">ifft()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up
repeatedly running FFT methods on tensors of same geometry with same
same configuration.</p>
<p>Changing <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.max_size</span></code> (default is
4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the
capacity of this cache. Some cuFFT plans may allocate GPU memory. You can
use <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.size</span></code> to query the number of
plans currently in cache, and
<code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.clear()</span></code> to clear the cache.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.backends.mkl.is_available()</span></code> to check if MKL is installed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of at least <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> <code class="docutils literal notranslate"><span class="pre">+</span> <span class="pre">1</span></code>
dimensions</p></li>
<li><p><strong>signal_ndim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the number of dimensions in each signal.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> can only be 1, 2 or 3</p></li>
<li><p><strong>normalized</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return normalized results.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor containing the complex-to-complex Fourier transform result</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># unbatched 2D FFT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[-0.0876,  1.7835],</span>
<span class="go">         [-2.0399, -2.9754],</span>
<span class="go">         [ 4.4773, -5.0119]],</span>

<span class="go">        [[-1.5716,  2.7631],</span>
<span class="go">         [-3.8846,  5.2652],</span>
<span class="go">         [ 0.2046, -0.7088]],</span>

<span class="go">        [[ 1.9938, -0.5901],</span>
<span class="go">         [ 6.5637,  6.4556],</span>
<span class="go">         [ 2.9865,  4.9318]],</span>

<span class="go">        [[ 7.0193,  1.1742],</span>
<span class="go">         [-1.3717, -2.1084],</span>
<span class="go">         [ 2.0289,  2.9357]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched 1D FFT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[[ 1.8385,  1.2827],</span>
<span class="go">         [-0.1831,  1.6593],</span>
<span class="go">         [ 2.4243,  0.5367]],</span>

<span class="go">        [[-0.9176, -1.5543],</span>
<span class="go">         [-3.9943, -2.9860],</span>
<span class="go">         [ 1.2838, -2.9420]],</span>

<span class="go">        [[-0.8854, -0.6860],</span>
<span class="go">         [ 2.4450,  0.0808],</span>
<span class="go">         [ 1.3076, -0.5768]],</span>

<span class="go">        [[-0.1231,  2.7411],</span>
<span class="go">         [-0.3075, -1.7295],</span>
<span class="go">         [-0.5384, -2.0299]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># arbitrary number of batch dimensions, 2D FFT</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, 3, 5, 5, 2])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ifft">
<code class="descclassname">torch.</code><code class="descname">ifft</code><span class="sig-paren">(</span><em>input</em>, <em>signal_ndim</em>, <em>normalized=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.ifft" title="Permalink to this definition">¶</a></dt>
<dd><p>Complex-to-complex Inverse Discrete Fourier Transform</p>
<p>This method computes the complex-to-complex inverse discrete Fourier
transform. Ignoring the batch dimensions, it computes the following
expression:</p>
<div class="math">
\[X[\omega_1, \dots, \omega_d] =
    \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]
     e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},

\]</div>
<p>where <span class="math">\(d\)</span> = <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> is number of dimensions for the
signal, and <span class="math">\(N_i\)</span> is the size of signal dimension <span class="math">\(i\)</span>.</p>
<p>The argument specifications are almost identical with <a class="reference internal" href="#torch.fft" title="torch.fft"><code class="xref py py-func docutils literal notranslate"><span class="pre">fft()</span></code></a>.
However, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this instead returns the
results multiplied by <span class="math">\(\sqrt{\prod_{i=1}^d N_i}\)</span>, to become a unitary
operator. Therefore, to invert a <a class="reference internal" href="#torch.fft" title="torch.fft"><code class="xref py py-func docutils literal notranslate"><span class="pre">fft()</span></code></a>, the <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized</span></code>
argument should be set identically for <a class="reference internal" href="#torch.fft" title="torch.fft"><code class="xref py py-func docutils literal notranslate"><span class="pre">fft()</span></code></a>.</p>
<p>Returns the real and the imaginary parts together as one tensor of the same
shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>The inverse of this function is <a class="reference internal" href="#torch.fft" title="torch.fft"><code class="xref py py-func docutils literal notranslate"><span class="pre">fft()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up
repeatedly running FFT methods on tensors of same geometry with same
same configuration.</p>
<p>Changing <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.max_size</span></code> (default is
4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the
capacity of this cache. Some cuFFT plans may allocate GPU memory. You can
use <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.size</span></code> to query the number of
plans currently in cache, and
<code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.clear()</span></code> to clear the cache.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.backends.mkl.is_available()</span></code> to check if MKL is installed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of at least <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> <code class="docutils literal notranslate"><span class="pre">+</span> <span class="pre">1</span></code>
dimensions</p></li>
<li><p><strong>signal_ndim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the number of dimensions in each signal.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> can only be 1, 2 or 3</p></li>
<li><p><strong>normalized</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return normalized results.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor containing the complex-to-complex inverse Fourier transform result</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[[ 1.2766,  1.3680],</span>
<span class="go">         [-0.8337,  2.0251],</span>
<span class="go">         [ 0.9465, -1.4390]],</span>

<span class="go">        [[-0.1890,  1.6010],</span>
<span class="go">         [ 1.1034, -1.9230],</span>
<span class="go">         [-0.9482,  1.0775]],</span>

<span class="go">        [[-0.7708, -0.8176],</span>
<span class="go">         [-0.1843, -0.2287],</span>
<span class="go">         [-1.9034, -0.2196]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ifft</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># recover x</span>
<span class="go">tensor([[[ 1.2766,  1.3680],</span>
<span class="go">         [-0.8337,  2.0251],</span>
<span class="go">         [ 0.9465, -1.4390]],</span>

<span class="go">        [[-0.1890,  1.6010],</span>
<span class="go">         [ 1.1034, -1.9230],</span>
<span class="go">         [-0.9482,  1.0775]],</span>

<span class="go">        [[-0.7708, -0.8176],</span>
<span class="go">         [-0.1843, -0.2287],</span>
<span class="go">         [-1.9034, -0.2196]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.rfft">
<code class="descclassname">torch.</code><code class="descname">rfft</code><span class="sig-paren">(</span><em>input</em>, <em>signal_ndim</em>, <em>normalized=False</em>, <em>onesided=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.rfft" title="Permalink to this definition">¶</a></dt>
<dd><p>Real-to-complex Discrete Fourier Transform</p>
<p>This method computes the real-to-complex discrete Fourier transform. It is
mathematically equivalent with <a class="reference internal" href="#torch.fft" title="torch.fft"><code class="xref py py-func docutils literal notranslate"><span class="pre">fft()</span></code></a> with differences only in
formats of the input and output.</p>
<p>This method supports 1D, 2D and 3D real-to-complex transforms, indicated
by <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> must be a tensor with at least
<code class="docutils literal notranslate"><span class="pre">signal_ndim</span></code> dimensions with optionally arbitrary number of leading batch
dimensions. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this normalizes the result
by dividing it with <span class="math">\(\sqrt{\prod_{i=1}^K N_i}\)</span> so that the operator is
unitary, where <span class="math">\(N_i\)</span> is the size of signal dimension <span class="math">\(i\)</span>.</p>
<p>The real-to-complex Fourier transform results follow conjugate symmetry:</p>
<div class="math">
\[X[\omega_1, \dots, \omega_d] = X^*[N_1 - \omega_1, \dots, N_d - \omega_d],

\]</div>
<p>where the index arithmetic is computed modulus the size of the corresponding
dimension, <span class="math">\(\ ^*\)</span> is the conjugate operator, and
<span class="math">\(d\)</span> = <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">onesided</span></code> flag controls whether to avoid
redundancy in the output results. If set to <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), the output will
not be full complex result of shape <span class="math">\((*, 2)\)</span>, where <span class="math">\(*\)</span> is the shape
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, but instead the last dimension will be halfed as of size
<span class="math">\(\lfloor \frac{N_d}{2} \rfloor + 1\)</span>.</p>
<p>The inverse of this function is <a class="reference internal" href="#torch.irfft" title="torch.irfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">irfft()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up
repeatedly running FFT methods on tensors of same geometry with same
same configuration.</p>
<p>Changing <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.max_size</span></code> (default is
4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the
capacity of this cache. Some cuFFT plans may allocate GPU memory. You can
use <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.size</span></code> to query the number of
plans currently in cache, and
<code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.clear()</span></code> to clear the cache.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.backends.mkl.is_available()</span></code> to check if MKL is installed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of at least <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> dimensions</p></li>
<li><p><strong>signal_ndim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the number of dimensions in each signal.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> can only be 1, 2 or 3</p></li>
<li><p><strong>normalized</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return normalized results.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>onesided</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return half of results to
avoid redundancy. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor containing the real-to-complex Fourier transform result</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 3, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, 5, 2])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.irfft">
<code class="descclassname">torch.</code><code class="descname">irfft</code><span class="sig-paren">(</span><em>input</em>, <em>signal_ndim</em>, <em>normalized=False</em>, <em>onesided=True</em>, <em>signal_sizes=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.irfft" title="Permalink to this definition">¶</a></dt>
<dd><p>Complex-to-real Inverse Discrete Fourier Transform</p>
<p>This method computes the complex-to-real inverse discrete Fourier transform.
It is mathematically equivalent with <a class="reference internal" href="#torch.ifft" title="torch.ifft"><code class="xref py py-func docutils literal notranslate"><span class="pre">ifft()</span></code></a> with differences only in
formats of the input and output.</p>
<p>The argument specifications are almost identical with <a class="reference internal" href="#torch.ifft" title="torch.ifft"><code class="xref py py-func docutils literal notranslate"><span class="pre">ifft()</span></code></a>.
Similar to <a class="reference internal" href="#torch.ifft" title="torch.ifft"><code class="xref py py-func docutils literal notranslate"><span class="pre">ifft()</span></code></a>, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
this normalizes the result by multiplying it with
<span class="math">\(\sqrt{\prod_{i=1}^K N_i}\)</span> so that the operator is unitary, where
<span class="math">\(N_i\)</span> is the size of signal dimension <span class="math">\(i\)</span>.</p>
<p>Due to the conjugate symmetry, <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> do not need to contain the full
complex frequency values. Roughly half of the values will be sufficient, as
is the case when <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is given by <a class="reference internal" href="#torch.rfft" title="torch.rfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">rfft()</span></code></a> with
<code class="docutils literal notranslate"><span class="pre">rfft(signal,</span> <span class="pre">onesided=True)</span></code>. In such case, set the <code class="xref py py-attr docutils literal notranslate"><span class="pre">onesided</span></code>
argument of this method to <code class="docutils literal notranslate"><span class="pre">True</span></code>. Moreover, the original signal shape
information can sometimes be lost, optionally set <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_sizes</span></code> to be
the size of the original signal (without the batch dimensions if in batched
mode) to recover it with correct shape.</p>
<p>Therefore, to invert an <a class="reference internal" href="#torch.rfft" title="torch.rfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">rfft()</span></code></a>, the <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">onesided</span></code> arguments should be set identically for <a class="reference internal" href="#torch.irfft" title="torch.irfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">irfft()</span></code></a>,
and preferrably a <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_sizes</span></code> is given to avoid size mismatch. See the
example below for a case of size mismatch.</p>
<p>See <a class="reference internal" href="#torch.rfft" title="torch.rfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">rfft()</span></code></a> for details on conjugate symmetry.</p>
<p>The inverse of this function is <a class="reference internal" href="#torch.rfft" title="torch.rfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">rfft()</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Generally speaking, the input of this function should contain values
following conjugate symmetry. Note that even if <code class="xref py py-attr docutils literal notranslate"><span class="pre">onesided</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, often symmetry on some part is still needed. When this
requirement is not satisfied, the behavior of <a class="reference internal" href="#torch.irfft" title="torch.irfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">irfft()</span></code></a> is
undefined. Since <a class="reference internal" href="index.html#torch.autograd.gradcheck" title="torch.autograd.gradcheck"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.gradcheck()</span></code></a> estimates numerical
Jacobian with point perturbations, <a class="reference internal" href="#torch.irfft" title="torch.irfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">irfft()</span></code></a> will almost
certainly fail the check.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up
repeatedly running FFT methods on tensors of same geometry with same
same configuration.</p>
<p>Changing <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.max_size</span></code> (default is
4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the
capacity of this cache. Some cuFFT plans may allocate GPU memory. You can
use <code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.size</span></code> to query the number of
plans currently in cache, and
<code class="docutils literal notranslate"><span class="pre">torch.backends.cuda.cufft_plan_cache.clear()</span></code> to clear the cache.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For CPU tensors, this method is currently only available with MKL. Use
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.backends.mkl.is_available()</span></code> to check if MKL is installed.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of at least <code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> <code class="docutils literal notranslate"><span class="pre">+</span> <span class="pre">1</span></code>
dimensions</p></li>
<li><p><strong>signal_ndim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the number of dimensions in each signal.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">signal_ndim</span></code> can only be 1, 2 or 3</p></li>
<li><p><strong>normalized</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return normalized results.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>onesided</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> was halfed to avoid
redundancy, e.g., by <a class="reference internal" href="#torch.rfft" title="torch.rfft"><code class="xref py py-func docutils literal notranslate"><span class="pre">rfft()</span></code></a>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>signal_sizes</strong> (list or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code>, optional) – the size of the original
signal (without batch dimension). Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor containing the complex-to-real inverse Fourier transform result</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([4, 3, 2])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># notice that with onesided=True, output size does not determine the original signal size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([4, 3, 2])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># now we use the original shape to recover x</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],</span>
<span class="go">        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],</span>
<span class="go">        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],</span>
<span class="go">        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">irfft</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># recover x</span>
<span class="go">tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],</span>
<span class="go">        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],</span>
<span class="go">        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],</span>
<span class="go">        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.stft">
<code class="descclassname">torch.</code><code class="descname">stft</code><span class="sig-paren">(</span><em>input</em>, <em>n_fft</em>, <em>hop_length=None</em>, <em>win_length=None</em>, <em>window=None</em>, <em>center=True</em>, <em>pad_mode='reflect'</em>, <em>normalized=False</em>, <em>onesided=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.stft" title="Permalink to this definition">¶</a></dt>
<dd><p>Short-time Fourier transform (STFT).</p>
<p>Ignoring the optional batch dimension, this method computes the following
expression:</p>
<div class="math">
\[X[m, \omega] = \sum_{k = 0}^{\text{win\_length-1}}%
                    \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ %
                    \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),

\]</div>
<p>where <span class="math">\(m\)</span> is the index of the sliding window, and <span class="math">\(\omega\)</span> is
the frequency that <span class="math">\(0 \leq \omega &lt; \text{n\_fft}\)</span>. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">onesided</span></code> is the default value <code class="docutils literal notranslate"><span class="pre">True</span></code>,</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> must be either a 1-D time sequence or a 2-D batch of time
sequences.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">hop_length</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), it is treated as equal to
<code class="docutils literal notranslate"><span class="pre">floor(n_fft</span> <span class="pre">/</span> <span class="pre">4)</span></code>.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">win_length</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), it is treated as equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">n_fft</span></code>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">window</span></code> can be a 1-D tensor of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">win_length</span></code>, e.g., from
<a class="reference internal" href="#torch.hann_window" title="torch.hann_window"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.hann_window()</span></code></a>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">window</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), it is
treated as if having <span class="math">\(1\)</span> everywhere in the window. If
<span class="math">\(\text{win\_length} &lt; \text{n\_fft}\)</span>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">window</span></code> will be padded on
both sides to length <code class="xref py py-attr docutils literal notranslate"><span class="pre">n_fft</span></code> before being applied.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">center</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be padded on
both sides so that the <span class="math">\(t\)</span>-th frame is centered at time
<span class="math">\(t \times \text{hop\_length}\)</span>. Otherwise, the <span class="math">\(t\)</span>-th frame
begins at time  <span class="math">\(t \times \text{hop\_length}\)</span>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">pad_mode</span></code> determines the padding method used on <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">center</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>. See <a class="reference internal" href="index.html#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a> for
all available options. Default is <code class="docutils literal notranslate"><span class="pre">&quot;reflect&quot;</span></code>.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">onesided</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), only values for <span class="math">\(\omega\)</span>
in <span class="math">\(\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]\)</span>
are returned because the real-to-complex Fourier transform satisfies the
conjugate symmetry, i.e., <span class="math">\(X[m, \omega] = X[m, \text{n\_fft} - \omega]^*\)</span>.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> (default is <code class="docutils literal notranslate"><span class="pre">False</span></code>), the function
returns the normalized STFT results, i.e., multiplied by <span class="math">\((\text{frame\_length})^{-0.5}\)</span>.</p></li>
</ul>
<p>Returns the real and the imaginary parts together as one tensor of size
<span class="math">\((* \times N \times T \times 2)\)</span>, where <span class="math">\(*\)</span> is the optional
batch size of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, <span class="math">\(N\)</span> is the number of frequencies where
STFT is applied, <span class="math">\(T\)</span> is the total number of frames used, and each pair
in the last dimension represents a complex number as the real part and the
imaginary part.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function changed signature at version 0.4.1. Calling with the
previous signature may cause error or return incorrect result.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>n_fft</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – size of Fourier transform</p></li>
<li><p><strong>hop_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the distance between neighboring sliding window
frames. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> (treated as equal to <code class="docutils literal notranslate"><span class="pre">floor(n_fft</span> <span class="pre">/</span> <span class="pre">4)</span></code>)</p></li>
<li><p><strong>win_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the size of window frame and STFT filter.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>  (treated as equal to <code class="xref py py-attr docutils literal notranslate"><span class="pre">n_fft</span></code>)</p></li>
<li><p><strong>window</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the optional window function.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> (treated as window of all <span class="math">\(1\)</span> s)</p></li>
<li><p><strong>center</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to pad <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> on both sides so
that the <span class="math">\(t\)</span>-th frame is centered at time <span class="math">\(t \times \text{hop\_length}\)</span>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>pad_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – controls the padding method used when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">center</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">&quot;reflect&quot;</span></code></p></li>
<li><p><strong>normalized</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return the normalized STFT results
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>onesided</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether to return half of results to
avoid redundancy Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor containing the STFT result with shape described above</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.bartlett_window">
<code class="descclassname">torch.</code><code class="descname">bartlett_window</code><span class="sig-paren">(</span><em>window_length</em>, <em>periodic=True</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.bartlett_window" title="Permalink to this definition">¶</a></dt>
<dd><p>Bartlett window function.</p>
<div class="math">
\[w[n] = 1 - \left| \frac{2n}{N-1} - 1 \right| = \begin{cases}
    \frac{2n}{N - 1} & \text{if } 0 \leq n \leq \frac{N - 1}{2} \\
    2 - \frac{2n}{N - 1} & \text{if } \frac{N - 1}{2} < n < N \\
\end{cases},

\]</div>
<p>where <span class="math">\(N\)</span> is the full window size.</p>
<p>The input <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> is a positive integer controlling the
returned window size. <code class="xref py py-attr docutils literal notranslate"><span class="pre">periodic</span></code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<a class="reference internal" href="#torch.stft" title="torch.stft"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.stft()</span></code></a>. Therefore, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">periodic</span></code> is true, the <span class="math">\(N\)</span> in
above formula is in fact <span class="math">\(\text{window\_length} + 1\)</span>. Also, we always have
<code class="docutils literal notranslate"><span class="pre">torch.bartlett_window(L,</span> <span class="pre">periodic=True)</span></code> equal to
<code class="docutils literal notranslate"><span class="pre">torch.bartlett_window(L</span> <span class="pre">+</span> <span class="pre">1,</span> <span class="pre">periodic=False)[:-1])</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> <span class="math">\(=1\)</span>, the returned window contains a single value 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>window_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of returned window</p></li>
<li><p><strong>periodic</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, returns a window to be used as periodic
function. If False, return a symmetric window.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). Only floating point types are supported.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned window tensor. Only
<code class="docutils literal notranslate"><span class="pre">torch.strided</span></code> (dense layout) is supported.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 1-D tensor of size <span class="math">\((\text{window\_length},)\)</span> containing the window</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.blackman_window">
<code class="descclassname">torch.</code><code class="descname">blackman_window</code><span class="sig-paren">(</span><em>window_length</em>, <em>periodic=True</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.blackman_window" title="Permalink to this definition">¶</a></dt>
<dd><p>Blackman window function.</p>
<div class="math">
\[w[n] = 0.42 - 0.5 \cos \left( \frac{2 \pi n}{N - 1} \right) + 0.08 \cos \left( \frac{4 \pi n}{N - 1} \right)

\]</div>
<p>where <span class="math">\(N\)</span> is the full window size.</p>
<p>The input <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> is a positive integer controlling the
returned window size. <code class="xref py py-attr docutils literal notranslate"><span class="pre">periodic</span></code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<a class="reference internal" href="#torch.stft" title="torch.stft"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.stft()</span></code></a>. Therefore, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">periodic</span></code> is true, the <span class="math">\(N\)</span> in
above formula is in fact <span class="math">\(\text{window\_length} + 1\)</span>. Also, we always have
<code class="docutils literal notranslate"><span class="pre">torch.blackman_window(L,</span> <span class="pre">periodic=True)</span></code> equal to
<code class="docutils literal notranslate"><span class="pre">torch.blackman_window(L</span> <span class="pre">+</span> <span class="pre">1,</span> <span class="pre">periodic=False)[:-1])</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> <span class="math">\(=1\)</span>, the returned window contains a single value 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>window_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of returned window</p></li>
<li><p><strong>periodic</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, returns a window to be used as periodic
function. If False, return a symmetric window.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). Only floating point types are supported.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned window tensor. Only
<code class="docutils literal notranslate"><span class="pre">torch.strided</span></code> (dense layout) is supported.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 1-D tensor of size <span class="math">\((\text{window\_length},)\)</span> containing the window</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.hamming_window">
<code class="descclassname">torch.</code><code class="descname">hamming_window</code><span class="sig-paren">(</span><em>window_length</em>, <em>periodic=True</em>, <em>alpha=0.54</em>, <em>beta=0.46</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.hamming_window" title="Permalink to this definition">¶</a></dt>
<dd><p>Hamming window function.</p>
<div class="math">
\[w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right),

\]</div>
<p>where <span class="math">\(N\)</span> is the full window size.</p>
<p>The input <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> is a positive integer controlling the
returned window size. <code class="xref py py-attr docutils literal notranslate"><span class="pre">periodic</span></code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<a class="reference internal" href="#torch.stft" title="torch.stft"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.stft()</span></code></a>. Therefore, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">periodic</span></code> is true, the <span class="math">\(N\)</span> in
above formula is in fact <span class="math">\(\text{window\_length} + 1\)</span>. Also, we always have
<code class="docutils literal notranslate"><span class="pre">torch.hamming_window(L,</span> <span class="pre">periodic=True)</span></code> equal to
<code class="docutils literal notranslate"><span class="pre">torch.hamming_window(L</span> <span class="pre">+</span> <span class="pre">1,</span> <span class="pre">periodic=False)[:-1])</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> <span class="math">\(=1\)</span>, the returned window contains a single value 1.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a generalized version of <a class="reference internal" href="#torch.hann_window" title="torch.hann_window"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.hann_window()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>window_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of returned window</p></li>
<li><p><strong>periodic</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, returns a window to be used as periodic
function. If False, return a symmetric window.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). Only floating point types are supported.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned window tensor. Only
<code class="docutils literal notranslate"><span class="pre">torch.strided</span></code> (dense layout) is supported.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 1-D tensor of size <span class="math">\((\text{window\_length},)\)</span> containing the window</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.hann_window">
<code class="descclassname">torch.</code><code class="descname">hann_window</code><span class="sig-paren">(</span><em>window_length</em>, <em>periodic=True</em>, <em>dtype=None</em>, <em>layout=torch.strided</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.hann_window" title="Permalink to this definition">¶</a></dt>
<dd><p>Hann window function.</p>
<div class="math">
\[w[n] = \frac{1}{2}\ \left[1 - \cos \left( \frac{2 \pi n}{N - 1} \right)\right] =
        \sin^2 \left( \frac{\pi n}{N - 1} \right),

\]</div>
<p>where <span class="math">\(N\)</span> is the full window size.</p>
<p>The input <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> is a positive integer controlling the
returned window size. <code class="xref py py-attr docutils literal notranslate"><span class="pre">periodic</span></code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<a class="reference internal" href="#torch.stft" title="torch.stft"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.stft()</span></code></a>. Therefore, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">periodic</span></code> is true, the <span class="math">\(N\)</span> in
above formula is in fact <span class="math">\(\text{window\_length} + 1\)</span>. Also, we always have
<code class="docutils literal notranslate"><span class="pre">torch.hann_window(L,</span> <span class="pre">periodic=True)</span></code> equal to
<code class="docutils literal notranslate"><span class="pre">torch.hann_window(L</span> <span class="pre">+</span> <span class="pre">1,</span> <span class="pre">periodic=False)[:-1])</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">window_length</span></code> <span class="math">\(=1\)</span>, the returned window contains a single value 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>window_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of returned window</p></li>
<li><p><strong>periodic</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, returns a window to be used as periodic
function. If False, return a symmetric window.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). Only floating point types are supported.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – the desired layout of returned window tensor. Only
<code class="docutils literal notranslate"><span class="pre">torch.strided</span></code> (dense layout) is supported.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A 1-D tensor of size <span class="math">\((\text{window\_length},)\)</span> containing the window</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="other-operations">
<h4>Other Operations<a class="headerlink" href="#other-operations" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.bincount">
<code class="descclassname">torch.</code><code class="descname">bincount</code><span class="sig-paren">(</span><em>self</em>, <em>weights=None</em>, <em>minlength=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.bincount" title="Permalink to this definition">¶</a></dt>
<dd><p>Count the frequency of each value in an array of non-negative ints.</p>
<p>The number of bins (size 1) is one larger than the largest value in
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> unless <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is empty, in which case the result is a
tensor of size 0. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">minlength</span></code> is specified, the number of bins is at least
<code class="xref py py-attr docutils literal notranslate"><span class="pre">minlength</span></code> and if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is empty, then the result is tensor of size
<code class="xref py py-attr docutils literal notranslate"><span class="pre">minlength</span></code> filled with zeros. If <code class="docutils literal notranslate"><span class="pre">n</span></code> is the value at position <code class="docutils literal notranslate"><span class="pre">i</span></code>,
<code class="docutils literal notranslate"><span class="pre">out[n]</span> <span class="pre">+=</span> <span class="pre">weights[i]</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">weights</span></code> is specified else
<code class="docutils literal notranslate"><span class="pre">out[n]</span> <span class="pre">+=</span> <span class="pre">1</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 1-d int tensor</p></li>
<li><p><strong>weights</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – optional, weight for each value in the input tensor.
Should be of same size as input tensor.</p></li>
<li><p><strong>minlength</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – optional, minimum number of bins. Should be non-negative.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tensor of shape <code class="docutils literal notranslate"><span class="pre">Size([max(input)</span> <span class="pre">+</span> <span class="pre">1])</span></code> if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is non-empty, else <code class="docutils literal notranslate"><span class="pre">Size(0)</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="p">,</span> <span class="n">weights</span>
<span class="go">(tensor([4, 3, 6, 3, 4]),</span>
<span class="go"> tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([0, 0, 0, 2, 2, 0, 1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="go">tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.broadcast_tensors">
<code class="descclassname">torch.</code><code class="descname">broadcast_tensors</code><span class="sig-paren">(</span><em>*tensors</em><span class="sig-paren">)</span> &#x2192; List of Tensors<a class="headerlink" href="#torch.broadcast_tensors" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts the given tensors according to <span class="xref std std-ref">broadcasting-semantics</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*tensors</strong> – any number of tensors of the same type</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>More than one element of a broadcasted tensor may refer to a single
memory location. As a result, in-place operations (especially ones that
are vectorized) may result in incorrect behavior. If you need to write
to the tensors, please clone them first.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 1, 2],</span>
<span class="go">        [0, 1, 2]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cartesian_prod">
<code class="descclassname">torch.</code><code class="descname">cartesian_prod</code><span class="sig-paren">(</span><em>*tensors</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cartesian_prod" title="Permalink to this definition">¶</a></dt>
<dd><p>Do cartesian product of the given sequence of tensors. The behavior is similar to
python’s <cite>itertools.product</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*tensors</strong> – any number of 1 dimensional tensors.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tensor equivalent to converting all the input tensors into lists,</dt><dd><p>do <cite>itertools.product</cite> on these lists, and finally convert the resulting list
into tensor.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="go">[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cartesian_prod</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">tensor_b</span><span class="p">)</span>
<span class="go">tensor([[1, 4],</span>
<span class="go">        [1, 5],</span>
<span class="go">        [2, 4],</span>
<span class="go">        [2, 5],</span>
<span class="go">        [3, 4],</span>
<span class="go">        [3, 5]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.combinations">
<code class="descclassname">torch.</code><code class="descname">combinations</code><span class="sig-paren">(</span><em>tensor</em>, <em>r=2</em>, <em>with_replacement=False</em><span class="sig-paren">)</span> &#x2192; seq<a class="headerlink" href="#torch.combinations" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute combinations of length <span class="math">\(r\)</span> of the given tensor. The behavior is similar to
python’s <cite>itertools.combinations</cite> when <cite>with_replacement</cite> is set to <cite>False</cite>, and
<cite>itertools.combinations_with_replacement</cite> when <cite>with_replacement</cite> is set to <cite>True</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 1D vector.</p></li>
<li><p><strong>r</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – number of elements to combine</p></li>
<li><p><strong>with_replacement</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether to allow duplication in combination</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor equivalent to converting all the input tensors into lists, do
<cite>itertools.combinations</cite> or <cite>itertools.combinations_with_replacement</cite> on these
lists, and finally convert the resulting list into tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="go">[(1, 2), (1, 3), (2, 3)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
<span class="go">[(1, 2, 3)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">combinations_with_replacement</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="go">[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">)</span>
<span class="go">tensor([[1, 2],</span>
<span class="go">        [1, 3],</span>
<span class="go">        [2, 3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[1, 2, 3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">with_replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[1, 1],</span>
<span class="go">        [1, 2],</span>
<span class="go">        [1, 3],</span>
<span class="go">        [2, 2],</span>
<span class="go">        [2, 3],</span>
<span class="go">        [3, 3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cross">
<code class="descclassname">torch.</code><code class="descname">cross</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>dim=-1</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.cross" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the cross product of vectors in dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must have the same size, and the size of their
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> dimension should be 3.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is not given, it defaults to the first dimension found with the
size 3.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the dimension to take the cross-product in.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.3956,  1.1455,  1.6895],</span>
<span class="go">        [-0.5849,  1.3672,  0.3599],</span>
<span class="go">        [-1.1626,  0.7180, -0.0521],</span>
<span class="go">        [-0.1339,  0.9902, -2.0225]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([[-0.0257, -1.4725, -1.2251],</span>
<span class="go">        [-1.1479, -0.7005, -1.9757],</span>
<span class="go">        [-1.3904,  0.3726, -1.1836],</span>
<span class="go">        [-0.9688, -0.7153,  0.2159]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1.0844, -0.5281,  0.6120],</span>
<span class="go">        [-2.4490, -1.5687,  1.9792],</span>
<span class="go">        [-0.8304, -1.3037,  0.5650],</span>
<span class="go">        [-1.2329,  1.9883,  1.0551]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go">tensor([[ 1.0844, -0.5281,  0.6120],</span>
<span class="go">        [-2.4490, -1.5687,  1.9792],</span>
<span class="go">        [-0.8304, -1.3037,  0.5650],</span>
<span class="go">        [-1.2329,  1.9883,  1.0551]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.diag">
<code class="descclassname">torch.</code><code class="descname">diag</code><span class="sig-paren">(</span><em>input</em>, <em>diagonal=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.diag" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor
with the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as the diagonal.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a matrix (2-D tensor), then returns a 1-D tensor with
the diagonal elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
</ul>
<p>The argument <a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code></a> controls which diagonal to consider:</p>
<ul class="simple">
<li><p>If <a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code></a> = 0, it is the main diagonal.</p></li>
<li><p>If <a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code></a> &gt; 0, it is above the main diagonal.</p></li>
<li><p>If <a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code></a> &lt; 0, it is below the main diagonal.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>diagonal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the diagonal to consider</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagonal()</span></code></a> always returns the diagonal of its input.</p>
<p><a class="reference internal" href="#torch.diagflat" title="torch.diagflat"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diagflat()</span></code></a> always constructs a tensor with diagonal elements
specified by the input.</p>
</div>
<p>Examples:</p>
<p>Get the square matrix where the input vector is the diagonal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([ 0.5950,-0.0872, 2.3298])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 0.5950, 0.0000, 0.0000],</span>
<span class="go">        [ 0.0000,-0.0872, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000, 2.3298]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000,-0.0872, 0.0000],</span>
<span class="go">        [ 0.0000, 0.0000, 0.0000, 2.3298],</span>
<span class="go">        [ 0.0000, 0.0000, 0.0000, 0.0000]])</span>
</pre></div>
</div>
<p>Get the k-th diagonal of a given matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-0.4264, 0.0255,-0.1064],</span>
<span class="go">        [ 0.8795,-0.2429, 0.1374],</span>
<span class="go">        [ 0.1029,-0.6482,-1.6300]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([-0.4264,-0.2429,-1.6300])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 0.0255, 0.1374])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.diag_embed">
<code class="descclassname">torch.</code><code class="descname">diag_embed</code><span class="sig-paren">(</span><em>input</em>, <em>offset=0</em>, <em>dim1=-2</em>, <em>dim2=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.diag_embed" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a tensor whose diagonals of certain 2D planes (specified by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code>) are filled by <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.
To facilitate creating batched diagonal matrices, the 2D planes formed by
the last two dimensions of the returned tensor are chosen by default.</p>
<p>The argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> controls which diagonal to consider:</p>
<ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> = 0, it is the main diagonal.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> &gt; 0, it is above the main diagonal.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> &lt; 0, it is below the main diagonal.</p></li>
</ul>
<p>The size of the new matrix will be calculated to make the specified diagonal
of the size of the last input dimension.
Note that for <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> other than <span class="math">\(0\)</span>, the order of <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code> matters. Exchanging them is equivalent to changing the
sign of <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code>.</p>
<p>Applying <a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.diagonal()</span></code></a> to the output of this function with
the same arguments yields a matrix identical to input. However,
<a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.diagonal()</span></code></a> has different default dimensions, so those
need to be explicitly specified.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor. Must be at least 1-dimensional.</p></li>
<li><p><strong>offset</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – which diagonal to consider. Default: 0
(main diagonal).</p></li>
<li><p><strong>dim1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – first dimension with respect to which to
take diagonal. Default: -2.</p></li>
<li><p><strong>dim2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – second dimension with respect to which to
take diagonal. Default: -1.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[[ 1.5410,  0.0000,  0.0000],</span>
<span class="go">         [ 0.0000, -0.2934,  0.0000],</span>
<span class="go">         [ 0.0000,  0.0000, -2.1788]],</span>

<span class="go">        [[ 0.5684,  0.0000,  0.0000],</span>
<span class="go">         [ 0.0000, -1.0845,  0.0000],</span>
<span class="go">         [ 0.0000,  0.0000, -1.3986]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],</span>
<span class="go">         [ 0.0000,  0.5684,  0.0000,  0.0000]],</span>

<span class="go">        [[ 0.0000,  0.0000, -0.2934,  0.0000],</span>
<span class="go">         [ 0.0000,  0.0000, -1.0845,  0.0000]],</span>

<span class="go">        [[ 0.0000,  0.0000,  0.0000, -2.1788],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000, -1.3986]],</span>

<span class="go">        [[ 0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000,  0.0000]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.diagflat">
<code class="descclassname">torch.</code><code class="descname">diagflat</code><span class="sig-paren">(</span><em>input</em>, <em>diagonal=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.diagflat" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a vector (1-D tensor), then returns a 2-D square tensor
with the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> as the diagonal.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a tensor with more than one dimension, then returns a
2-D tensor with diagonal elements equal to a flattened <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
</ul>
<p>The argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> controls which diagonal to consider:</p>
<ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> = 0, it is the main diagonal.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> &gt; 0, it is above the main diagonal.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> &lt; 0, it is below the main diagonal.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>offset</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the diagonal to consider. Default: 0 (main
diagonal).</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([-0.2956, -0.9068,  0.1695])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diagflat</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[-0.2956,  0.0000,  0.0000],</span>
<span class="go">        [ 0.0000, -0.9068,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000,  0.1695]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diagflat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000, -0.9068,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000,  0.1695],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000,  0.0000]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.2094, -0.3018],</span>
<span class="go">        [-0.1516,  1.9342]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diagflat</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],</span>
<span class="go">        [ 0.0000, -0.3018,  0.0000,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000, -0.1516,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000,  1.9342]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.diagonal">
<code class="descclassname">torch.</code><code class="descname">diagonal</code><span class="sig-paren">(</span><em>input</em>, <em>offset=0</em>, <em>dim1=0</em>, <em>dim2=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.diagonal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a partial view of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with the its diagonal elements
with respect to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim2</span></code> appended as a dimension
at the end of the shape.</p>
<p>The argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> controls which diagonal to consider:</p>
<ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> = 0, it is the main diagonal.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> &gt; 0, it is above the main diagonal.</p></li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> &lt; 0, it is below the main diagonal.</p></li>
</ul>
<p>Applying <a class="reference internal" href="#torch.diag_embed" title="torch.diag_embed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.diag_embed()</span></code></a> to the output of this function with
the same arguments yields a diagonal matrix with the diagonal entries
of the input. However, <a class="reference internal" href="#torch.diag_embed" title="torch.diag_embed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.diag_embed()</span></code></a> has different default
dimensions, so those need to be explicitly specified.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor. Must be at least 2-dimensional.</p></li>
<li><p><strong>offset</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – which diagonal to consider. Default: 0
(main diagonal).</p></li>
<li><p><strong>dim1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – first dimension with respect to which to
take diagonal. Default: 0.</p></li>
<li><p><strong>dim2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – second dimension with respect to which to
take diagonal. Default: 1.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To take a batch diagonal, pass in dim1=-2, dim2=-1.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-1.0854,  1.1431, -0.1752],</span>
<span class="go">        [ 0.8536, -0.0905,  0.0360],</span>
<span class="go">        [ 0.6927, -0.3735, -0.4945]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([-1.0854, -0.0905, -0.4945])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([ 1.1431,  0.0360])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offset</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],</span>
<span class="go">         [-1.1065,  1.0401, -0.2235, -0.7938]],</span>

<span class="go">        [[-1.7325, -0.3081,  0.6166,  0.2335],</span>
<span class="go">         [ 1.0500,  0.7336, -0.3836, -1.1015]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.einsum">
<code class="descclassname">torch.</code><code class="descname">einsum</code><span class="sig-paren">(</span><em>equation</em>, <em>*operands</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.einsum" title="Permalink to this definition">¶</a></dt>
<dd><p>This function provides a way of computing multilinear expressions (i.e. sums of products) using the
Einstein summation convention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>equation</strong> (<em>string</em>) – The equation is given in terms of lower case letters (indices) to be associated
with each dimension of the operands and result. The left hand side lists the operands
dimensions, separated by commas. There should be one index letter per tensor dimension.
The right hand side follows after <cite>-&gt;</cite> and gives the indices for the output.
If the <cite>-&gt;</cite> and right hand side are omitted, it implicitly defined as the alphabetically
sorted list of all indices appearing exactly once in the left hand side.
The indices not apprearing in the output are summed over after multiplying the operands
entries.
If an index appears several times for the same operand, a diagonal is taken.
Ellipses <cite>…</cite> represent a fixed number of dimensions. If the right hand side is inferred,
the ellipsis dimensions are at the beginning of the output.</p></li>
<li><p><strong>operands</strong> (<em>list of Tensors</em>) – The operands to compute the Einstein sum of.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;i,j-&gt;ij&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># outer product</span>
<span class="go">tensor([[-0.0570, -0.0286, -0.0231,  0.0197],</span>
<span class="go">        [ 1.2616,  0.6335,  0.5113, -0.4351],</span>
<span class="go">        [ 1.4452,  0.7257,  0.5857, -0.4984],</span>
<span class="go">        [-0.4647, -0.2333, -0.1883,  0.1603],</span>
<span class="go">        [-1.1130, -0.5588, -0.4510,  0.3838]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bn,anm,bm-&gt;ba&#39;</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="c1"># compare torch.nn.functional.bilinear</span>
<span class="go">tensor([[-0.3430, -5.2405,  0.4494],</span>
<span class="go">        [ 0.3311,  5.5201, -3.0356]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="n">As</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Bs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bij,bjk-&gt;bik&#39;</span><span class="p">,</span> <span class="n">As</span><span class="p">,</span> <span class="n">Bs</span><span class="p">)</span> <span class="c1"># batch matrix multiplication</span>
<span class="go">tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],</span>
<span class="go">         [-1.6706, -0.8097, -0.8025, -2.1183]],</span>

<span class="go">        [[ 4.2239,  0.3107, -0.5756, -0.2354],</span>
<span class="go">         [-1.4558, -0.3460,  1.5087, -0.8530]],</span>

<span class="go">        [[ 2.8153,  1.8787, -4.3839, -1.2112],</span>
<span class="go">         [ 0.3728, -2.1131,  0.0921,  0.8305]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ii-&gt;i&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span> <span class="c1"># diagonal</span>
<span class="go">tensor([-0.7825,  0.8291, -0.1936])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ii-&gt;...i&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span> <span class="c1"># batch diagonal</span>
<span class="go">tensor([[-1.0864,  0.7292,  0.0569],</span>
<span class="go">        [-0.9725, -1.0270,  0.6493],</span>
<span class="go">        [ 0.5832, -1.1716, -1.5084],</span>
<span class="go">        [ 0.4041, -1.1690,  0.8570]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ij-&gt;...ji&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># batch permute</span>
<span class="go">torch.Size([2, 3, 5, 4])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.flatten">
<code class="descclassname">torch.</code><code class="descname">flatten</code><span class="sig-paren">(</span><em>input</em>, <em>start_dim=0</em>, <em>end_dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>Flattens a contiguous range of dims in a tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>start_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the first dim to flatten</p></li>
<li><p><strong>end_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the last dim to flatten</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="go">                       [3, 4]],</span>
<span class="go">                      [[5, 6],</span>
<span class="go">                       [7, 8]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="go">tensor([1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[1, 2, 3, 4],</span>
<span class="go">        [5, 6, 7, 8]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.flip">
<code class="descclassname">torch.</code><code class="descname">flip</code><span class="sig-paren">(</span><em>input</em>, <em>dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.flip" title="Permalink to this definition">¶</a></dt>
<dd><p>Reverse the order of a n-D tensor along given axis in dims.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>dims</strong> (<em>a list</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – axis to flip on</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[[ 0,  1],</span>
<span class="go">         [ 2,  3]],</span>

<span class="go">        [[ 4,  5],</span>
<span class="go">         [ 6,  7]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">tensor([[[ 6,  7],</span>
<span class="go">         [ 4,  5]],</span>

<span class="go">        [[ 2,  3],</span>
<span class="go">         [ 0,  1]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.rot90">
<code class="descclassname">torch.</code><code class="descname">rot90</code><span class="sig-paren">(</span><em>input</em>, <em>k</em>, <em>dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.rot90" title="Permalink to this definition">¶</a></dt>
<dd><p>Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.
Rotation direction is from the first towards the second axis if k &gt; 0, and from the second towards the first for k &lt; 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>k</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of times to rotate</p></li>
<li><p><strong>dims</strong> (<em>a list</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – axis to rotate</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[0, 1],</span>
<span class="go">        [2, 3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rot90</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">tensor([[1, 3],</span>
<span class="go">        [0, 2]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[[0, 1],</span>
<span class="go">         [2, 3]],</span>

<span class="go">        [[4, 5],</span>
<span class="go">         [6, 7]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rot90</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">tensor([[[1, 3],</span>
<span class="go">         [0, 2]],</span>

<span class="go">        [[5, 7],</span>
<span class="go">         [4, 6]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.histc">
<code class="descclassname">torch.</code><code class="descname">histc</code><span class="sig-paren">(</span><em>input</em>, <em>bins=100</em>, <em>min=0</em>, <em>max=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.histc" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the histogram of a tensor.</p>
<p>The elements are sorted into equal width bins between <a class="reference internal" href="#torch.min" title="torch.min"><code class="xref py py-attr docutils literal notranslate"><span class="pre">min</span></code></a> and
<a class="reference internal" href="#torch.max" title="torch.max"><code class="xref py py-attr docutils literal notranslate"><span class="pre">max</span></code></a>. If <a class="reference internal" href="#torch.min" title="torch.min"><code class="xref py py-attr docutils literal notranslate"><span class="pre">min</span></code></a> and <a class="reference internal" href="#torch.max" title="torch.max"><code class="xref py py-attr docutils literal notranslate"><span class="pre">max</span></code></a> are both zero, the minimum and
maximum values of the data are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>bins</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of histogram bins</p></li>
<li><p><strong>min</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – lower end of the range (inclusive)</p></li>
<li><p><strong>max</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – upper end of the range (inclusive)</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Histogram represented as a tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([ 0.,  2.,  1.,  0.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.meshgrid">
<code class="descclassname">torch.</code><code class="descname">meshgrid</code><span class="sig-paren">(</span><em>*tensors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.meshgrid" title="Permalink to this definition">¶</a></dt>
<dd><p>Take <span class="math">\(N\)</span> tensors, each of which can be either scalar or 1-dimensional
vector, and create <span class="math">\(N\)</span> N-dimensional grids, where the <span class="math">\(i\)</span> <sup>th</sup> grid is defined by
expanding the <span class="math">\(i\)</span> <sup>th</sup> input over dimensions defined by other inputs.</p>
<blockquote>
<div><dl class="simple">
<dt>Args:</dt><dd><p>tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be
treated as tensors of size <span class="math">\((1,)\)</span> automatically</p>
</dd>
<dt>Returns:</dt><dd><p>seq (sequence of Tensors): If the input has <span class="math">\(k\)</span> tensors of size
<span class="math">\((N_1,), (N_2,), \ldots , (N_k,)\)</span>, then the output would also has <span class="math">\(k\)</span> tensors,
where all tensors are of size <span class="math">\((N_1, N_2, \ldots , N_k)\)</span>.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grid_x</span><span class="p">,</span> <span class="n">grid_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grid_x</span>
<span class="go">tensor([[1, 1, 1],</span>
<span class="go">        [2, 2, 2],</span>
<span class="go">        [3, 3, 3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grid_y</span>
<span class="go">tensor([[4, 5, 6],</span>
<span class="go">        [4, 5, 6],</span>
<span class="go">        [4, 5, 6]])</span>
</pre></div>
</div>
</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="torch.renorm">
<code class="descclassname">torch.</code><code class="descname">renorm</code><span class="sig-paren">(</span><em>input</em>, <em>p</em>, <em>dim</em>, <em>maxnorm</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.renorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor where each sub-tensor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> along dimension
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is normalized such that the <cite>p</cite>-norm of the sub-tensor is lower
than the value <code class="xref py py-attr docutils literal notranslate"><span class="pre">maxnorm</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the norm of a row is lower than <cite>maxnorm</cite>, the row is unchanged</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the power for the norm computation</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to slice over to get the sub-tensors</p></li>
<li><p><strong>maxnorm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the maximum norm to keep each sub-tensor under</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([ 2.,  2.,  2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([ 3.,  3.,  3.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1.,  1.,  1.],</span>
<span class="go">        [ 2.,  2.,  2.],</span>
<span class="go">        [ 3.,  3.,  3.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">renorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[ 1.0000,  1.0000,  1.0000],</span>
<span class="go">        [ 1.6667,  1.6667,  1.6667],</span>
<span class="go">        [ 1.6667,  1.6667,  1.6667]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.roll">
<code class="descclassname">torch.</code><code class="descname">roll</code><span class="sig-paren">(</span><em>input</em>, <em>shifts</em>, <em>dims=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.roll" title="Permalink to this definition">¶</a></dt>
<dd><p>Roll the tensor along the given dimension(s). Elements that are shifted beyond the
last position are re-introduced at the first position. If a dimension is not
specified, the tensor will be flattened before rolling and then restored
to the original shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>shifts</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of python:ints</em>) – The number of places by which the elements
of the tensor are shifted. If shifts is a tuple, dims must be a tuple of
the same size, and each dimension will be rolled by the corresponding
value</p></li>
<li><p><strong>dims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of python:ints</em>) – Axis along which to roll</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[1, 2],</span>
<span class="go">        [3, 4],</span>
<span class="go">        [5, 6],</span>
<span class="go">        [7, 8]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[7, 8],</span>
<span class="go">        [1, 2],</span>
<span class="go">        [3, 4],</span>
<span class="go">        [5, 6]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[3, 4],</span>
<span class="go">        [5, 6],</span>
<span class="go">        [7, 8],</span>
<span class="go">        [1, 2]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="go">tensor([[6, 5],</span>
<span class="go">        [8, 7],</span>
<span class="go">        [2, 1],</span>
<span class="go">        [4, 3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.tensordot">
<code class="descclassname">torch.</code><code class="descname">tensordot</code><span class="sig-paren">(</span><em>a</em>, <em>b</em>, <em>dims=2</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.tensordot" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a contraction of a and b over multiple dimensions.</p>
<p><a class="reference internal" href="#torch.tensordot" title="torch.tensordot"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensordot</span></code></a> implements a generalizes the matrix product.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Left tensor to contract</p></li>
<li><p><strong>b</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Right tensor to contract</p></li>
<li><p><strong>dims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of two lists of python:integers</em>) – number of dimensions to
contract or explicit lists of dimensions for <code class="xref py py-attr docutils literal notranslate"><span class="pre">a</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">b</span></code> respectively</p></li>
</ul>
</dd>
</dl>
<p>When called with an integer argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">dims</span></code> = <span class="math">\(d\)</span>, and the number of
dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">a</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">b</span></code> is <span class="math">\(m\)</span> and <span class="math">\(n\)</span>, respectively,
it computes</p>
<div class="math">
\[r_{i_0,...,i_{m-d}, i_d,...,i_n}
  = \sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \times b_{k_0,...,k_{d-1}, i_d,...,i_n}.

\]</div>
<p>When called with <code class="xref py py-attr docutils literal notranslate"><span class="pre">dims</span></code> of the list form, the given dimensions will be contracted
in place of the last <span class="math">\(d\)</span> of <code class="xref py py-attr docutils literal notranslate"><span class="pre">a</span></code> and the first <span class="math">\(d\)</span> of <span class="math">\(b\)</span>. The sizes
in these dimensions must match, but <a class="reference internal" href="#torch.tensordot" title="torch.tensordot"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensordot</span></code></a> will deal with broadcasted
dimensions.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">60.</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">24.</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">tensor([[4400., 4730.],</span>
<span class="go">        [4532., 4874.],</span>
<span class="go">        [4664., 5018.],</span>
<span class="go">        [4796., 5162.],</span>
<span class="go">        [4928., 5306.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="go">tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],</span>
<span class="go">        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],</span>
<span class="go">        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.trace">
<code class="descclassname">torch.</code><code class="descname">trace</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.trace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the sum of the elements of the diagonal of the input 2-D matrix.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1.,  2.,  3.],</span>
<span class="go">        [ 4.,  5.,  6.],</span>
<span class="go">        [ 7.,  8.,  9.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor(15.)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.tril">
<code class="descclassname">torch.</code><code class="descname">tril</code><span class="sig-paren">(</span><em>input</em>, <em>diagonal=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.tril" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the other elements of the result tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> are set to 0.</p>
<p>The lower triangular part of the matrix is defined as the elements on and
below the diagonal.</p>
<p>The argument <a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code></a> controls which diagonal to consider. If
<a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code></a> = 0, all elements on and below the main diagonal are
retained. A positive value includes just as many diagonals above the main
diagonal, and similarly a negative value excludes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
<span class="math">\(\lbrace (i, i) \rbrace\)</span> for <span class="math">\(i \in [0, \min\{d_{1}, d_{2}\} - 1]\)</span> where
<span class="math">\(d_{1}, d_{2}\)</span> are the dimensions of the matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>diagonal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the diagonal to consider</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[-1.0813, -0.8619,  0.7105],</span>
<span class="go">        [ 0.0935,  0.1380,  2.2112],</span>
<span class="go">        [-0.3409, -0.9828,  0.0289]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[-1.0813,  0.0000,  0.0000],</span>
<span class="go">        [ 0.0935,  0.1380,  0.0000],</span>
<span class="go">        [-0.3409, -0.9828,  0.0289]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],</span>
<span class="go">        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],</span>
<span class="go">        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],</span>
<span class="go">        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],</span>
<span class="go">        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],</span>
<span class="go">        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.tril_indices">
<code class="descclassname">torch.</code><code class="descname">tril_indices</code><span class="sig-paren">(</span><em>row</em>, <em>column</em>, <em>offset=0</em>, <em>dtype=torch.long</em>, <em>device='cpu'</em>, <em>layout=torch.strided</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.tril_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the indices of the lower triangular part of a <code class="xref py py-attr docutils literal notranslate"><span class="pre">row</span></code>-by-
<code class="xref py py-attr docutils literal notranslate"><span class="pre">column</span></code> matrix in a 2-by-N Tensor, where the first row contains row
coordinates of all indices and the second row contains column coordinates.
Indices are ordered based on rows and then columns.</p>
<p>The lower triangular part of the matrix is defined as the elements on and
below the diagonal.</p>
<p>The argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> controls which diagonal to consider. If
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> = 0, all elements on and below the main diagonal are
retained. A positive value includes just as many diagonals above the main
diagonal, and similarly a negative value excludes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
<span class="math">\(\lbrace (i, i) \rbrace\)</span> for <span class="math">\(i \in [0, \min\{d_{1}, d_{2}\} - 1]\)</span>
where <span class="math">\(d_{1}, d_{2}\)</span> are the dimensions of the matrix.</p>
<p>NOTE: when running on ‘cuda’, row * col must be less than <span class="math">\(2^{59}\)</span> to
prevent overflow during calculation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>row</strong> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – number of rows in the 2-D matrix.</p></li>
<li><p><strong>column</strong> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – number of columns in the 2-D matrix.</p></li>
<li><p><strong>offset</strong> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – diagonal offset from the main diagonal.
Default: if not provided, 0.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.long</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – currently only support <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 1, 1, 2, 2, 2],</span>
<span class="go">        [0, 0, 1, 0, 1, 2]])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[1, 2, 2, 3, 3, 3],</span>
<span class="go">        [0, 0, 1, 0, 1, 2]])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],</span>
<span class="go">        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.triu">
<code class="descclassname">torch.</code><code class="descname">triu</code><span class="sig-paren">(</span><em>input</em>, <em>diagonal=0</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.triu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, the other elements of the result tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> are set to 0.</p>
<p>The upper triangular part of the matrix is defined as the elements on and
above the diagonal.</p>
<p>The argument <a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code></a> controls which diagonal to consider. If
<a class="reference internal" href="#torch.diagonal" title="torch.diagonal"><code class="xref py py-attr docutils literal notranslate"><span class="pre">diagonal</span></code></a> = 0, all elements on and below the main diagonal are
retained. A positive value excludes just as many diagonals above the main
diagonal, and similarly a negative value includes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
<span class="math">\(\lbrace (i, i) \rbrace\)</span> for <span class="math">\(i \in [0, \min\{d_{1}, d_{2}\} - 1]\)</span> where
<span class="math">\(d_{1}, d_{2}\)</span> are the dimensions of the matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>diagonal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the diagonal to consider</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.2309,  0.5207,  2.0049],</span>
<span class="go">        [ 0.2072, -1.0680,  0.6602],</span>
<span class="go">        [ 0.3480, -0.5211, -0.4573]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor([[ 0.2309,  0.5207,  2.0049],</span>
<span class="go">        [ 0.0000, -1.0680,  0.6602],</span>
<span class="go">        [ 0.0000,  0.0000, -0.4573]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.0000,  0.5207,  2.0049],</span>
<span class="go">        [ 0.0000,  0.0000,  0.6602],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.2309,  0.5207,  2.0049],</span>
<span class="go">        [ 0.2072, -1.0680,  0.6602],</span>
<span class="go">        [ 0.0000, -0.5211, -0.4573]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],</span>
<span class="go">        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],</span>
<span class="go">        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],</span>
<span class="go">        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],</span>
<span class="go">        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],</span>
<span class="go">        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],</span>
<span class="go">        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],</span>
<span class="go">        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.triu_indices">
<code class="descclassname">torch.</code><code class="descname">triu_indices</code><span class="sig-paren">(</span><em>row</em>, <em>column</em>, <em>offset=0</em>, <em>dtype=torch.long</em>, <em>device='cpu'</em>, <em>layout=torch.strided</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.triu_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the indices of the upper triangular part of a <code class="xref py py-attr docutils literal notranslate"><span class="pre">row</span></code> by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">column</span></code> matrix in a 2-by-N Tensor, where the first row contains row
coordinates of all indices and the second row contains column coordinates.
Indices are ordered based on rows and then columns.</p>
<p>The upper triangular part of the matrix is defined as the elements on and
above the diagonal.</p>
<p>The argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> controls which diagonal to consider. If
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offset</span></code> = 0, all elements on and above the main diagonal are
retained. A positive value excludes just as many diagonals above the main
diagonal, and similarly a negative value includes just as many diagonals below
the main diagonal. The main diagonal are the set of indices
<span class="math">\(\lbrace (i, i) \rbrace\)</span> for <span class="math">\(i \in [0, \min\{d_{1}, d_{2}\} - 1]\)</span>
where <span class="math">\(d_{1}, d_{2}\)</span> are the dimensions of the matrix.</p>
<p>NOTE: when running on ‘cuda’, row * col must be less than <span class="math">\(2^{59}\)</span> to
prevent overflow during calculation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>row</strong> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – number of rows in the 2-D matrix.</p></li>
<li><p><strong>column</strong> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – number of columns in the 2-D matrix.</p></li>
<li><p><strong>offset</strong> (<code class="docutils literal notranslate"><span class="pre">int</span></code>) – diagonal offset from the main diagonal.
Default: if not provided, 0.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired data type of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.long</span></code>.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <a class="reference internal" href="#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code></a>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p></li>
<li><p><strong>layout</strong> (<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>, optional) – currently only support <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 0, 0, 1, 1, 2],</span>
<span class="go">        [0, 1, 2, 1, 2, 2]])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],</span>
<span class="go">        [0, 1, 2, 0, 1, 2, 1, 2, 2]])</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 0, 1],</span>
<span class="go">        [1, 2, 2]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="blas-and-lapack-operations">
<h4>BLAS and LAPACK Operations<a class="headerlink" href="#blas-and-lapack-operations" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.addbmm">
<code class="descclassname">torch.</code><code class="descname">addbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.addbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a batch matrix-matrix product of matrices stored
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code>,
with a reduced add step (all matrix multiplications get accumulated
along the first dimension).
<code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> is added to the final result.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code> must be 3-D tensors each containing the
same number of matrices.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> is a <span class="math">\((b \times n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code> is a
<span class="math">\((b \times m \times p)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> must be
<span class="xref std std-ref">broadcastable</span> with a <span class="math">\((n \times p)\)</span> tensor
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be a <span class="math">\((n \times p)\)</span> tensor.</p>
<div class="math">
\[out = \beta\ \text{mat} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)

\]</div>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code>
must be real numbers, otherwise they should be integers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> (<span class="math">\(\beta\)</span>)</p></li>
<li><p><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be added</p></li>
<li><p><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <cite>batch1 &#64; batch2</cite> (<span class="math">\(\alpha\)</span>)</p></li>
<li><p><strong>batch1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first batch of matrices to be multiplied</p></li>
<li><p><strong>batch2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second batch of matrices to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addbmm</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">)</span>
<span class="go">tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],</span>
<span class="go">        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],</span>
<span class="go">        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addmm">
<code class="descclassname">torch.</code><code class="descname">addmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.addmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix multiplication of the matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.
The matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> is added to the final result.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> is a <span class="math">\((n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code> is a
<span class="math">\((m \times p)\)</span> tensor, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> must be
<span class="xref std std-ref">broadcastable</span> with a <span class="math">\((n \times p)\)</span> tensor
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be a <span class="math">\((n \times p)\)</span> tensor.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> are scaling factors on matrix-vector product between
<code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code> and the added matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> respectively.</p>
<div class="math">
\[\text{out} = \beta\ \text{mat} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)

\]</div>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> must be real numbers, otherwise they should be integers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> (<span class="math">\(\beta\)</span>)</p></li>
<li><p><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be added</p></li>
<li><p><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <span class="math">\(mat1 &#64; mat2\)</span> (<span class="math">\(\alpha\)</span>)</p></li>
<li><p><strong>mat1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first matrix to be multiplied</p></li>
<li><p><strong>mat2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second matrix to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mat1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mat2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">mat1</span><span class="p">,</span> <span class="n">mat2</span><span class="p">)</span>
<span class="go">tensor([[-4.8716,  1.4671, -1.3746],</span>
<span class="go">        [ 0.7573, -3.9555, -2.8681]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addmv">
<code class="descclassname">torch.</code><code class="descname">addmv</code><span class="sig-paren">(</span><em>beta=1</em>, <em>tensor</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.addmv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix-vector product of the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> and
the vector <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code>.
The vector <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> is added to the final result.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> is a <span class="math">\((n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code> is a 1-D tensor of
size <cite>m</cite>, then <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must be
<span class="xref std std-ref">broadcastable</span> with a 1-D tensor of size <cite>n</cite> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be 1-D tensor of size <cite>n</cite>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> are scaling factors on matrix-vector product between
<code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code> and the added tensor <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> respectively.</p>
<div class="math">
\[\text{out} = \beta\ \text{tensor} + \alpha\ (\text{mat} \mathbin{@} \text{vec})

\]</div>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> must be real numbers, otherwise they should be integers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> (<span class="math">\(\beta\)</span>)</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – vector to be added</p></li>
<li><p><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <span class="math">\(mat &#64; vec\)</span> (<span class="math">\(\alpha\)</span>)</p></li>
<li><p><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be multiplied</p></li>
<li><p><strong>vec</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – vector to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addmv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
<span class="go">tensor([-0.3768, -5.5565])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addr">
<code class="descclassname">torch.</code><code class="descname">addr</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.addr" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the outer-product of vectors <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code>
and adds it to the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code>.</p>
<p>Optional values <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> are scaling factors on the
outer product between <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code> and the added matrix
<code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> respectively.</p>
<div class="math">
\[\text{out} = \beta\ \text{mat} + \alpha\ (\text{vec1} \otimes \text{vec2})

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec1</span></code> is a vector of size <cite>n</cite> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code> is a vector
of size <cite>m</cite>, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> must be
<span class="xref std std-ref">broadcastable</span> with a matrix of size
<span class="math">\((n \times m)\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be a matrix of size
<span class="math">\((n \times m)\)</span>.</p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> must be real numbers, otherwise they should be integers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> (<span class="math">\(\beta\)</span>)</p></li>
<li><p><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be added</p></li>
<li><p><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <span class="math">\(\text{vec1} \otimes \text{vec2}\)</span> (<span class="math">\(\alpha\)</span>)</p></li>
<li><p><strong>vec1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first vector of the outer product</p></li>
<li><p><strong>vec2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second vector of the outer product</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vec1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addr</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 2.,  4.],</span>
<span class="go">        [ 3.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.baddbmm">
<code class="descclassname">torch.</code><code class="descname">baddbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.baddbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a batch matrix-matrix product of matrices in <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> is added to the final result.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code> must be 3-D tensors each containing the same
number of matrices.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> is a <span class="math">\((b \times n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code> is a
<span class="math">\((b \times m \times p)\)</span> tensor, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> must be
<span class="xref std std-ref">broadcastable</span> with a
<span class="math">\((b \times n \times p)\)</span> tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be a
<span class="math">\((b \times n \times p)\)</span> tensor. Both <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> mean the
same as the scaling factors used in <a class="reference internal" href="#torch.addbmm" title="torch.addbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.addbmm()</span></code></a>.</p>
<div class="math">
\[\text{out}_i = \beta\ \text{mat}_i + \alpha\ (\text{batch1}_i \mathbin{@} \text{batch2}_i)

\]</div>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">beta</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> must be real numbers, otherwise they should be integers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> (<span class="math">\(\beta\)</span>)</p></li>
<li><p><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to be added</p></li>
<li><p><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <span class="math">\(\text{batch1} \mathbin{&#64;} \text{batch2}\)</span> (<span class="math">\(\alpha\)</span>)</p></li>
<li><p><strong>batch1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first batch of matrices to be multiplied</p></li>
<li><p><strong>batch2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second batch of matrices to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">baddbmm</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.bmm">
<code class="descclassname">torch.</code><code class="descname">bmm</code><span class="sig-paren">(</span><em>batch1</em>, <em>batch2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.bmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a batch matrix-matrix product of matrices stored in <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code> must be 3-D tensors each containing
the same number of matrices.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch1</span></code> is a <span class="math">\((b \times n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch2</span></code> is a
<span class="math">\((b \times m \times p)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be a
<span class="math">\((b \times n \times p)\)</span> tensor.</p>
<div class="math">
\[\text{out}_i = \text{batch1}_i \mathbin{@} \text{batch2}_i

\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function does not <span class="xref std std-ref">broadcast</span>.
For broadcasting matrix products, see <a class="reference internal" href="#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first batch of matrices to be multiplied</p></li>
<li><p><strong>batch2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second batch of matrices to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">batch1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.btrifact">
<code class="descclassname">torch.</code><code class="descname">btrifact</code><span class="sig-paren">(</span><em>A</em>, <em>pivot=True) -&gt; (Tensor</em>, <em>IntTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.btrifact" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch LU factorization.</p>
<p>Returns a tuple containing the LU factorization and pivots. Pivoting is done if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">pivot</span></code> is set.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>LU factorization with <code class="xref py py-attr docutils literal notranslate"><span class="pre">pivot</span></code> = <code class="docutils literal notranslate"><span class="pre">True</span></code> is not available for CPU, and attempting
to do so will throw an error. However, LU factorization with <code class="xref py py-attr docutils literal notranslate"><span class="pre">pivot</span></code> = <code class="docutils literal notranslate"><span class="pre">True</span></code> is
available for CUDA.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to factor</p></li>
<li><p><strong>pivot</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether pivoting is done</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing factorization and pivots.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_LU</span><span class="p">,</span> <span class="n">pivots</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">btrifact</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_LU</span>
<span class="go">tensor([[[ 1.3506,  2.5558, -0.0816],</span>
<span class="go">         [ 0.1684,  1.1551,  0.1940],</span>
<span class="go">         [ 0.1193,  0.6189, -0.5497]],</span>

<span class="go">        [[ 0.4526,  1.2526, -0.3285],</span>
<span class="go">         [-0.7988,  0.7175, -0.9701],</span>
<span class="go">         [ 0.2634, -0.9255, -0.3459]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pivots</span>
<span class="go">tensor([[ 3,  3,  3],</span>
<span class="go">        [ 3,  3,  3]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.btrifact_with_info">
<code class="descclassname">torch.</code><code class="descname">btrifact_with_info</code><span class="sig-paren">(</span><em>A</em>, <em>pivot=True) -&gt; (Tensor</em>, <em>IntTensor</em>, <em>IntTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.btrifact_with_info" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch LU factorization with additional error information.</p>
<p>This is a version of <a class="reference internal" href="#torch.btrifact" title="torch.btrifact"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.btrifact()</span></code></a> that always creates an info
<cite>IntTensor</cite>, and returns it as the third return value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to factor</p></li>
<li><p><strong>pivot</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls whether pivoting is done</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing factorization, pivots, and an <cite>IntTensor</cite> where non-zero
values indicate whether factorization for each minibatch sample succeeds.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">btrifact_with_info</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">info</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LU factorization succeeded for all samples!&#39;</span><span class="p">)</span>
<span class="go">LU factorization succeeded for all samples!</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.btrisolve">
<code class="descclassname">torch.</code><code class="descname">btrisolve</code><span class="sig-paren">(</span><em>b</em>, <em>LU_data</em>, <em>LU_pivots</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.btrisolve" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch LU solve.</p>
<p>Returns the LU solve of the linear system <span class="math">\(Ax = b\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>b</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the RHS tensor</p></li>
<li><p><strong>LU_data</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the pivoted LU factorization of A from <a class="reference internal" href="#torch.btrifact" title="torch.btrifact"><code class="xref py py-meth docutils literal notranslate"><span class="pre">btrifact()</span></code></a>.</p></li>
<li><p><strong>LU_pivots</strong> (<em>IntTensor</em>) – the pivots of the LU factorization</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_LU</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">btrifact</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">btrisolve</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="o">*</span><span class="n">A_LU</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="o">-</span> <span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="go">tensor(1.00000e-07 *</span>
<span class="go">       2.8312)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.btriunpack">
<code class="descclassname">torch.</code><code class="descname">btriunpack</code><span class="sig-paren">(</span><em>LU_data</em>, <em>LU_pivots</em>, <em>unpack_data=True</em>, <em>unpack_pivots=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.btriunpack" title="Permalink to this definition">¶</a></dt>
<dd><p>Unpacks the data and pivots from a batched LU factorization (btrifact) of a tensor.</p>
<p>Returns a tuple of tensors as <code class="docutils literal notranslate"><span class="pre">(the</span> <span class="pre">pivots,</span> <span class="pre">the</span> <span class="pre">L</span> <span class="pre">tensor,</span> <span class="pre">the</span> <span class="pre">U</span> <span class="pre">tensor)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>LU_data</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the packed LU factorization data</p></li>
<li><p><strong>LU_pivots</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the packed LU factorization pivots</p></li>
<li><p><strong>unpack_data</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – flag indicating if the data should be unpacked</p></li>
<li><p><strong>unpack_pivots</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – flag indicating if the pivots should be unpacked</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_LU</span><span class="p">,</span> <span class="n">pivots</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">btrifact</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P</span><span class="p">,</span> <span class="n">A_L</span><span class="p">,</span> <span class="n">A_U</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">btriunpack</span><span class="p">(</span><span class="n">A_LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># can recover A from factorization</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">A_L</span><span class="p">,</span> <span class="n">A_U</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.chain_matmul">
<code class="descclassname">torch.</code><code class="descname">chain_matmul</code><span class="sig-paren">(</span><em>*matrices</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.chain_matmul" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the matrix product of the <span class="math">\(N\)</span> 2-D tensors. This product is efficiently computed
using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms
of arithmetic operations (<a class="reference external" href="https://mitpress.mit.edu/books/introduction-algorithms-third-edition">[CLRS]</a>). Note that since this is a function to compute the product, <span class="math">\(N\)</span>
needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.
If <span class="math">\(N\)</span> is 1, then this is a no-op - the original matrix is returned as is.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>matrices</strong> (<em>Tensors...</em>) – a sequence of 2 or more 2-D tensors whose product is to be determined.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>if the <span class="math">\(i^{th}\)</span> tensor was of dimensions <span class="math">\(p_{i} \times p_{i + 1}\)</span>, then the product
would be of dimensions <span class="math">\(p_{1} \times p_{N + 1}\)</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">chain_matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="go">tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],</span>
<span class="go">        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],</span>
<span class="go">        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cholesky">
<code class="descclassname">torch.</code><code class="descname">cholesky</code><span class="sig-paren">(</span><em>A</em>, <em>upper=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.cholesky" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Cholesky decomposition of a symmetric positive-definite
matrix <span class="math">\(A\)</span> or for batches of symmetric positive-definite matrices.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the returned matrix <cite>U</cite> is upper-triangular, and
the decomposition has the form:</p>
<div class="math">
\[A = U^TU\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the returned matrix <cite>L</cite> is lower-triangular, and
the decomposition has the form:</p>
<div class="math">
\[A = LL^T\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code> is a batch of symmetric positive-definite
matrices, then the returned tensor will be composed of upper-triangular Cholesky factors
of each of the individual matrices. Similarly, when <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the returned
tensor will be composed of lower-triangular Cholesky factors of each of the individual
matrices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of size (<a href="#id2"><span class="problematic" id="id3">*</span></a>, n, n) where <cite>*</cite> is zero or more
batch dimensions consisting of symmetric positive-definite matrices.</p></li>
<li><p><strong>upper</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – flag that indicates whether to return a
upper or lower triangular matrix. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output matrix</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">t</span><span class="p">())</span> <span class="c1"># make symmetric positive-definite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 2.4112, -0.7486,  1.4551],</span>
<span class="go">        [-0.7486,  1.3544,  0.1294],</span>
<span class="go">        [ 1.4551,  0.1294,  1.6724]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span>
<span class="go">tensor([[ 1.5528,  0.0000,  0.0000],</span>
<span class="go">        [-0.4821,  1.0592,  0.0000],</span>
<span class="go">        [ 0.9371,  0.5487,  0.7023]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
<span class="go">tensor([[ 2.4112, -0.7486,  1.4551],</span>
<span class="go">        [-0.7486,  1.3544,  0.1294],</span>
<span class="go">        [ 1.4551,  0.1294,  1.6724]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="mf">1e-03</span> <span class="c1"># make symmetric positive-definite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">a</span><span class="p">))</span> <span class="c1"># Max non-zero</span>
<span class="go">tensor(2.3842e-07)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cholesky_solve">
<code class="descclassname">torch.</code><code class="descname">cholesky_solve</code><span class="sig-paren">(</span><em>b</em>, <em>u</em>, <em>upper=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.cholesky_solve" title="Permalink to this definition">¶</a></dt>
<dd><p>Solves a linear system of equations with a positive semidefinite
matrix to be inverted given its Cholesky factor matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">u</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">u</span></code> is and lower triangular and <cite>c</cite> is
returned such that:</p>
<div class="math">
\[c = (u u^T)^{-1} b

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> or not provided, <code class="xref py py-attr docutils literal notranslate"><span class="pre">u</span></code> is upper triangular
and <cite>c</cite> is returned such that:</p>
<div class="math">
\[c = (u^T u)^{-1} b

\]</div>
<p><cite>torch.cholesky_solve(b, u)</cite> can take in 2D inputs <cite>b, u</cite> or inputs that are
batches of 2D matrices. If the inputs are batches, then returns
batched outputs <cite>c</cite></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> keyword only supports 2D matrix inputs, that is,
<cite>b, u</cite> must be 2D matrices.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>b</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input matrix of size <span class="math">\((*, m, k)\)</span>,
where <span class="math">\(*\)</span> is zero or more batch dimensions</p></li>
<li><p><strong>u</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input matrix of size <span class="math">\((*, m, m)\)</span>,
where <span class="math">\(*\)</span> is zero of more batch dimensions composed of
upper or lower triangular Cholesky factor</p></li>
<li><p><strong>upper</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to consider the Cholesky factor as a
lower or upper triangular matrix. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor for <cite>c</cite></p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">t</span><span class="p">())</span> <span class="c1"># make symmetric positive definite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 0.7747, -1.9549,  1.3086],</span>
<span class="go">        [-1.9549,  6.7546, -5.4114],</span>
<span class="go">        [ 1.3086, -5.4114,  4.8733]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([[-0.6355,  0.9891],</span>
<span class="go">        [ 0.1974,  1.4706],</span>
<span class="go">        [-0.4115, -0.6225]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>
<span class="go">tensor([[ -8.1625,  19.6097],</span>
<span class="go">        [ -5.8398,  14.2387],</span>
<span class="go">        [ -4.3771,  10.4173]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">inverse</span><span class="p">(),</span> <span class="n">b</span><span class="p">)</span>
<span class="go">tensor([[ -8.1626,  19.6097],</span>
<span class="go">        [ -5.8398,  14.2387],</span>
<span class="go">        [ -4.3771,  10.4173]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.dot">
<code class="descclassname">torch.</code><code class="descname">dot</code><span class="sig-paren">(</span><em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the dot product (inner product) of two tensors.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function does not <span class="xref std std-ref">broadcast</span>.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">tensor(7)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.eig">
<code class="descclassname">torch.</code><code class="descname">eig</code><span class="sig-paren">(</span><em>a</em>, <em>eigenvectors=False</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.eig" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the eigenvalues and eigenvectors of a real square matrix.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since eigenvalues and eigenvectors might be complex, backward pass is supported only</p>
</div>
<p>for <a class="reference internal" href="#torch.symeig" title="torch.symeig"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.symeig()</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the square matrix of shape <span class="math">\((n \times n)\)</span> for which the eigenvalues and eigenvectors
will be computed</p></li>
<li><p><strong>eigenvectors</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> to compute both eigenvalues and eigenvectors;
otherwise, only eigenvalues will be computed</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the output tensors</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A namedtuple (eigenvalues, eigenvectors) containing</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>eigenvalues</strong> (<em>Tensor</em>): Shape <span class="math">\((n \times 2)\)</span>. Each row is an eigenvalue of <code class="docutils literal notranslate"><span class="pre">a</span></code>,
where the first element is the real part and the second element is the imaginary part.
The eigenvalues are not necessarily ordered.</p></li>
<li><p><strong>eigenvectors</strong> (<em>Tensor</em>): If <code class="docutils literal notranslate"><span class="pre">eigenvectors=False</span></code>, it’s an empty tensor.
Otherwise, this tensor of shape <span class="math">\((n \times n)\)</span> can be used to compute normalized (unit length)
eigenvectors of corresponding eigenvalues as follows.
If the corresponding <cite>eigenvalues[j]</cite> is a real number, column <cite>eigenvectors[:, j]</cite> is the eigenvector
corresponding to <cite>eigenvalues[j]</cite>.
If the corresponding <cite>eigenvalues[j]</cite> and <cite>eigenvalues[j + 1]</cite> form a complex conjugate pair, then the
true eigenvectors can be computed as
<span class="math">\(\text{true eigenvector}[j] = eigenvectors[:, j] + i \times eigenvectors[:, j + 1]\)</span>,
<span class="math">\(\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \times eigenvectors[:, j + 1]\)</span>.</p></li>
</ul>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.gels">
<code class="descclassname">torch.</code><code class="descname">gels</code><span class="sig-paren">(</span><em>B</em>, <em>A</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.gels" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the solution to the least squares and least norm problems for a full
rank matrix <span class="math">\(A\)</span> of size <span class="math">\((m \times n)\)</span> and a matrix <span class="math">\(B\)</span> of
size <span class="math">\((m \times k)\)</span>.</p>
<p>If <span class="math">\(m \geq n\)</span>, <a class="reference internal" href="#torch.gels" title="torch.gels"><code class="xref py py-func docutils literal notranslate"><span class="pre">gels()</span></code></a> solves the least-squares problem:</p>
<div class="math">
\[\begin{array}{ll}
\min_X & \|AX-B\|_2.
\end{array}\]</div>
<p>If <span class="math">\(m &lt; n\)</span>, <a class="reference internal" href="#torch.gels" title="torch.gels"><code class="xref py py-func docutils literal notranslate"><span class="pre">gels()</span></code></a> solves the least-norm problem:</p>
<div class="math">
\[\begin{array}{ll}
\min_X & \|X\|_2 & \text{subject to} & AX = B.
\end{array}\]</div>
<p>Returned tensor <span class="math">\(X\)</span> has shape <span class="math">\((\max(m, n) \times k)\)</span>. The first <span class="math">\(n\)</span>
rows of <span class="math">\(X\)</span> contains the solution. If <span class="math">\(m \geq n\)</span>, the residual sum of squares
for the solution in each column is given by the sum of squares of elements in the
remaining <span class="math">\(m - n\)</span> rows of that column.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>B</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the matrix <span class="math">\(B\)</span></p></li>
<li><p><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the <span class="math">\(m\)</span> by <span class="math">\(n\)</span> matrix <span class="math">\(A\)</span></p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the optional destination tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A tuple containing:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>X</strong> (<em>Tensor</em>): the least squares solution</p></li>
<li><p><strong>qr</strong> (<em>Tensor</em>): the details of the QR factorization</p></li>
</ul>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned matrices will always be transposed, irrespective of the strides
of the input matrices. That is, they will have stride <cite>(1, m)</cite> instead of
<cite>(m, 1)</cite>.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">                      [2, 3, 4],</span>
<span class="go">                      [3, 5, 2],</span>
<span class="go">                      [4, 2, 5],</span>
<span class="go">                      [5, 4, 3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">10.</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span>
<span class="go">                      [ 12, 14],</span>
<span class="go">                      [ 14, 12],</span>
<span class="go">                      [ 16, 16],</span>
<span class="go">                      [ 18, 16]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gels</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">tensor([[  2.0000,   1.0000],</span>
<span class="go">        [  1.0000,   1.0000],</span>
<span class="go">        [  1.0000,   2.0000],</span>
<span class="go">        [ 10.9635,   4.8501],</span>
<span class="go">        [  8.9332,   5.2418]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.geqrf">
<code class="descclassname">torch.</code><code class="descname">geqrf</code><span class="sig-paren">(</span><em>input</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.geqrf" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a low-level function for calling LAPACK directly. This function
returns a namedtuple (a, tau) as defined in <a class="reference external" href="https://software.intel.com/en-us/node/521004">LAPACK documentation for geqrf</a> .</p>
<p>You’ll generally want to use <a class="reference internal" href="#torch.qr" title="torch.qr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.qr()</span></code></a> instead.</p>
<p>Computes a QR decomposition of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, but without constructing
<span class="math">\(Q\)</span> and <span class="math">\(R\)</span> as explicit separate matrices.</p>
<p>Rather, this directly calls the underlying LAPACK function <cite>?geqrf</cite>
which produces a sequence of ‘elementary reflectors’.</p>
<p>See <a class="reference external" href="https://software.intel.com/en-us/node/521004">LAPACK documentation for geqrf</a> for further details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input matrix</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the output tuple of (Tensor, Tensor)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.ger">
<code class="descclassname">torch.</code><code class="descname">ger</code><span class="sig-paren">(</span><em>vec1</em>, <em>vec2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.ger" title="Permalink to this definition">¶</a></dt>
<dd><p>Outer product of <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code>.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec1</span></code> is a vector of size <span class="math">\(n\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec2</span></code> is a vector of
size <span class="math">\(m\)</span>, then <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> must be a matrix of size <span class="math">\((n \times m)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function does not <span class="xref std std-ref">broadcast</span>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vec1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 1-D input vector</p></li>
<li><p><strong>vec2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 1-D input vector</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – optional output matrix</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ger</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>
<span class="go">tensor([[  1.,   2.,   3.],</span>
<span class="go">        [  2.,   4.,   6.],</span>
<span class="go">        [  3.,   6.,   9.],</span>
<span class="go">        [  4.,   8.,  12.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.gesv">
<code class="descclassname">torch.</code><code class="descname">gesv</code><span class="sig-paren">(</span><em>b</em>, <em>A</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.gesv" title="Permalink to this definition">¶</a></dt>
<dd><p>This function returns the solution to the system of linear equations represented
by <span class="math">\(AX = B\)</span> and the LU factorization of A, in order as a tuple <cite>X, LU</cite>.</p>
<p>For more information regarding <a class="reference internal" href="#torch.gesv" title="torch.gesv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gesv()</span></code></a>, please check <a class="reference internal" href="#torch.solve" title="torch.solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.solve()</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torch.gesv" title="torch.gesv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gesv()</span></code></a> is deprecated in favour of <a class="reference internal" href="#torch.solve" title="torch.solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.solve()</span></code></a> and will be removed in the
next release. Please use <a class="reference internal" href="#torch.solve" title="torch.solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.solve()</span></code></a> instead.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.inverse">
<code class="descclassname">torch.</code><code class="descname">inverse</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the inverse of the square matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> can be batches
of 2D square tensors, in which case this function would return a tensor composed of
individual inverses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Irrespective of the original strides, the returned tensors will be
transposed, i.e. with strides like <cite>input.contiguous().transpose(-2, -1).strides()</cite></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor of size (<a href="#id4"><span class="problematic" id="id5">*</span></a>, n, n) where <cite>*</cite> is zero or more
batch dimensions</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the optional output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span>
<span class="go">tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],</span>
<span class="go">        [ 0.0000,  1.0000,  0.0000,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000,  1.0000,  0.0000],</span>
<span class="go">        [ 0.0000, -0.0000, -0.0000,  1.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">4</span><span class="p">)))</span> <span class="c1"># Max non-zero</span>
<span class="go">tensor(1.1921e-07)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Batched inverse example</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span> <span class="c1"># Max non-zero</span>
<span class="go">tensor(1.9073e-06)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.det">
<code class="descclassname">torch.</code><code class="descname">det</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.det" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates determinant of a 2D square tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Backward through <a class="reference internal" href="#torch.det" title="torch.det"><code class="xref py py-meth docutils literal notranslate"><span class="pre">det()</span></code></a> internally uses SVD results when <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code> is
not invertible. In this case, double backward through <a class="reference internal" href="#torch.det" title="torch.det"><code class="xref py py-meth docutils literal notranslate"><span class="pre">det()</span></code></a> will be
unstable in when <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code> doesn’t have distinct singular values. See
<a class="reference internal" href="#torch.svd" title="torch.svd"><code class="xref py py-meth docutils literal notranslate"><span class="pre">svd()</span></code></a> for details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The input 2D square tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="go">tensor(3.7641)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.logdet">
<code class="descclassname">torch.</code><code class="descname">logdet</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.logdet" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates log determinant of a 2D square tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Result is <code class="docutils literal notranslate"><span class="pre">-inf</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code> has zero log determinant, and is <code class="docutils literal notranslate"><span class="pre">nan</span></code> if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code> has negative determinant.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Backward through <a class="reference internal" href="#torch.logdet" title="torch.logdet"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logdet()</span></code></a> internally uses SVD results when <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code>
is not invertible. In this case, double backward through <a class="reference internal" href="#torch.logdet" title="torch.logdet"><code class="xref py py-meth docutils literal notranslate"><span class="pre">logdet()</span></code></a> will
be unstable in when <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code> doesn’t have distinct singular values. See
<a class="reference internal" href="#torch.svd" title="torch.svd"><code class="xref py py-meth docutils literal notranslate"><span class="pre">svd()</span></code></a> for details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The input 2D square tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="go">tensor(0.2611)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="go">tensor(-1.3430)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.slogdet">
<code class="descclassname">torch.</code><code class="descname">slogdet</code><span class="sig-paren">(</span><em>A) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.slogdet" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the sign and log value of a 2D square tensor’s determinant.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">A</span></code> has zero determinant, this returns <code class="docutils literal notranslate"><span class="pre">(0,</span> <span class="pre">-inf)</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Backward through <a class="reference internal" href="#torch.slogdet" title="torch.slogdet"><code class="xref py py-meth docutils literal notranslate"><span class="pre">slogdet()</span></code></a> internally uses SVD results when <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code>
is not invertible. In this case, double backward through <a class="reference internal" href="#torch.slogdet" title="torch.slogdet"><code class="xref py py-meth docutils literal notranslate"><span class="pre">slogdet()</span></code></a>
will be unstable in when <code class="xref py py-attr docutils literal notranslate"><span class="pre">A</span></code> doesn’t have distinct singular values.
See <a class="reference internal" href="#torch.svd" title="torch.svd"><code class="xref py py-meth docutils literal notranslate"><span class="pre">svd()</span></code></a> for details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The input 2D square tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing the sign of the determinant, and the log value of the
absolute determinant.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="go">tensor(-4.8215)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="go">tensor(nan)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">slogdet</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="go">(tensor(-1.), tensor(1.5731))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.matmul">
<code class="descclassname">torch.</code><code class="descname">matmul</code><span class="sig-paren">(</span><em>tensor1</em>, <em>tensor2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.matmul" title="Permalink to this definition">¶</a></dt>
<dd><p>Matrix product of two tensors.</p>
<p>The behavior depends on the dimensionality of the tensors as follows:</p>
<ul class="simple">
<li><p>If both tensors are 1-dimensional, the dot product (scalar) is returned.</p></li>
<li><p>If both arguments are 2-dimensional, the matrix-matrix product is returned.</p></li>
<li><p>If the first argument is 1-dimensional and the second argument is 2-dimensional,
a 1 is prepended to its dimension for the purpose of the matrix multiply.
After the matrix multiply, the prepended dimension is removed.</p></li>
<li><p>If the first argument is 2-dimensional and the second argument is 1-dimensional,
the matrix-vector product is returned.</p></li>
<li><p>If both arguments are at least 1-dimensional and at least one argument is
N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first
argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
The non-matrix (i.e. batch) dimensions are <span class="xref std std-ref">broadcasted</span> (and thus
must be broadcastable).  For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor1</span></code> is a
<span class="math">\((j \times 1 \times n \times m)\)</span> tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor2</span></code> is a <span class="math">\((k \times m \times p)\)</span>
tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be an <span class="math">\((j \times k \times n \times p)\)</span> tensor.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The 1-dimensional dot product version of this function does not support an <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> parameter.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first tensor to be multiplied</p></li>
<li><p><strong>tensor2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second tensor to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># vector x vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># matrix x vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x broadcasted vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x batched matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># batched matrix x broadcasted matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.matrix_power">
<code class="descclassname">torch.</code><code class="descname">matrix_power</code><span class="sig-paren">(</span><em>input</em>, <em>n</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.matrix_power" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the matrix raised to the power <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> for square matrices.
For batch of matrices, each individual matrix is raised to the power <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> is negative, then the inverse of the matrix (if invertible) is
raised to the power <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code>.  For a batch of matrices, the batched inverse
(if invertible) is raised to the power <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> is 0, then an identity matrix
is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>n</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the power to raise the matrix to</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[[-1.9975, -1.9610],</span>
<span class="go">         [ 0.9592, -2.3364]],</span>

<span class="go">        [[-1.2534, -1.3429],</span>
<span class="go">         [ 0.4153, -1.4664]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matrix_power</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[[  3.9392, -23.9916],</span>
<span class="go">         [ 11.7357,  -0.2070]],</span>

<span class="go">        [[  0.2468,  -6.7168],</span>
<span class="go">         [  2.0774,  -0.8187]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.matrix_rank">
<code class="descclassname">torch.</code><code class="descname">matrix_rank</code><span class="sig-paren">(</span><em>input</em>, <em>tol=None</em>, <em>bool symmetric=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.matrix_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the numerical rank of a 2-D tensor. The method to compute the
matrix rank is done using SVD by default. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">symmetric</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is assumed to be symmetric, and the computation of the
rank is done by obtaining the eigenvalues.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">tol</span></code> is the threshold below which the singular values (or the eigenvalues
when <code class="xref py py-attr docutils literal notranslate"><span class="pre">symmetric</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>) are considered to be 0. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">tol</span></code> is not
specified, <code class="xref py py-attr docutils literal notranslate"><span class="pre">tol</span></code> is set to <code class="docutils literal notranslate"><span class="pre">S.max()</span> <span class="pre">*</span> <span class="pre">max(S.size())</span> <span class="pre">*</span> <span class="pre">eps</span></code> where <cite>S</cite> is the
singular values (or the eigenvalues when <code class="xref py py-attr docutils literal notranslate"><span class="pre">symmetric</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>), and <code class="docutils literal notranslate"><span class="pre">eps</span></code>
is the epsilon value for the datatype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2-D tensor</p></li>
<li><p><strong>tol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – the tolerance value. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – indicates whether <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is symmetric.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">tensor(10)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="go">tensor(9)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mm">
<code class="descclassname">torch.</code><code class="descname">mm</code><span class="sig-paren">(</span><em>mat1</em>, <em>mat2</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.mm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix multiplication of the matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> is a <span class="math">\((n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code> is a
<span class="math">\((m \times p)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be a <span class="math">\((n \times p)\)</span> tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function does not <span class="xref std std-ref">broadcast</span>.
For broadcasting matrix products, see <a class="reference internal" href="#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mat1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the first matrix to be multiplied</p></li>
<li><p><strong>mat2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second matrix to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mat1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mat2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">mat1</span><span class="p">,</span> <span class="n">mat2</span><span class="p">)</span>
<span class="go">tensor([[ 0.4851,  0.5037, -0.3633],</span>
<span class="go">        [-0.0760, -3.6705,  2.4784]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mv">
<code class="descclassname">torch.</code><code class="descname">mv</code><span class="sig-paren">(</span><em>mat</em>, <em>vec</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.mv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix-vector product of the matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> and the vector
<code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> is a <span class="math">\((n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">vec</span></code> is a 1-D tensor of
size <span class="math">\(m\)</span>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be 1-D of size <span class="math">\(n\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function does not <span class="xref std std-ref">broadcast</span>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – matrix to be multiplied</p></li>
<li><p><strong>vec</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – vector to be multiplied</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
<span class="go">tensor([ 1.0404, -0.6361])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.orgqr">
<code class="descclassname">torch.</code><code class="descname">orgqr</code><span class="sig-paren">(</span><em>a</em>, <em>tau</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.orgqr" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the orthogonal matrix <cite>Q</cite> of a QR factorization, from the <cite>(a, tau)</cite>
tuple returned by <a class="reference internal" href="#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a>.</p>
<p>This directly calls the underlying LAPACK function <cite>?orgqr</cite>.
See <a class="reference external" href="https://software.intel.com/en-us/mkl-developer-reference-c-orgqr">LAPACK documentation for orgqr</a> for further details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the <cite>a</cite> from <a class="reference internal" href="#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a>.</p></li>
<li><p><strong>tau</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the <cite>tau</cite> from <a class="reference internal" href="#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.ormqr">
<code class="descclassname">torch.</code><code class="descname">ormqr</code><span class="sig-paren">(</span><em>a</em>, <em>tau</em>, <em>mat</em>, <em>left=True</em>, <em>transpose=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.ormqr" title="Permalink to this definition">¶</a></dt>
<dd><p>Multiplies <cite>mat</cite> by the orthogonal <cite>Q</cite> matrix of the QR factorization
formed by <a class="reference internal" href="#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a> that is represented by <cite>(a, tau)</cite>.</p>
<p>This directly calls the underlying LAPACK function <cite>?ormqr</cite>.
See <a class="reference external" href="https://software.intel.com/en-us/mkl-developer-reference-c-ormqr">LAPACK documentation for ormqr</a> for further details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the <cite>a</cite> from <a class="reference internal" href="#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a>.</p></li>
<li><p><strong>tau</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the <cite>tau</cite> from <a class="reference internal" href="#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a>.</p></li>
<li><p><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the matrix to be multiplied.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.pinverse">
<code class="descclassname">torch.</code><code class="descname">pinverse</code><span class="sig-paren">(</span><em>input</em>, <em>rcond=1e-15</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.pinverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.
Please look at <a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose inverse</a> for more details</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method is implemented using the Singular Value Decomposition.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The pseudo-inverse is not necessarily a continuous function in the elements of the matrix <a class="reference external" href="https://epubs.siam.org/doi/10.1137/0117004">[1]</a>.
Therefore, derivatives are not always existent, and exist for a constant rank only <a class="reference external" href="https://www.jstor.org/stable/2156365">[2]</a>.
However, this method is backprop-able due to the implementation by using SVD results, and
could be unstable. Double-backward will also be unstable due to the usage of SVD internally.
See <a class="reference internal" href="#torch.svd" title="torch.svd"><code class="xref py py-meth docutils literal notranslate"><span class="pre">svd()</span></code></a> for more details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The input 2D tensor of dimensions <span class="math">\(m \times n\)</span></p></li>
<li><p><strong>rcond</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – A floating point value to determine the cutoff for small singular values.
Default: 1e-15</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The pseudo-inverse of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of dimensions <span class="math">\(n \times m\)</span></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],</span>
<span class="go">        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],</span>
<span class="go">        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[ 0.0600, -0.1933, -0.2090],</span>
<span class="go">        [-0.0903, -0.0817, -0.4752],</span>
<span class="go">        [-0.7124, -0.1631, -0.2272],</span>
<span class="go">        [ 0.1356,  0.3933, -0.5023],</span>
<span class="go">        [-0.0308, -0.1725, -0.5216]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.potrf">
<code class="descclassname">torch.</code><code class="descname">potrf</code><span class="sig-paren">(</span><em>a</em>, <em>upper=True</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.potrf" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Cholesky decomposition of a symmetric positive-definite
matrix <span class="math">\(A\)</span>.</p>
<p>For more information regarding <a class="reference internal" href="#torch.potrf" title="torch.potrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.potrf()</span></code></a>, please check <a class="reference internal" href="#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torch.potrf" title="torch.potrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.potrf()</span></code></a> is deprecated in favour of <a class="reference internal" href="#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a> and will be removed
in the next release. Please use <a class="reference internal" href="#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a> instead and note that the <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code>
argument in <a class="reference internal" href="#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a> defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.potri">
<code class="descclassname">torch.</code><code class="descname">potri</code><span class="sig-paren">(</span><em>u</em>, <em>upper=True</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.potri" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the inverse of a positive semidefinite matrix given its
Cholesky factor <code class="xref py py-attr docutils literal notranslate"><span class="pre">u</span></code>: returns matrix <cite>inv</cite></p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> or not provided, <code class="xref py py-attr docutils literal notranslate"><span class="pre">u</span></code> is upper
triangular such that the returned tensor is</p>
<div class="math">
\[inv = (u^T u)^{-1}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">u</span></code> is lower triangular
such that the returned tensor is</p>
<div class="math">
\[inv = (uu^{T})^{-1}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>u</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2-D tensor, a upper or lower triangular
Cholesky factor</p></li>
<li><p><strong>upper</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to return a upper (default) or lower triangular matrix</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor for <cite>inv</cite></p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">t</span><span class="p">())</span> <span class="c1"># make symmetric positive definite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[  0.9935,  -0.6353,   1.5806],</span>
<span class="go">        [ -0.6353,   0.8769,  -1.7183],</span>
<span class="go">        [  1.5806,  -1.7183,  10.6618]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">potri</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
<span class="go">tensor([[ 1.9314,  1.2251, -0.0889],</span>
<span class="go">        [ 1.2251,  2.4439,  0.2122],</span>
<span class="go">        [-0.0889,  0.2122,  0.1412]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span>
<span class="go">tensor([[ 1.9314,  1.2251, -0.0889],</span>
<span class="go">        [ 1.2251,  2.4439,  0.2122],</span>
<span class="go">        [-0.0889,  0.2122,  0.1412]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.potrs">
<code class="descclassname">torch.</code><code class="descname">potrs</code><span class="sig-paren">(</span><em>b</em>, <em>u</em>, <em>upper=True</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.potrs" title="Permalink to this definition">¶</a></dt>
<dd><p>Solves a linear system of equations with a positive semidefinite
matrix to be inverted given its Cholesky factor matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">u</span></code>.</p>
<p>For more information regarding <a class="reference internal" href="#torch.potrs" title="torch.potrs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.potrs()</span></code></a>, please check <a class="reference internal" href="#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torch.potrs" title="torch.potrs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.potrs()</span></code></a> is deprecated in favour of <a class="reference internal" href="#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a> and will be
removed in the next release. Please use <a class="reference internal" href="#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a> instead and note that
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> argument in <a class="reference internal" href="#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a> defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.pstrf">
<code class="descclassname">torch.</code><code class="descname">pstrf</code><span class="sig-paren">(</span><em>a</em>, <em>upper=True</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.pstrf" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the pivoted Cholesky decomposition of a symmetric positive-definite
matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">a</span></code>. returns a namedtuple (u, pivot) of matrice.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> or not provided, <cite>u</cite> is upper triangular
such that <span class="math">\(a = p^T u^T u p\)</span>, with <cite>p</cite> the permutation given by <cite>pivot</cite>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, <cite>u</cite> is lower triangular such that
<span class="math">\(a = p^T u u^T p\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torch.pstrf" title="torch.pstrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pstrf()</span></code></a> is deprecated in favour of <a class="reference internal" href="#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a> and will
be removed in the next release.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2-D tensor</p></li>
<li><p><strong>upper</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to return a upper (default) or lower triangular matrix</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – namedtuple of <cite>u</cite> and <cite>pivot</cite> tensors</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">t</span><span class="p">())</span> <span class="c1"># make symmetric positive definite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[ 3.5405, -0.4577,  0.8342],</span>
<span class="go">        [-0.4577,  1.8244, -0.1996],</span>
<span class="go">        [ 0.8342, -0.1996,  3.7493]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">u</span><span class="p">,</span><span class="n">piv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pstrf</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">u</span>
<span class="go">tensor([[ 1.9363,  0.4308, -0.1031],</span>
<span class="go">        [ 0.0000,  1.8316, -0.2256],</span>
<span class="go">        [ 0.0000,  0.0000,  1.3277]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">piv</span>
<span class="go">tensor([ 2,  0,  1], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">piv</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">piv</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="c1"># make pivot permutation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span><span class="n">u</span><span class="p">)),</span><span class="n">p</span><span class="p">)</span> <span class="c1"># reconstruct</span>
<span class="go">tensor([[ 3.5405, -0.4577,  0.8342],</span>
<span class="go">        [-0.4577,  1.8244, -0.1996],</span>
<span class="go">        [ 0.8342, -0.1996,  3.7493]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.qr">
<code class="descclassname">torch.</code><code class="descname">qr</code><span class="sig-paren">(</span><em>input</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.qr" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the QR decomposition of a matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, and returns a namedtuple
(Q, R) of matrices such that <span class="math">\(\text{input} = Q R\)</span>, with <span class="math">\(Q\)</span> being an
orthogonal matrix and <span class="math">\(R\)</span> being an upper triangular matrix.</p>
<p>This returns the thin (reduced) QR factorization.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>precision may be lost if the magnitudes of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
are large</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While it should always give you a valid decomposition, it may not
give you the same one across platforms - it will depend on your
LAPACK implementation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Irrespective of the original strides, the returned matrix <span class="math">\(Q\)</span> will be
transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2-D tensor</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – tuple of <cite>Q</cite> and <cite>R</cite> tensors</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">12.</span><span class="p">,</span> <span class="o">-</span><span class="mi">51</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">167</span><span class="p">,</span> <span class="o">-</span><span class="mi">68</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="o">-</span><span class="mi">41</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span>
<span class="go">tensor([[-0.8571,  0.3943,  0.3314],</span>
<span class="go">        [-0.4286, -0.9029, -0.0343],</span>
<span class="go">        [ 0.2857, -0.1714,  0.9429]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span>
<span class="go">tensor([[ -14.0000,  -21.0000,   14.0000],</span>
<span class="go">        [   0.0000, -175.0000,   70.0000],</span>
<span class="go">        [   0.0000,    0.0000,  -35.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
<span class="go">tensor([[  12.,  -51.,    4.],</span>
<span class="go">        [   6.,  167.,  -68.],</span>
<span class="go">        [  -4.,   24.,  -41.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">q</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
<span class="go">tensor([[ 1.,  0.,  0.],</span>
<span class="go">        [ 0.,  1., -0.],</span>
<span class="go">        [ 0., -0.,  1.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.solve">
<code class="descclassname">torch.</code><code class="descname">solve</code><span class="sig-paren">(</span><em>B</em>, <em>A</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.solve" title="Permalink to this definition">¶</a></dt>
<dd><p>This function returns the solution to the system of linear
equations represented by <span class="math">\(AX = B\)</span> and the LU factorization of
A, in order as a tuple <cite>X, LU</cite>.</p>
<p><cite>LU</cite> contains <cite>L</cite> and <cite>U</cite> factors for LU factorization of <cite>A</cite>.</p>
<p><cite>torch.solve(B, A)</cite> can take in 2D inputs <cite>B, A</cite> or inputs that are
batches of 2D matrices. If the inputs are batches, then returns
batched outputs <cite>X, LU</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Irrespective of the original strides, the returned matrices
<cite>X</cite> and <cite>LU</cite> will be transposed, i.e. with strides like
<cite>B.contiguous().transpose(-1, -2).strides()</cite> and
<cite>A.contiguous().transpose(-1, -2).strides()</cite> respectively.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>B</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input matrix of size <span class="math">\((*, m, k)\)</span> , where <span class="math">\(*\)</span>
is zero or more batch dimensions.</p></li>
<li><p><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input square matrix of size <span class="math">\((*, m, m)\)</span>, where
<span class="math">\(*\)</span> is zero or more batch dimensions.</p></li>
<li><p><strong>out</strong> (<em>(</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>)</em><em>, </em><em>optional</em>) – optional output tuple.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">6.80</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.11</span><span class="p">,</span>  <span class="mf">5.66</span><span class="p">,</span>  <span class="mf">5.97</span><span class="p">,</span>  <span class="mf">8.23</span><span class="p">],</span>
<span class="go">                      [-6.05, -3.30,  5.36, -4.44,  1.08],</span>
<span class="go">                      [-0.45,  2.58, -2.70,  0.27,  9.04],</span>
<span class="go">                      [8.32,  2.71,  4.35,  -7.17,  2.14],</span>
<span class="go">                      [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">4.02</span><span class="p">,</span>  <span class="mf">6.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.22</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.57</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.03</span><span class="p">],</span>
<span class="go">                      [-1.56,  4.00, -8.67,  1.75,  2.86],</span>
<span class="go">                      [9.81, -4.09, -4.57, -8.61,  8.99]]).t()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">LU</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="go">tensor(1.00000e-06 *</span>
<span class="go">       7.0977)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Batched solver example</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">LU</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">tensor(1.00000e-06 *</span>
<span class="go">   3.6386)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.svd">
<code class="descclassname">torch.</code><code class="descname">svd</code><span class="sig-paren">(</span><em>input</em>, <em>some=True</em>, <em>compute_uv=True</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.svd" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">svd(A)</span></code> returns a namedtuple <code class="docutils literal notranslate"><span class="pre">(U,</span> <span class="pre">S,</span> <span class="pre">V)</span></code> which the singular value
decomposition of a input real matrix <cite>A</cite> of size <cite>(n x m)</cite> such that
<span class="math">\(A = USV^T\)</span>.</p>
<p><cite>U</cite> is of shape <span class="math">\((n \times n)\)</span>.</p>
<p><cite>S</cite> is a diagonal matrix of shape <span class="math">\((n \times m)\)</span>, represented as a vector
of size <span class="math">\(\min(n, m)\)</span> containing the non-negative diagonal entries.</p>
<p><cite>V</cite> is of shape <span class="math">\((m \times m)\)</span>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">some</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), the returned <cite>U</cite> and <cite>V</cite> matrices will
contain only <span class="math">\(min(n, m)\)</span> orthonormal columns.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">compute_uv</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the returned <cite>U</cite> and <cite>V</cite> matrices will be zero matrices
of shape <span class="math">\((n \times n)\)</span> and <span class="math">\((m \times m)\)</span> respectively. <code class="xref py py-attr docutils literal notranslate"><span class="pre">some</span></code> will be ignored here.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The implementation of SVD on CPU uses the LAPACK routine <cite>?gesdd</cite> (a divide-and-conquer
algorithm) instead of <cite>?gesvd</cite> for speed. Analogously, the SVD on GPU uses the MAGMA routine
<cite>gesdd</cite> as well.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Irrespective of the original strides, the returned matrix <cite>U</cite>
will be transposed, i.e. with strides <cite>(1, n)</cite> instead of <cite>(n, 1)</cite>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Extra care needs to be taken when backward through <cite>U</cite> and <cite>V</cite>
outputs. Such operation is really only stable when <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is
full rank with all distinct singular values. Otherwise, <code class="docutils literal notranslate"><span class="pre">NaN</span></code> can
appear as the gradients are not properly defined. Also, notice that
double backward will usually do an additional backward through <cite>U</cite> and
<cite>V</cite> even if the original backward is only on <cite>S</cite>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">some</span></code> = <code class="docutils literal notranslate"><span class="pre">False</span></code>, the gradients on <code class="docutils literal notranslate"><span class="pre">U[:,</span> <span class="pre">min(n,</span> <span class="pre">m):]</span></code>
and <code class="docutils literal notranslate"><span class="pre">V[:,</span> <span class="pre">min(n,</span> <span class="pre">m):]</span></code> will be ignored in backward as those vectors
can be arbitrary bases of the subspaces.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">compute_uv</span></code> = <code class="docutils literal notranslate"><span class="pre">False</span></code>, backward cannot be performed since <code class="docutils literal notranslate"><span class="pre">U</span></code> and <code class="docutils literal notranslate"><span class="pre">V</span></code>
from the forward pass is required for the backward operation.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input 2-D tensor</p></li>
<li><p><strong>some</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – controls the shape of returned <cite>U</cite> and <cite>V</cite></p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the output tuple of tensors</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">8.79</span><span class="p">,</span>  <span class="mf">6.11</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.15</span><span class="p">,</span>  <span class="mf">9.57</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.49</span><span class="p">,</span>  <span class="mf">9.84</span><span class="p">],</span>
<span class="go">                      [9.93,  6.91, -7.93,  1.64,  4.02,  0.15],</span>
<span class="go">                      [9.83,  5.04,  4.86,  8.83,  9.80, -8.99],</span>
<span class="go">                      [5.45, -0.27,  4.85,  0.74, 10.00, -6.02],</span>
<span class="go">                      [3.16,  7.98,  3.01,  5.80,  4.27, -5.31]]).t()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="vm">__class__</span>
<span class="go">&lt;class &#39;torch.return_types.svd&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">u</span>
<span class="go">tensor([[-0.5911,  0.2632,  0.3554,  0.3143,  0.2299],</span>
<span class="go">        [-0.3976,  0.2438, -0.2224, -0.7535, -0.3636],</span>
<span class="go">        [-0.0335, -0.6003, -0.4508,  0.2334, -0.3055],</span>
<span class="go">        [-0.4297,  0.2362, -0.6859,  0.3319,  0.1649],</span>
<span class="go">        [-0.4697, -0.3509,  0.3874,  0.1587, -0.5183],</span>
<span class="go">        [ 0.2934,  0.5763, -0.0209,  0.3791, -0.6526]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>
<span class="go">tensor([ 27.4687,  22.6432,   8.5584,   5.9857,   2.0149])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span>
<span class="go">tensor([[-0.2514,  0.8148, -0.2606,  0.3967, -0.2180],</span>
<span class="go">        [-0.3968,  0.3587,  0.7008, -0.4507,  0.1402],</span>
<span class="go">        [-0.6922, -0.2489, -0.2208,  0.2513,  0.5891],</span>
<span class="go">        [-0.3662, -0.3686,  0.3859,  0.4342, -0.6265],</span>
<span class="go">        [-0.4076, -0.0980, -0.4933, -0.6227, -0.4396]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)),</span> <span class="n">v</span><span class="o">.</span><span class="n">t</span><span class="p">()))</span>
<span class="go">tensor(1.00000e-06 *</span>
<span class="go">       9.3738)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.symeig">
<code class="descclassname">torch.</code><code class="descname">symeig</code><span class="sig-paren">(</span><em>input</em>, <em>eigenvectors=False</em>, <em>upper=True</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.symeig" title="Permalink to this definition">¶</a></dt>
<dd><p>This function returns eigenvalues and eigenvectors
of a real symmetric matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, represented by a namedtuple
(eigenvalues, eigenvectors).</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <span class="math">\(V\)</span> are <span class="math">\((m \times m)\)</span> matrices and <span class="math">\(e\)</span> is a
<span class="math">\(m\)</span> dimensional vector.</p>
<p>This function calculates all eigenvalues (and vectors) of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
such that <span class="math">\(\text{input} = V \text{diag}(e) V^T\)</span>.</p>
<p>The boolean argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">eigenvectors</span></code> defines computation of
eigenvectors or eigenvalues only.</p>
<p>If it is <code class="docutils literal notranslate"><span class="pre">False</span></code>, only eigenvalues are computed. If it is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
both eigenvalues and eigenvectors are computed.</p>
<p>Since the input matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is supposed to be symmetric,
only the upper triangular portion is used by default.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">upper</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then lower triangular portion is used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Irrespective of the original strides, the returned matrix <cite>V</cite> will
be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Extra care needs to be taken when backward through outputs. Such
operation is really only stable when all eigenvalues are distinct.
Otherwise, <code class="docutils literal notranslate"><span class="pre">NaN</span></code> can appear as the gradients are not properly defined.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input symmetric matrix</p></li>
<li><p><strong>eigenvectors</strong> (<em>boolean</em><em>, </em><em>optional</em>) – controls whether eigenvectors have to be computed</p></li>
<li><p><strong>upper</strong> (<em>boolean</em><em>, </em><em>optional</em>) – controls whether to consider upper-triangular or lower-triangular region</p></li>
<li><p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the output tuple of (Tensor, Tensor)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>A namedtuple (eigenvalues, eigenvectors) containing</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>eigenvalues</strong> (<em>Tensor</em>): Shape <span class="math">\((m)\)</span>. Each element is an eigenvalue of <code class="docutils literal notranslate"><span class="pre">input</span></code>,
The eigenvalues are in ascending order.</p></li>
<li><p><strong>eigenvectors</strong> (<em>Tensor</em>): Shape <span class="math">\((m \times m)\)</span>.
If <code class="docutils literal notranslate"><span class="pre">eigenvectors=False</span></code>, it’s a tensor filled with zeros.
Otherwise, this tensor contains the orthonormal eigenvectors of the <code class="docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
</ul>
</div></blockquote>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.96</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">],</span>
<span class="go">                      [-6.49,  3.80,  0.00,  0.00,  0.00],</span>
<span class="go">                      [-0.47, -6.39,  4.17,  0.00,  0.00],</span>
<span class="go">                      [-7.20,  1.50, -1.51,  5.70,  0.00],</span>
<span class="go">                      [-0.65, -6.34,  2.67,  1.80, -7.10]]).t()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">symeig</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>
<span class="go">tensor([-11.0656,  -6.2287,   0.8640,   8.8655,  16.0948])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span>
<span class="go">tensor([[-0.2981, -0.6075,  0.4026, -0.3745,  0.4896],</span>
<span class="go">        [-0.5078, -0.2880, -0.4066, -0.3572, -0.6053],</span>
<span class="go">        [-0.0816, -0.3843, -0.6600,  0.5008,  0.3991],</span>
<span class="go">        [-0.0036, -0.4467,  0.4553,  0.6204, -0.4564],</span>
<span class="go">        [-0.8041,  0.4480,  0.1725,  0.3108,  0.1622]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.trtrs">
<code class="descclassname">torch.</code><code class="descname">trtrs</code><span class="sig-paren">(</span><em>b</em>, <em>A</em>, <em>upper=True</em>, <em>transpose=False</em>, <em>unitriangular=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.trtrs" title="Permalink to this definition">¶</a></dt>
<dd><p>Solves a system of equations with a triangular coefficient matrix <span class="math">\(A\)</span>
and multiple right-hand sides <code class="xref py py-attr docutils literal notranslate"><span class="pre">b</span></code>.</p>
<p>In particular, solves <span class="math">\(AX = b\)</span> and assumes <span class="math">\(A\)</span> is upper-triangular
with the default keyword arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input triangular coefficient matrix</p></li>
<li><p><strong>b</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – multiple right-hand sides. Each column of <span class="math">\(b\)</span> is a
right-hand side for the system of equations.</p></li>
<li><p><strong>upper</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to solve the upper-triangular system
of equations (default) or the lower-triangular system of equations. Default: True.</p></li>
<li><p><strong>transpose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether <span class="math">\(A\)</span> should be transposed before
being sent into the solver. Default: False.</p></li>
<li><p><strong>unitriangular</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether <span class="math">\(A\)</span> is unit triangular.
If True, the diagonal elements of <span class="math">\(A\)</span> are assumed to be
1 and not referenced from <span class="math">\(A\)</span>. Default: False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple <span class="math">\((X, M)\)</span> where <span class="math">\(M\)</span> is a clone of <span class="math">\(A\)</span> and <span class="math">\(X\)</span>
is the solution to <span class="math">\(AX = b\)</span> (or whatever variant of the system of
equations, depending on the keyword arguments.)</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>A: <span class="math">\((N, N)\)</span></p></li>
<li><p>b: <span class="math">\((N, C)\)</span></p></li>
<li><p>output[0]: <span class="math">\((N, C)\)</span></p></li>
<li><p>output[1]: <span class="math">\((N, N)\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">triu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span>
<span class="go">tensor([[ 1.1527, -1.0753],</span>
<span class="go">        [ 0.0000,  0.7986]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([[-0.0210,  2.3513, -1.5492],</span>
<span class="go">        [ 1.5429,  0.7403, -1.0243]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">trtrs</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="go">(tensor([[ 1.7840,  2.9045, -2.5405],</span>
<span class="go">        [ 1.9319,  0.9269, -1.2826]]), tensor([[ 1.1527, -1.0753],</span>
<span class="go">        [ 0.0000,  0.7986]]))</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="utilities">
<h3>Utilities<a class="headerlink" href="#utilities" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.compiled_with_cxx11_abi">
<code class="descclassname">torch.</code><code class="descname">compiled_with_cxx11_abi</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.compiled_with_cxx11_abi" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1</p>
</dd></dl>

</div>
</div>
<span id="document-tensors"></span><div class="section" id="torch-tensor">
<span id="tensor-doc"></span><h2>torch.Tensor<a class="headerlink" href="#torch-tensor" title="Permalink to this headline">¶</a></h2>
<p>A <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> is a multi-dimensional matrix containing elements of
a single data type.</p>
<p>Torch defines eight CPU tensor types and eight GPU tensor types:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 19%" />
<col style="width: 34%" />
<col style="width: 21%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Data type</p></th>
<th class="head"><p>dtype</p></th>
<th class="head"><p>CPU tensor</p></th>
<th class="head"><p>GPU tensor</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.FloatTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.double</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.DoubleTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.DoubleTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.half</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.HalfTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.HalfTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>8-bit integer (unsigned)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code></p></td>
<td><p><a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code></a></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.ByteTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>8-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int8</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.CharTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.CharTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>16-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.short</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ShortTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.ShortTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>32-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.int</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.IntTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.IntTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.long</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.LongTensor</span></code></p></td>
<td><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.cuda.LongTensor</span></code></p></td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> is an alias for the default tensor type (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>).</p>
<p>A tensor can be constructed from a Python <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a> or sequence using the
<a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> constructor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]])</span>
<span class="go">tensor([[ 1.0000, -1.0000],</span>
<span class="go">        [ 1.0000, -1.0000]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]))</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> and just want to change its <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> flag, use
<a class="reference internal" href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">requires_grad_()</span></code></a> or
<a class="reference internal" href="index.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">detach()</span></code></a> to avoid a copy.
If you have a numpy array and want to avoid a copy, use
<a class="reference internal" href="index.html#torch.as_tensor" title="torch.as_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_tensor()</span></code></a>.</p>
</div>
<p>A tensor of specific data type can be constructed by passing a
<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and/or a <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> to a
constructor or tensor creation op:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">tensor([[ 0,  0,  0,  0],</span>
<span class="go">        [ 0,  0,  0,  0]], dtype=torch.int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],</span>
<span class="go">        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
<p>The contents of a tensor can be accessed and modified using Python’s indexing
and slicing notation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="go">tensor(6)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 1,  8,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
</pre></div>
</div>
<p>Use <a class="reference internal" href="#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.item()</span></code></a> to get a Python number from a tensor containing a
single value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor(2.5000)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">2.5</span>
</pre></div>
</div>
<p>A tensor can be created with <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad=True</span></code> so that
<a class="reference internal" href="index.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a> records operations on them for automatic differentiation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[ 2.0000, -2.0000],</span>
<span class="go">        [ 2.0000,  2.0000]])</span>
</pre></div>
</div>
<p>Each tensor has an associated <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Storage</span></code>, which holds its data.
The tensor class provides multi-dimensional, <a class="reference external" href="https://en.wikipedia.org/wiki/Stride_of_an_array">strided</a>
view of a storage and defines numeric operations on it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on the <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, and
<a class="reference internal" href="index.html#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a> attributes of a <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>, see
<a class="reference internal" href="index.html#tensor-attributes-doc"><span class="std std-ref">Tensor Attributes</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Methods which mutate a tensor are marked with an underscore suffix.
For example, <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.FloatTensor.abs_()</span></code> computes the absolute value
in-place and returns the modified tensor, while <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.FloatTensor.abs()</span></code>
computes the result in a new tensor.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To change an existing tensor’s <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> and/or <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, consider using
<a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code></a> method on the tensor.</p>
</div>
<dl class="class">
<dt id="torch.Tensor">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><a class="headerlink" href="#torch.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>There are a few main ways to create a tensor, depending on your use case.</p>
<ul class="simple">
<li><p>To create a tensor with pre-existing data, use <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code></a>.</p></li>
<li><p>To create a tensor with specific size, use <code class="docutils literal notranslate"><span class="pre">torch.*</span></code> tensor creation
ops (see <a class="reference internal" href="index.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a>).</p></li>
<li><p>To create a tensor with the same size (and similar types) as another tensor,
use <code class="docutils literal notranslate"><span class="pre">torch.*_like</span></code> tensor creation ops
(see <a class="reference internal" href="index.html#tensor-creation-ops"><span class="std std-ref">Creation Ops</span></a>).</p></li>
<li><p>To create a tensor with similar type but different size as another tensor,
use <code class="docutils literal notranslate"><span class="pre">tensor.new_*</span></code> creation ops.</p></li>
</ul>
<dl class="method">
<dt id="torch.Tensor.new_tensor">
<code class="descname">new_tensor</code><span class="sig-paren">(</span><em>data</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code> as the tensor data.
By default, the returned Tensor has the same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><a class="reference internal" href="#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="docutils literal notranslate"><span class="pre">data</span></code> and want to avoid a copy, use <a class="reference internal" href="#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code></a>
or <a class="reference internal" href="index.html#torch.Tensor.detach" title="torch.Tensor.detach"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.detach()</span></code></a>.
If you have a numpy array and want to avoid a copy, use
<a class="reference internal" href="index.html#torch.from_numpy" title="torch.from_numpy"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.from_numpy()</span></code></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When data is a tensor <cite>x</cite>, <a class="reference internal" href="#torch.Tensor.new_tensor" title="torch.Tensor.new_tensor"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_tensor()</span></code></a> reads out ‘the data’ from whatever it is passed,
and constructs a leaf variable. Therefore <code class="docutils literal notranslate"><span class="pre">tensor.new_tensor(x)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach()</span></code>
and <code class="docutils literal notranslate"><span class="pre">tensor.new_tensor(x,</span> <span class="pre">requires_grad=True)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach().requires_grad_(True)</span></code>.
The equivalents using <code class="docutils literal notranslate"><span class="pre">clone()</span></code> and <code class="docutils literal notranslate"><span class="pre">detach()</span></code> are recommended.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>array_like</em>) – The returned Tensor copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="go">tensor([[ 0,  1],</span>
<span class="go">        [ 2,  3]], dtype=torch.int8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_full">
<code class="descname">new_full</code><span class="sig-paren">(</span><em>size</em>, <em>fill_value</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_full" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="xref py py-attr docutils literal notranslate"><span class="pre">fill_value</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fill_value</strong> (<em>scalar</em>) – the number to fill the output tensor with.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="mf">3.141592</span><span class="p">)</span>
<span class="go">tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416],</span>
<span class="go">        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_empty">
<code class="descname">new_empty</code><span class="sig-paren">(</span><em>size</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with uninitialized data.
By default, the returned Tensor has the same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],</span>
<span class="go">        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_ones">
<code class="descname">new_ones</code><span class="sig-paren">(</span><em>size</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">1</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int...</em>) – a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the
shape of the output tensor.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 1,  1,  1],</span>
<span class="go">        [ 1,  1,  1]], dtype=torch.int32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new_zeros">
<code class="descname">new_zeros</code><span class="sig-paren">(</span><em>size</em>, <em>dtype=None</em>, <em>device=None</em>, <em>requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.new_zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> filled with <code class="docutils literal notranslate"><span class="pre">0</span></code>.
By default, the returned Tensor has the same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and
<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int...</em>) – a list, tuple, or <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Size</span></code> of integers defining the
shape of the output tensor.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, optional) – the desired type of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> as this tensor.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, optional) – the desired device of returned tensor.
Default: if None, same <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as this tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If autograd should record operations on the
returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]], dtype=torch.float64)</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.is_cuda">
<code class="descname">is_cuda</code><a class="headerlink" href="#torch.Tensor.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the Tensor is stored on the GPU, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.device">
<code class="descname">device</code><a class="headerlink" href="#torch.Tensor.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Is the <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> where this Tensor is.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.abs">
<code class="descname">abs</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.abs_">
<code class="descname">abs_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.abs_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.abs" title="torch.Tensor.abs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos">
<code class="descname">acos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos_">
<code class="descname">acos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.acos_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.acos" title="torch.Tensor.acos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.add" title="Permalink to this definition">¶</a></dt>
<dd><p>add(value=1, other) -&gt; Tensor</p>
<p>See <a class="reference internal" href="index.html#torch.add" title="torch.add"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add_">
<code class="descname">add_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.add_" title="Permalink to this definition">¶</a></dt>
<dd><p>add_(value=1, other) -&gt; Tensor</p>
<p>In-place version of <a class="reference internal" href="#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm">
<code class="descname">addbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm_">
<code class="descname">addbmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addbmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv">
<code class="descname">addcdiv</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv_">
<code class="descname">addcdiv_</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul">
<code class="descname">addcmul</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcmul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul_">
<code class="descname">addcmul_</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addcmul_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm">
<code class="descname">addmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm_">
<code class="descname">addmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv">
<code class="descname">addmv</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addmv" title="torch.addmv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv_">
<code class="descname">addmv_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addmv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmv" title="torch.Tensor.addmv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr">
<code class="descname">addr</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addr" title="torch.addr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr_">
<code class="descname">addr_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.addr_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addr" title="torch.Tensor.addr"><code class="xref py py-meth docutils literal notranslate"><span class="pre">addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.allclose">
<code class="descname">allclose</code><span class="sig-paren">(</span><em>other</em>, <em>rtol=1e-05</em>, <em>atol=1e-08</em>, <em>equal_nan=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.allclose" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.allclose" title="torch.allclose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.allclose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.apply_">
<code class="descname">apply_</code><span class="sig-paren">(</span><em>callable</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.apply_" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the function <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> to each element in the tensor, replacing
each element with the value returned by <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function only works with CPU tensors and should not be used in code
sections that require high performance.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.argmax">
<code class="descname">argmax</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.argmax" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.argmax" title="torch.argmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmax()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.argmin">
<code class="descname">argmin</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.argmin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.argmin" title="torch.argmin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.argmin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin">
<code class="descname">asin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin_">
<code class="descname">asin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.asin_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan">
<code class="descname">atan</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2">
<code class="descname">atan2</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2_">
<code class="descname">atan2_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan2_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan2" title="torch.Tensor.atan2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan_">
<code class="descname">atan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.atan_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan" title="torch.Tensor.atan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm">
<code class="descname">baddbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm_">
<code class="descname">baddbmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli">
<code class="descname">bernoulli</code><span class="sig-paren">(</span><em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a result tensor where each <span class="math">\(\texttt{result[i]}\)</span> is independently
sampled from <span class="math">\(\text{Bernoulli}(\texttt{self[i]})\)</span>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> must have
floating point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, and the result will have the same <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
<p>See <a class="reference internal" href="index.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli_">
<code class="descname">bernoulli_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.bernoulli_" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descname">bernoulli_</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Fills each location of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> with an independent sample from
<span class="math">\(\text{Bernoulli}(\texttt{p})\)</span>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> can have integral
<code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
</dd></dl>

<dl class="function">
<dt>
<code class="descname">bernoulli_</code><span class="sig-paren">(</span><em>p_tensor</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">p_tensor</span></code> should be a tensor containing probabilities to be used for
drawing the binary random number.</p>
<p>The <span class="math">\(\text{i}^{th}\)</span> element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will be set to a
value sampled from <span class="math">\(\text{Bernoulli}(\texttt{p\_tensor[i]})\)</span>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> can have integral <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, but <code class="xref py py-attr docutils literal notranslate"><span class="pre">p_tensor</span></code> must have
floating point <code class="docutils literal notranslate"><span class="pre">dtype</span></code>.</p>
</dd></dl>

<p>See also <a class="reference internal" href="#torch.Tensor.bernoulli" title="torch.Tensor.bernoulli"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bernoulli()</span></code></a> and <a class="reference internal" href="index.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bmm">
<code class="descname">bmm</code><span class="sig-paren">(</span><em>batch2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.bmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.bmm" title="torch.bmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.bmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.byte">
<code class="descname">byte</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.byte" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.byte()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.uint8)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.btrifact">
<code class="descname">btrifact</code><span class="sig-paren">(</span><em>pivot=True) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.btrifact" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.btrifact" title="torch.btrifact"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.btrifact()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.btrifact_with_info">
<code class="descname">btrifact_with_info</code><span class="sig-paren">(</span><em>pivot=True) -&gt; (Tensor</em>, <em>Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.btrifact_with_info" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.btrifact_with_info" title="torch.btrifact_with_info"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.btrifact_with_info()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.btrisolve">
<code class="descname">btrisolve</code><span class="sig-paren">(</span><em>LU_data</em>, <em>LU_pivots</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.btrisolve" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.btrisolve" title="torch.btrisolve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.btrisolve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cauchy_">
<code class="descname">cauchy_</code><span class="sig-paren">(</span><em>median=0</em>, <em>sigma=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cauchy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p>
<div class="math">
\[f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - \text{median})^2 + \sigma^2}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil">
<code class="descname">ceil</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil_">
<code class="descname">ceil_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ceil_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ceil" title="torch.Tensor.ceil"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.char">
<code class="descname">char</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.char" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.char()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int8)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cholesky">
<code class="descname">cholesky</code><span class="sig-paren">(</span><em>upper=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cholesky" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cholesky_solve">
<code class="descname">cholesky_solve</code><span class="sig-paren">(</span><em>input2</em>, <em>upper=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cholesky_solve" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.chunk">
<code class="descname">chunk</code><span class="sig-paren">(</span><em>chunks</em>, <em>dim=0</em><span class="sig-paren">)</span> &#x2192; List of Tensors<a class="headerlink" href="#torch.Tensor.chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.chunk" title="torch.chunk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.chunk()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp">
<code class="descname">clamp</code><span class="sig-paren">(</span><em>min</em>, <em>max</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clamp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp_">
<code class="descname">clamp_</code><span class="sig-paren">(</span><em>min</em>, <em>max</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clamp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. The copy has the same size and data
type as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike <cite>copy_()</cite>, this function is recorded in the computation graph. Gradients
propagating to the cloned tensor will propagate to the original tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.contiguous">
<code class="descname">contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a contiguous tensor containing the same data as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous, this function returns the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.copy_">
<code class="descname">copy_</code><span class="sig-paren">(</span><em>src</em>, <em>non_blocking=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and returns
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> tensor must be <span class="xref std std-ref">broadcastable</span>
with the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. It may be of a different data type or reside on a
different device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source tensor to copy from</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> and this copy is between CPU and GPU,
the copy may occur asynchronously with respect to the host. For other
cases, this argument has no effect.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos">
<code class="descname">cos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos_">
<code class="descname">cos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cos_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cos" title="torch.Tensor.cos"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh">
<code class="descname">cosh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh_">
<code class="descname">cosh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cosh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cosh" title="torch.Tensor.cosh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CPU memory.</p>
<p>If this object is already in CPU memory and on the correct device,
then no copy is performed and the original object is returned.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cross">
<code class="descname">cross</code><span class="sig-paren">(</span><em>other</em>, <em>dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cross" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cross" title="torch.cross"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cross()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em>, <em>non_blocking=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device,
then no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>) – The destination GPU device.
Defaults to the current CUDA device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host.
Otherwise, the argument has no effect. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumprod">
<code class="descname">cumprod</code><span class="sig-paren">(</span><em>dim</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cumprod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumprod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumsum">
<code class="descname">cumsum</code><span class="sig-paren">(</span><em>dim</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.cumsum" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cumsum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.data_ptr">
<code class="descname">data_ptr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the address of the first element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.det">
<code class="descname">det</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.det" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.det" title="torch.det"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.det()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.diag">
<code class="descname">diag</code><span class="sig-paren">(</span><em>diagonal=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.diag" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.diag" title="torch.diag"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.diag_embed">
<code class="descname">diag_embed</code><span class="sig-paren">(</span><em>offset=0</em>, <em>dim1=-2</em>, <em>dim2=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.diag_embed" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.diag_embed" title="torch.diag_embed"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.diag_embed()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dim">
<code class="descname">dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dist">
<code class="descname">dist</code><span class="sig-paren">(</span><em>other</em>, <em>p=2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.dist" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.dist" title="torch.dist"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dist()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div">
<code class="descname">div</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.div" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div_">
<code class="descname">div_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.div_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-meth docutils literal notranslate"><span class="pre">div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dot">
<code class="descname">dot</code><span class="sig-paren">(</span><em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.dot" title="torch.dot"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.dot()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.double" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.double()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float64)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eig">
<code class="descname">eig</code><span class="sig-paren">(</span><em>eigenvectors=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.eig" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.eig" title="torch.eig"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.element_size">
<code class="descname">element_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.element_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size in bytes of an individual element.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq">
<code class="descname">eq</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.eq" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq_">
<code class="descname">eq_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.eq_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-meth docutils literal notranslate"><span class="pre">eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.equal">
<code class="descname">equal</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.equal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.equal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erf">
<code class="descname">erf</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.erf" title="torch.erf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erf_">
<code class="descname">erf_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erf_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.erf" title="torch.Tensor.erf"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfc">
<code class="descname">erfc</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.erfc" title="torch.erfc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfc_">
<code class="descname">erfc_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfc_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.erfc" title="torch.Tensor.erfc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erfc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfinv">
<code class="descname">erfinv</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfinv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.erfinv" title="torch.erfinv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.erfinv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.erfinv_">
<code class="descname">erfinv_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.erfinv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.erfinv" title="torch.Tensor.erfinv"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erfinv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp">
<code class="descname">exp</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp_">
<code class="descname">exp_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.exp" title="torch.Tensor.exp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expm1">
<code class="descname">expm1</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expm1" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.expm1" title="torch.expm1"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expm1()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expm1_">
<code class="descname">expm1_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expm1_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.expm1" title="torch.Tensor.expm1"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expm1()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expand" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new view of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with singleton dimensions expanded
to a larger size.</p>
<p>Passing -1 as the size for a dimension means not changing the size of
that dimension.</p>
<p>Tensor can be also expanded to a larger number of dimensions, and the
new ones will be appended at the front. For the new dimensions, the
size cannot be set to -1.</p>
<p>Expanding a tensor does not allocate new memory, but only creates a
new view on the existing tensor where a dimension of size one is
expanded to a larger size by setting the <code class="docutils literal notranslate"><span class="pre">stride</span></code> to 0. Any dimension
of size 1 can be expanded to an arbitrary value without allocating new
memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired expanded size</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>More than one element of an expanded tensor may refer to a single
memory location. As a result, in-place operations (especially ones that
are vectorized) may result in incorrect behavior. If you need to write
to the tensors, please clone them first.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>   <span class="c1"># -1 means not changing the size of that dimension</span>
<span class="go">tensor([[ 1,  1,  1,  1],</span>
<span class="go">        [ 2,  2,  2,  2],</span>
<span class="go">        [ 3,  3,  3,  3]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand_as">
<code class="descname">expand_as</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.expand_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Expand this tensor to the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.expand_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.expand(other.size())</span></code>.</p>
<p>Please see <a class="reference internal" href="#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">expand</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exponential_">
<code class="descname">exponential_</code><span class="sig-paren">(</span><em>lambd=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.exponential_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the exponential distribution:</p>
<div class="math">
\[f(x) = \lambda e^{-\lambda x}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fill_">
<code class="descname">fill_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with the specified value.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.flatten">
<code class="descname">flatten</code><span class="sig-paren">(</span><em>input</em>, <em>start_dim=0</em>, <em>end_dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>see <a class="reference internal" href="index.html#torch.flatten" title="torch.flatten"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flatten()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.flip">
<code class="descname">flip</code><span class="sig-paren">(</span><em>dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.flip" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.flip" title="torch.flip"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.flip()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.float" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.float()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float32)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor">
<code class="descname">floor</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor_">
<code class="descname">floor_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.floor_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.floor" title="torch.Tensor.floor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod">
<code class="descname">fmod</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fmod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.fmod" title="torch.fmod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod_">
<code class="descname">fmod_</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.fmod_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.fmod" title="torch.Tensor.fmod"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac">
<code class="descname">frac</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.frac" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac_">
<code class="descname">frac_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.frac_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.frac" title="torch.Tensor.frac"><code class="xref py py-meth docutils literal notranslate"><span class="pre">frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gather">
<code class="descname">gather</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.gather" title="torch.gather"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gather()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge">
<code class="descname">ge</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ge" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge_">
<code class="descname">ge_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ge_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gels">
<code class="descname">gels</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gels" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.gels" title="torch.gels"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gels()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geometric_">
<code class="descname">geometric_</code><span class="sig-paren">(</span><em>p</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.geometric_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements drawn from the geometric distribution:</p>
<div class="math">
\[f(X=k) = (1 - p)^{k - 1} p\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geqrf">
<code class="descname">geqrf</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.geqrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.geqrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ger">
<code class="descname">ger</code><span class="sig-paren">(</span><em>vec2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ger" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ger" title="torch.ger"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ger()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gesv">
<code class="descname">gesv</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.gesv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.solve" title="torch.solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.solve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.get_device">
<code class="descname">get_device</code><span class="sig-paren">(</span><em>) -&gt; Device ordinal (Integer</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.get_device" title="Permalink to this definition">¶</a></dt>
<dd><p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
For CPU tensors, an error is thrown.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>  <span class="c1"># RuntimeError: get_device is not implemented for type torch.FloatTensor</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt">
<code class="descname">gt</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt_">
<code class="descname">gt_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.gt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.half" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.half()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.float16)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.histc">
<code class="descname">histc</code><span class="sig-paren">(</span><em>bins=100</em>, <em>min=0</em>, <em>max=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.histc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.histc" title="torch.histc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.histc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_add_">
<code class="descname">index_add_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_add_" title="Permalink to this definition">¶</a></dt>
<dd><p>Accumulate the elements of <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by adding
to the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>
and <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> is added to the
<code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – indices of <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> to select from</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to add</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[  2.,   3.,   4.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  8.,   9.,  10.],</span>
<span class="go">        [  1.,   1.,   1.],</span>
<span class="go">        [  5.,   6.,   7.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_add">
<code class="descname">index_add</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_add_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_copy_">
<code class="descname">index_copy_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements of <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor by selecting
the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>. For example, if <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">==</span> <span class="pre">0</span></code>
and <code class="docutils literal notranslate"><span class="pre">index[i]</span> <span class="pre">==</span> <span class="pre">j</span></code>, then the <code class="docutils literal notranslate"><span class="pre">i</span></code>th row of <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> is copied to the
<code class="docutils literal notranslate"><span class="pre">j</span></code>th row of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>The <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>th dimension of <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must have the same size as the
length of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> (which must be a vector), and all other dimensions must
match <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, or an error will be raised.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – indices of <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> to select from</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to copy</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_copy_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.,  3.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 7.,  8.,  9.],</span>
<span class="go">        [ 0.,  0.,  0.],</span>
<span class="go">        [ 4.,  5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_copy">
<code class="descname">index_copy</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.index_copy_" title="torch.Tensor.index_copy_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_copy_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_fill_">
<code class="descname">index_fill_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>val</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the elements of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with value <code class="xref py py-attr docutils literal notranslate"><span class="pre">val</span></code> by
selecting the indices in the order given in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to fill in</p></li>
<li><p><strong>val</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the value to fill with</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[-1.,  2., -1.],</span>
<span class="go">        [-1.,  5., -1.],</span>
<span class="go">        [-1.,  8., -1.]])</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_fill">
<code class="descname">index_fill</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.index_fill_" title="torch.Tensor.index_fill_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.index_fill_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_put_">
<code class="descname">index_put_</code><span class="sig-paren">(</span><em>indices</em>, <em>value</em>, <em>accumulate=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_put_" title="Permalink to this definition">¶</a></dt>
<dd><p>Puts values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> into the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> using
the indices specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code> (which is a tuple of Tensors). The
expression <code class="docutils literal notranslate"><span class="pre">tensor.index_put_(indices,</span> <span class="pre">value)</span></code> is equivalent to
<code class="docutils literal notranslate"><span class="pre">tensor[indices]</span> <span class="pre">=</span> <span class="pre">value</span></code>. Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if indices
contain duplicate elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> (<em>tuple of LongTensor</em>) – tensors used to index into <cite>self</cite>.</p></li>
<li><p><strong>value</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor of same dtype as <cite>self</cite>.</p></li>
<li><p><strong>accumulate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to accumulate into self</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_put">
<code class="descname">index_put</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.index_put" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_select">
<code class="descname">index_select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.index_select" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.index_select" title="torch.index_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.index_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.int">
<code class="descname">int</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.int" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.int()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int32)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.inverse">
<code class="descname">inverse</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.inverse" title="torch.inverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.inverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_contiguous">
<code class="descname">is_contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is contiguous in memory in C order.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_floating_point">
<code class="descname">is_floating_point</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_floating_point" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if the data type of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is a floating point data type.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_pinned">
<code class="descname">is_pinned</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if this tensor resides in pinned memory</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_set_to">
<code class="descname">is_set_to</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#torch.Tensor.is_set_to" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if this object refers to the same <code class="docutils literal notranslate"><span class="pre">THTensor</span></code> object from the
Torch C API as the given tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_signed">
<code class="descname">is_signed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.is_signed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.item">
<code class="descname">item</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; number<a class="headerlink" href="#torch.Tensor.item" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of this tensor as a standard Python number. This only works
for tensors with one element. For other cases, see <a class="reference internal" href="#torch.Tensor.tolist" title="torch.Tensor.tolist"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tolist()</span></code></a>.</p>
<p>This operation is not differentiable.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="go">1.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.kthvalue">
<code class="descname">kthvalue</code><span class="sig-paren">(</span><em>k</em>, <em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.kthvalue" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.kthvalue()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le">
<code class="descname">le</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.le" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le_">
<code class="descname">le_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.le_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-meth docutils literal notranslate"><span class="pre">le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp">
<code class="descname">lerp</code><span class="sig-paren">(</span><em>end</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lerp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.lerp" title="torch.lerp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp_">
<code class="descname">lerp_</code><span class="sig-paren">(</span><em>end</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lerp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lerp" title="torch.Tensor.lerp"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log">
<code class="descname">log</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_">
<code class="descname">log_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log" title="torch.Tensor.log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logdet">
<code class="descname">logdet</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logdet" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.logdet" title="torch.logdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logdet()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log10">
<code class="descname">log10</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log10" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.log10" title="torch.log10"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log10()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log10_">
<code class="descname">log10_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log10_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log10" title="torch.Tensor.log10"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log10()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p">
<code class="descname">log1p</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p_">
<code class="descname">log1p_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log1p_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log2">
<code class="descname">log2</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log2" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.log2" title="torch.log2"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.log2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log2_">
<code class="descname">log2_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.log2_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log2" title="torch.Tensor.log2"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_normal_">
<code class="descname">log_normal_</code><span class="sig-paren">(</span><em>mean=1</em>, <em>std=2</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.log_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers samples from the log-normal distribution
parameterized by the given mean <span class="math">\(\mu\)</span> and standard deviation
<span class="math">\(\sigma\)</span>. Note that <a class="reference internal" href="index.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="index.html#torch.std" title="torch.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a> are the mean and
standard deviation of the underlying normal distribution, and not of the
returned distribution:</p>
<div class="math">
\[f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\ e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.logsumexp">
<code class="descname">logsumexp</code><span class="sig-paren">(</span><em>dim</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.logsumexp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.logsumexp" title="torch.logsumexp"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.logsumexp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.long">
<code class="descname">long</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.long" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.long()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int64)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt">
<code class="descname">lt</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt_">
<code class="descname">lt_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.lt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.map_">
<code class="descname">map_</code><span class="sig-paren">(</span><em>tensor</em>, <em>callable</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.map_" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> for each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and the given
<a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> and stores the results in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor and
the given <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> must be <span class="xref std std-ref">broadcastable</span>.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">callable</span></code> should have the signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">number</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_scatter_">
<code class="descname">masked_scatter_</code><span class="sig-paren">(</span><em>mask</em>, <em>source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_scatter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies elements from <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor at positions where
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is one.
The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be <span class="xref std std-ref">broadcastable</span>
with the shape of the underlying tensor. The <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> should have at least
as many elements as the number of ones in <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mask</strong> (<a class="reference internal" href="index.html#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – the binary mask</p></li>
<li><p><strong>source</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor to copy from</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> operates on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor, not on the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_scatter">
<code class="descname">masked_scatter</code><span class="sig-paren">(</span><em>mask</em>, <em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.masked_scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.masked_scatter_" title="torch.Tensor.masked_scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_scatter_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_fill_">
<code class="descname">masked_fill_</code><span class="sig-paren">(</span><em>mask</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> is
one. The shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> must be
<span class="xref std std-ref">broadcastable</span> with the shape of the underlying
tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mask</strong> (<a class="reference internal" href="index.html#torch.ByteTensor" title="torch.ByteTensor"><em>ByteTensor</em></a>) – the binary mask</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the value to fill in with</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_fill">
<code class="descname">masked_fill</code><span class="sig-paren">(</span><em>mask</em>, <em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.masked_fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.masked_fill_" title="torch.Tensor.masked_fill_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.masked_fill_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_select">
<code class="descname">masked_select</code><span class="sig-paren">(</span><em>mask</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.masked_select" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.masked_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.matmul">
<code class="descname">matmul</code><span class="sig-paren">(</span><em>tensor2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.matmul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.matmul" title="torch.matmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.matrix_power">
<code class="descname">matrix_power</code><span class="sig-paren">(</span><em>n</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.matrix_power" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.matrix_power" title="torch.matrix_power"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.matrix_power()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.max">
<code class="descname">max</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; Tensor or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.max" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.max()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mean">
<code class="descname">mean</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; Tensor or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mean()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.median">
<code class="descname">median</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.median" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.median" title="torch.median"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.median()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.min">
<code class="descname">min</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; Tensor or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.min" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.min()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mm">
<code class="descname">mm</code><span class="sig-paren">(</span><em>mat2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mode">
<code class="descname">mode</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mode" title="torch.mode"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mode()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul">
<code class="descname">mul</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul_">
<code class="descname">mul_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mul_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.multinomial">
<code class="descname">multinomial</code><span class="sig-paren">(</span><em>num_samples</em>, <em>replacement=False</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mv">
<code class="descname">mv</code><span class="sig-paren">(</span><em>vec</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mvlgamma">
<code class="descname">mvlgamma</code><span class="sig-paren">(</span><em>p</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mvlgamma" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mvlgamma" title="torch.mvlgamma"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mvlgamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mvlgamma_">
<code class="descname">mvlgamma_</code><span class="sig-paren">(</span><em>p</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.mvlgamma_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.mvlgamma" title="torch.Tensor.mvlgamma"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mvlgamma()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.narrow">
<code class="descname">narrow</code><span class="sig-paren">(</span><em>dimension</em>, <em>start</em>, <em>length</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.narrow" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.narrow" title="torch.narrow"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.narrow()</span></code></a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3],</span>
<span class="go">        [ 4,  5,  6]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 2,  3],</span>
<span class="go">        [ 5,  6],</span>
<span class="go">        [ 8,  9]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ndimension">
<code class="descname">ndimension</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.ndimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dim()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne">
<code class="descname">ne</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ne" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne_">
<code class="descname">ne_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ne_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg">
<code class="descname">neg</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.neg" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg_">
<code class="descname">neg_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.neg_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-meth docutils literal notranslate"><span class="pre">neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nelement">
<code class="descname">nelement</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.nelement" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-meth docutils literal notranslate"><span class="pre">numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nonzero">
<code class="descname">nonzero</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.Tensor.nonzero" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nonzero()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.norm">
<code class="descname">norm</code><span class="sig-paren">(</span><em>p='fro'</em>, <em>dim=None</em>, <em>keepdim=False</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.norm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.norm" title="torch.norm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.norm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.normal_">
<code class="descname">normal_</code><span class="sig-paren">(</span><em>mean=0</em>, <em>std=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with elements samples from the normal distribution
parameterized by <a class="reference internal" href="index.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mean</span></code></a> and <a class="reference internal" href="index.html#torch.std" title="torch.std"><code class="xref py py-attr docutils literal notranslate"><span class="pre">std</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numel">
<code class="descname">numel</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.numel" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.numel" title="torch.numel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numpy">
<code class="descname">numpy</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; numpy.ndarray<a class="headerlink" href="#torch.Tensor.numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor as a NumPy <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>. This tensor and the
returned <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code> share the same underlying storage. Changes to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will be reflected in the <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code> and vice versa.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.orgqr">
<code class="descname">orgqr</code><span class="sig-paren">(</span><em>input2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.orgqr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.orgqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ormqr">
<code class="descname">ormqr</code><span class="sig-paren">(</span><em>input2</em>, <em>input3</em>, <em>left=True</em>, <em>transpose=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.ormqr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ormqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.permute">
<code class="descname">permute</code><span class="sig-paren">(</span><em>*dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.permute" title="Permalink to this definition">¶</a></dt>
<dd><p>Permute the dimensions of this tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*dims</strong> (<em>int...</em>) – The desired ordering of dimensions</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([5, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pin_memory">
<code class="descname">pin_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.pinverse">
<code class="descname">pinverse</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pinverse" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.pinverse" title="torch.pinverse"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pinverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potrf">
<code class="descname">potrf</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.potrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cholesky" title="torch.cholesky"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potri">
<code class="descname">potri</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.potri" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.potri" title="torch.potri"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.potri()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potrs">
<code class="descname">potrs</code><span class="sig-paren">(</span><em>u</em>, <em>upper=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.potrs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cholesky_solve" title="torch.cholesky_solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cholesky_solve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow">
<code class="descname">pow</code><span class="sig-paren">(</span><em>exponent</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.pow" title="torch.pow"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow_">
<code class="descname">pow_</code><span class="sig-paren">(</span><em>exponent</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.pow_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.prod">
<code class="descname">prod</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.prod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.prod" title="torch.prod"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.prod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pstrf">
<code class="descname">pstrf</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pstrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.pstrf" title="torch.pstrf"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pstrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.put_">
<code class="descname">put_</code><span class="sig-paren">(</span><em>indices</em>, <em>tensor</em>, <em>accumulate=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.put_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements from <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> into the positions specified by
indices. For the purpose of indexing, the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor is treated as if
it were a 1-D tensor.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">accumulate</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the elements in <a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a> are added to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>. If accumulate is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the behavior is undefined if indices
contain duplicate elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>indices</strong> (<em>LongTensor</em>) – the indices into self</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor containing values to copy from</p></li>
<li><p><strong>accumulate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to accumulate into self</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="go">                        [6, 7, 8]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">src</span><span class="o">.</span><span class="n">put_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]))</span>
<span class="go">tensor([[  4,   9,   5],</span>
<span class="go">        [ 10,   7,   8]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.qr">
<code class="descname">qr</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.qr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.qr" title="torch.qr"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.qr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.random_">
<code class="descname">random_</code><span class="sig-paren">(</span><em>from=0</em>, <em>to=None</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.random_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the discrete uniform
distribution over <code class="docutils literal notranslate"><span class="pre">[from,</span> <span class="pre">to</span> <span class="pre">-</span> <span class="pre">1]</span></code>. If not specified, the values are usually
only bounded by <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s data type. However, for floating point
types, if unspecified, range will be <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^mantissa]</span></code> to ensure that every
value is representable. For example, <cite>torch.tensor(1, dtype=torch.double).random_()</cite>
will be uniform in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2^53]</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal">
<code class="descname">reciprocal</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal_">
<code class="descname">reciprocal_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder">
<code class="descname">remainder</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.remainder" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.remainder" title="torch.remainder"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder_">
<code class="descname">remainder_</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.remainder_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.remainder" title="torch.Tensor.remainder"><code class="xref py py-meth docutils literal notranslate"><span class="pre">remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm">
<code class="descname">renorm</code><span class="sig-paren">(</span><em>p</em>, <em>dim</em>, <em>maxnorm</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.renorm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.renorm" title="torch.renorm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm_">
<code class="descname">renorm_</code><span class="sig-paren">(</span><em>p</em>, <em>dim</em>, <em>maxnorm</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.renorm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.renorm" title="torch.Tensor.renorm"><code class="xref py py-meth docutils literal notranslate"><span class="pre">renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.repeat">
<code class="descname">repeat</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeats this tensor along the specified dimensions.</p>
<p>Unlike <a class="reference internal" href="#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a>, this function copies the tensor’s data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.repeat()</span></code> behaves differently from
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html">numpy.repeat</a>,
but is more similar to
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html">numpy.tile</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – The number of times to repeat this tensor along each
dimension</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3],</span>
<span class="go">        [ 1,  2,  3,  1,  2,  3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.requires_grad_">
<code class="descname">requires_grad_</code><span class="sig-paren">(</span><em>requires_grad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on this tensor: sets this tensor’s
<a class="reference internal" href="index.html#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> attribute in-place. Returns this tensor.</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">require_grad_()</span></code>’s main use case is to tell autograd to begin recording
operations on a Tensor <code class="docutils literal notranslate"><span class="pre">tensor</span></code>. If <code class="docutils literal notranslate"><span class="pre">tensor</span></code> has <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>
(because it was obtained through a DataLoader, or required preprocessing or
initialization), <code class="docutils literal notranslate"><span class="pre">tensor.requires_grad_()</span></code> makes it so that autograd will
begin to record operations on <code class="docutils literal notranslate"><span class="pre">tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If autograd should record operations on this tensor.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Let&#39;s say we want to preprocess some saved weights and use</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the result as new weights.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">saved_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">loaded_weights</span><span class="p">)</span>  <span class="c1"># some function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span>
<span class="go">tensor([-0.5503,  0.4926, -2.1158, -0.8303])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now, start to record operations done to weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([-1.1007,  0.9853, -4.2316, -1.6606])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reshape">
<code class="descname">reshape</code><span class="sig-paren">(</span><em>*shape</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reshape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor with the same data and number of elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>
but with the specified shape. This method returns a view if <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code> is
compatible with the current shape. See <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code></a> on when it is
possible to return a view.</p>
<p>See <a class="reference internal" href="index.html#torch.reshape" title="torch.reshape"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.reshape()</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> (<em>tuple of python:ints</em><em> or </em><em>int...</em>) – the desired shape</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reshape_as">
<code class="descname">reshape_as</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.reshape_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor as the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.reshape_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.reshape(other.sizes())</span></code>.
This method returns a view if <code class="docutils literal notranslate"><span class="pre">other.sizes()</span></code> is compatible with the current
shape. See <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.view()</span></code></a> on when it is possible to return a view.</p>
<p>Please see <a class="reference internal" href="index.html#torch.reshape" title="torch.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">reshape</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same shape
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_">
<code class="descname">resize_</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.resize_" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to the specified size. If the number of elements is
larger than the current storage size, then the underlying storage is resized
to fit the new number of elements. If the number of elements is smaller, the
underlying storage is not changed. Existing elements are preserved but any new
memory is uninitialized.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This is a low-level method. The storage is reinterpreted as C-contiguous,
ignoring the current strides (unless the target size equals the current
size, in which case the tensor is left unchanged). For most purposes, you
will instead want to use <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a>, which checks for
contiguity, or <a class="reference internal" href="#torch.Tensor.reshape" title="torch.Tensor.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a>, which copies data if needed. To
change the size in-place with custom strides, see <a class="reference internal" href="#torch.Tensor.set_" title="torch.Tensor.set_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired size</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1,  2],</span>
<span class="go">        [ 3,  4]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_as_">
<code class="descname">resize_as_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.resize_as_" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor to be the same size as the specified
<a class="reference internal" href="index.html#torch.tensor" title="torch.tensor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code></a>. This is equivalent to <code class="docutils literal notranslate"><span class="pre">self.resize_(tensor.size())</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.roll">
<code class="descname">roll</code><span class="sig-paren">(</span><em>shifts</em>, <em>dims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.roll" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.roll" title="torch.roll"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.roll()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round">
<code class="descname">round</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.round" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round_">
<code class="descname">round_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.round_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.round" title="torch.Tensor.round"><code class="xref py py-meth docutils literal notranslate"><span class="pre">round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt">
<code class="descname">rsqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt_">
<code class="descname">rsqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_">
<code class="descname">scatter_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>src</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Writes all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code>, its output
index is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by
the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p>This is the reverse operation of the manner described in <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> (if it is a Tensor) should have same
number of dimensions. It is also required that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">src.size(d)</span></code>
for all dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all
dimensions <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.</p>
<p>Moreover, as for <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>, the values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> must be
between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">self.size(dim)</span> <span class="pre">-</span> <span class="pre">1</span></code> inclusive, and all values in a row
along the specified dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> must be unique.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter,
can be either empty or the same size of src.
When empty, the operation returns identity</p></li>
<li><p><strong>src</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source element(s) to scatter,
incase <cite>value</cite> is not specified</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the source element(s) to scatter,
incase <cite>src</cite> is not specified</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],</span>
<span class="go">        [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],</span>
<span class="go">        [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],</span>
<span class="go">        [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span> <span class="mf">1.23</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span>
<span class="go">tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],</span>
<span class="go">        [ 0.0000,  0.0000,  0.0000,  1.2300]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter">
<code class="descname">scatter</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>source</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_add_">
<code class="descname">scatter_add_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_add_" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds all values from the tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> into <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> at the indices
specified in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> tensor in a similar fashion as
<a class="reference internal" href="#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scatter_()</span></code></a>. For each value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>, it is added to
an index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> which is specified by its index in <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>
for <code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">!=</span> <span class="pre">dim</span></code> and by the corresponding value in <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> for
<code class="docutils literal notranslate"><span class="pre">dimension</span> <span class="pre">=</span> <span class="pre">dim</span></code>.</p>
<p>For a 3-D tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> is updated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">other</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</pre></div>
</div>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> should have same number of
dimensions. It is also required that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">other.size(d)</span></code> for all
dimensions <code class="docutils literal notranslate"><span class="pre">d</span></code>, and that <code class="docutils literal notranslate"><span class="pre">index.size(d)</span> <span class="pre">&lt;=</span> <span class="pre">self.size(d)</span></code> for all dimensions
<code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">!=</span> <span class="pre">dim</span></code>.</p>
<p>Moreover, as for <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gather()</span></code></a>, the values of <code class="xref py py-attr docutils literal notranslate"><span class="pre">index</span></code> must be
between <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">self.size(dim)</span> <span class="pre">-</span> <span class="pre">1</span></code> inclusive, and all values in a row along
the specified dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> must be unique.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the axis along which to index</p></li>
<li><p><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter and add,
can be either empty or the same size of src.
When empty, the operation returns identity.</p></li>
<li><p><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the source elements to scatter and add</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],</span>
<span class="go">        [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>
<span class="go">tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],</span>
<span class="go">        [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],</span>
<span class="go">        [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_add">
<code class="descname">scatter_add</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>source</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.scatter_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Out-of-place version of <a class="reference internal" href="#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.scatter_add_()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.select">
<code class="descname">select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Slices the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor along the selected dimension at the given index.
This function returns a tensor with the given dimension removed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to slice</p></li>
<li><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the index to select with</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.Tensor.select" title="torch.Tensor.select"><code class="xref py py-meth docutils literal notranslate"><span class="pre">select()</span></code></a> is equivalent to slicing. For example,
<code class="docutils literal notranslate"><span class="pre">tensor.select(0,</span> <span class="pre">index)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">tensor[index]</span></code> and
<code class="docutils literal notranslate"><span class="pre">tensor.select(2,</span> <span class="pre">index)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">tensor[:,:,index]</span></code>.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.set_">
<code class="descname">set_</code><span class="sig-paren">(</span><em>source=None</em>, <em>storage_offset=0</em>, <em>size=None</em>, <em>stride=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.set_" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the underlying storage, size, and strides. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a tensor,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor will share the same storage and have the same size and
strides as <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code>. Changes to elements in one tensor will be reflected
in the other.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">source</span></code> is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Storage</span></code>, the method sets the underlying
storage, offset, size, and stride.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) – the tensor or storage to use</p></li>
<li><p><strong>storage_offset</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the offset in the storage</p></li>
<li><p><strong>size</strong> (<em>torch.Size</em><em>, </em><em>optional</em>) – the desired size. Defaults to the size of the source.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the desired stride. Defaults to C-contiguous strides.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.share_memory_">
<code class="descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the underlying storage to shared memory.</p>
<p>This is a no-op if the underlying storage is already in shared memory
and for CUDA tensors. Tensors in shared memory cannot be resized.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.short">
<code class="descname">short</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.short" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">self.short()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.to(torch.int16)</span></code>. See <a class="reference internal" href="#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-func docutils literal notranslate"><span class="pre">to()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid">
<code class="descname">sigmoid</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid_">
<code class="descname">sigmoid_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign">
<code class="descname">sign</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sign" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sign" title="torch.sign"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign_">
<code class="descname">sign_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sign_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sign" title="torch.Tensor.sign"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin">
<code class="descname">sin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin_">
<code class="descname">sin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sin_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sin" title="torch.Tensor.sin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh">
<code class="descname">sinh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh_">
<code class="descname">sinh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sinh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sinh" title="torch.Tensor.sinh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.size">
<code class="descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Size<a class="headerlink" href="#torch.Tensor.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. The returned value is a subclass of
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 4, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.slogdet">
<code class="descname">slogdet</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.slogdet" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.slogdet" title="torch.slogdet"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.slogdet()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.solve">
<code class="descname">solve</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &#x2192; Tensor, Tensor<a class="headerlink" href="#torch.Tensor.solve" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.solve" title="torch.solve"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.solve()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sort">
<code class="descname">sort</code><span class="sig-paren">(</span><em>dim=-1</em>, <em>descending=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sort()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.split">
<code class="descname">split</code><span class="sig-paren">(</span><em>split_size</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.split" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.split" title="torch.split"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.split()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sparse_mask">
<code class="descname">sparse_mask</code><span class="sig-paren">(</span><em>input</em>, <em>mask</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sparse_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new SparseTensor with values from Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> filtered
by indices of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code> and values are ignored. <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code>
must have the same shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an input Tensor</p></li>
<li><p><strong>mask</strong> (<em>SparseTensor</em>) – a SparseTensor which we filter <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> based on its indices</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nnz</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nnz</span><span class="p">,)),</span>
<span class="go">                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nnz</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span><span class="o">.</span><span class="n">sparse_mask</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[0, 0, 0, 2],</span>
<span class="go">                       [0, 1, 4, 3]]),</span>
<span class="go">       values=tensor([[[ 1.6550,  0.2397],</span>
<span class="go">                       [-0.1611, -0.0779]],</span>

<span class="go">                      [[ 0.2326, -1.0558],</span>
<span class="go">                       [ 1.4711,  1.9678]],</span>

<span class="go">                      [[-0.5138, -0.0411],</span>
<span class="go">                       [ 1.9417,  0.5158]],</span>

<span class="go">                      [[ 0.0793,  0.0036],</span>
<span class="go">                       [-0.2569, -0.1055]]]),</span>
<span class="go">       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt">
<code class="descname">sqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt_">
<code class="descname">sqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sqrt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze">
<code class="descname">squeeze</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze_">
<code class="descname">squeeze_</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.squeeze_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.std">
<code class="descname">std</code><span class="sig-paren">(</span><em>dim=None</em>, <em>unbiased=True</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.std" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.std" title="torch.std"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.std()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage">
<code class="descname">storage</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Storage<a class="headerlink" href="#torch.Tensor.storage" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the underlying storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage_offset">
<code class="descname">storage_offset</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#torch.Tensor.storage_offset" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor’s offset in the underlying storage in terms of
number of storage elements (not bytes).</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage_type">
<code class="descname">storage_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.storage_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.stride">
<code class="descname">stride</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; tuple or int<a class="headerlink" href="#torch.Tensor.stride" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the stride of <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor.</p>
<p>Stride is the jump necessary to go from one element to the next one in the
specified dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>. A tuple of all strides is returned when no
argument is passed in. Otherwise, an integer value is returned as the stride in
the particular dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the desired dimension in which stride is required</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(5, 1)</span>
<span class="go">&gt;&gt;&gt;x.stride(0)</span>
<span class="go">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub">
<code class="descname">sub</code><span class="sig-paren">(</span><em>value</em>, <em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Subtracts a scalar or tensor from <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor. If both <code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> are specified, each element of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is scaled by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">value</span></code> before being used.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is a tensor, the shape of <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> must be
<span class="xref std std-ref">broadcastable</span> with the shape of the underlying
tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub_">
<code class="descname">sub_</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sub_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sub()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sum">
<code class="descname">sum</code><span class="sig-paren">(</span><em>dim=None</em>, <em>keepdim=False</em>, <em>dtype=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sum" title="torch.sum"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.svd">
<code class="descname">svd</code><span class="sig-paren">(</span><em>some=True</em>, <em>compute_uv=True) -&gt; (Tensor</em>, <em>Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.svd" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.svd" title="torch.svd"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.svd()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.symeig">
<code class="descname">symeig</code><span class="sig-paren">(</span><em>eigenvectors=False</em>, <em>upper=True) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.symeig" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.symeig" title="torch.symeig"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.symeig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t">
<code class="descname">t</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.t" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.t" title="torch.t"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t_">
<code class="descname">t_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-meth docutils literal notranslate"><span class="pre">t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.to">
<code class="descname">to</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Tensor dtype and/or device conversion. A <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> are
inferred from the arguments of <code class="docutils literal notranslate"><span class="pre">self.to(*args,</span> <span class="pre">**kwargs)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">self</span></code> Tensor already
has the correct <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, then <code class="docutils literal notranslate"><span class="pre">self</span></code> is returned.
Otherwise, the returned tensor is a copy of <code class="docutils literal notranslate"><span class="pre">self</span></code> with the desired
<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>.</p>
</div>
<p>Here are the ways to call <code class="docutils literal notranslate"><span class="pre">to</span></code>:</p>
<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>dtype</em>, <em>non_blocking=False</em>, <em>copy=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with the specified <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></p>
</dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>device=None</em>, <em>dtype=None</em>, <em>non_blocking=False</em>, <em>copy=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with the specified <a class="reference internal" href="#torch.Tensor.device" title="torch.Tensor.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> and (optional)
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> it is inferred to be <code class="docutils literal notranslate"><span class="pre">self.dtype</span></code>.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert asynchronously with respect to
the host if possible, e.g., converting a CPU Tensor with pinned memory to a
CUDA Tensor.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">copy</span></code> is set, a new Tensor is created even when the Tensor
already matches the desired conversion.</p>
</dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>other</em>, <em>non_blocking=False</em>, <em>copy=False</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd><p>Returns a Tensor with same <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> and <a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> as
the Tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code>, tries to convert
asynchronously with respect to the host if possible, e.g., converting a CPU
Tensor with pinned memory to a CUDA Tensor.
When <code class="xref py py-attr docutils literal notranslate"><span class="pre">copy</span></code> is set, a new Tensor is created even when the Tensor
already matches the desired conversion.</p>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Initially dtype=float32, device=cpu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cuda0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">other</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">tensor([[-0.5044,  0.0005],</span>
<span class="go">        [ 0.3310, -0.0584]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.take">
<code class="descname">take</code><span class="sig-paren">(</span><em>indices</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.take" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.take" title="torch.take"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.take()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan">
<code class="descname">tan</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.tan" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan_">
<code class="descname">tan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tan_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tan" title="torch.Tensor.tan"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh">
<code class="descname">tanh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.tanh" title="torch.tanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh_">
<code class="descname">tanh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tanh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tanh" title="torch.Tensor.tanh"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tolist">
<code class="descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>”
tolist() -&gt; list or number</p>
<p>Returns the tensor as a (nested) list. For scalars, a standard
Python number is returned, just like with <a class="reference internal" href="#torch.Tensor.item" title="torch.Tensor.item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item()</span></code></a>.
Tensors are automatically moved to the CPU first if necessary.</p>
<p>This operation is not differentiable.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">[[0.012766935862600803, 0.5415473580360413],</span>
<span class="go"> [-0.08909505605697632, 0.7729271650314331]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="go">0.012766935862600803</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.topk">
<code class="descname">topk</code><span class="sig-paren">(</span><em>k</em>, <em>dim=None</em>, <em>largest=True</em>, <em>sorted=True) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.topk" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.topk()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.to_sparse">
<code class="descname">to_sparse</code><span class="sig-paren">(</span><em>sparseDims</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.to_sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
<a class="reference internal" href="index.html#sparse-docs"><span class="std std-ref">coordinate format</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sparseDims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – the number of sparse dimensions to include in the new sparse tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span>
<span class="go">tensor([[ 0,  0,  0],</span>
<span class="go">        [ 9,  0, 10],</span>
<span class="go">        [ 0,  0,  0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
<span class="go">tensor(indices=tensor([[1, 1],</span>
<span class="go">                       [0, 2]]),</span>
<span class="go">       values=tensor([ 9, 10]),</span>
<span class="go">       size=(3, 3), nnz=2, layout=torch.sparse_coo)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor(indices=tensor([[1]]),</span>
<span class="go">       values=tensor([[ 9,  0, 10]]),</span>
<span class="go">       size=(3, 3), nnz=1, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trace">
<code class="descname">trace</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trace" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.trace" title="torch.trace"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trace()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose">
<code class="descname">transpose</code><span class="sig-paren">(</span><em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose_">
<code class="descname">transpose_</code><span class="sig-paren">(</span><em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.transpose_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril">
<code class="descname">tril</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tril" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.tril" title="torch.tril"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril_">
<code class="descname">tril_</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.tril_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tril" title="torch.Tensor.tril"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu">
<code class="descname">triu</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.triu" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.triu" title="torch.triu"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu_">
<code class="descname">triu_</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.triu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.triu" title="torch.Tensor.triu"><code class="xref py py-meth docutils literal notranslate"><span class="pre">triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trtrs">
<code class="descname">trtrs</code><span class="sig-paren">(</span><em>A</em>, <em>upper=True</em>, <em>transpose=False</em>, <em>unitriangular=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.trtrs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.trtrs" title="torch.trtrs"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trtrs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc">
<code class="descname">trunc</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc_">
<code class="descname">trunc_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.trunc_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.trunc" title="torch.Tensor.trunc"><code class="xref py py-meth docutils literal notranslate"><span class="pre">trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>dtype=None</em>, <em>non_blocking=False</em>, <em>**kwargs</em><span class="sig-paren">)</span> &#x2192; str or Tensor<a class="headerlink" href="#torch.Tensor.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a><em> or </em><em>string</em>) – The desired type</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, and the source is in pinned memory
and destination is on the GPU or vice versa, the copy is performed
asynchronously with respect to the host. Otherwise, the argument
has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">async</span></code> arg is deprecated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type_as">
<code class="descname">type_as</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.type_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor cast to the type of the given tensor.</p>
<p>This is a no-op if the tensor is already of the correct type. This is
equivalent to <code class="docutils literal notranslate"><span class="pre">self.type(tensor.type())</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the tensor which has the desired type</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unfold">
<code class="descname">unfold</code><span class="sig-paren">(</span><em>dim</em>, <em>size</em>, <em>step</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor which contains all slices of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor in the dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a>.</p>
<p>Step between two slices is given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">step</span></code>.</p>
<p>If <cite>sizedim</cite> is the size of dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> for <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code>, the size of
dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code></a> in the returned tensor will be
<cite>(sizedim - size) / step + 1</cite>.</p>
<p>An additional dimension of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code></a> is appended in the returned tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension in which unfolding happens</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of each slice that is unfolded</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the step between each slice</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 2.,  3.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 4.,  5.],</span>
<span class="go">        [ 5.,  6.],</span>
<span class="go">        [ 6.,  7.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  2.],</span>
<span class="go">        [ 3.,  4.],</span>
<span class="go">        [ 5.,  6.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.uniform_">
<code class="descname">uniform_</code><span class="sig-paren">(</span><em>from=0</em>, <em>to=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with numbers sampled from the continuous uniform
distribution:</p>
<div class="math">
\[P(x) = \dfrac{1}{\text{to} - \text{from}}

\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unique">
<code class="descname">unique</code><span class="sig-paren">(</span><em>sorted=True</em>, <em>return_inverse=False</em>, <em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.unique" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the unique scalar elements of the tensor as a 1-D tensor.</p>
<p>See <a class="reference internal" href="index.html#torch.unique" title="torch.unique"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unique()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze">
<code class="descname">unsqueeze</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze_">
<code class="descname">unsqueeze_</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.unsqueeze_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.var">
<code class="descname">var</code><span class="sig-paren">(</span><em>dim=None</em>, <em>unbiased=True</em>, <em>keepdim=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.var" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.var" title="torch.var"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.var()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view">
<code class="descname">view</code><span class="sig-paren">(</span><em>*shape</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.view" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the same data as the <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor but of a
different <code class="xref py py-attr docutils literal notranslate"><span class="pre">shape</span></code>.</p>
<p>The returned tensor shares the same data and must have the same number
of elements, but may have a different size. For a tensor to be viewed, the new
view size must be compatible with its original size and stride, i.e., each new
view dimension must either be a subspace of an original dimension, or only span
across original dimensions <span class="math">\(d, d+1, \dots, d+k\)</span> that satisfy the following
contiguity-like condition that <span class="math">\(\forall i = 0, \dots, k-1\)</span>,</p>
<div class="math">
\[\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]\]</div>
<p>Otherwise, <a class="reference internal" href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-meth docutils literal notranslate"><span class="pre">contiguous()</span></code></a> needs to be called before the tensor can be
viewed. See also: <a class="reference internal" href="index.html#torch.reshape" title="torch.reshape"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reshape()</span></code></a>, which returns a view if the shapes are
compatible, and copies (equivalent to calling <a class="reference internal" href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-meth docutils literal notranslate"><span class="pre">contiguous()</span></code></a>) otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired size</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([16])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 8])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Swaps 2nd and 3rd dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 2, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># Does not change tensor layout in memory</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 2, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view_as">
<code class="descname">view_as</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.view_as" title="Permalink to this definition">¶</a></dt>
<dd><p>View this tensor as the same size as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.
<code class="docutils literal notranslate"><span class="pre">self.view_as(other)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">self.view(other.size())</span></code>.</p>
<p>Please see <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a> for more information about <code class="docutils literal notranslate"><span class="pre">view</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>) – The result tensor has the same size
as <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.zero_">
<code class="descname">zero_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.Tensor.zero_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills <code class="xref py py-attr docutils literal notranslate"><span class="pre">self</span></code> tensor with zeros.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.ByteTensor">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">ByteTensor</code><a class="headerlink" href="#torch.ByteTensor" title="Permalink to this definition">¶</a></dt>
<dd><p>The following methods are unique to <a class="reference internal" href="#torch.ByteTensor" title="torch.ByteTensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.ByteTensor</span></code></a>.</p>
<dl class="method">
<dt id="torch.ByteTensor.all">
<code class="descname">all</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.ByteTensor.all" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descname">all</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool</dt>
<dd></dd></dl>

<p>Returns True if all elements in the tensor are non-zero, False otherwise.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[1, 0, 0]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="go">tensor(0, dtype=torch.uint8)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descname">all</code><span class="sig-paren">(</span><em>dim</em>, <em>keepdim=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns True if all elements in each row of the tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> are non-zero, False otherwise.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="index.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting
in the output tensor having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 0],</span>
<span class="go">        [0, 0],</span>
<span class="go">        [0, 1],</span>
<span class="go">        [1, 1]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([0, 0, 0, 1], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.ByteTensor.any">
<code class="descname">any</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.ByteTensor.any" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descname">any</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool</dt>
<dd></dd></dl>

<p>Returns True if any elements in the tensor are non-zero, False otherwise.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[0, 0, 1]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
<span class="go">tensor(1, dtype=torch.uint8)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descname">any</code><span class="sig-paren">(</span><em>dim</em>, <em>keepdim=False</em>, <em>out=None</em><span class="sig-paren">)</span> &#x2192; Tensor</dt>
<dd></dd></dl>

<p>Returns True if any elements in each row of the tensor in the given
dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> are non-zero, False otherwise.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output tensor is of the same size as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> where it is of size 1.
Otherwise, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is squeezed (see <a class="reference internal" href="index.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting
in the output tensor having 1 fewer dimension than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([[1, 0],</span>
<span class="go">        [0, 0],</span>
<span class="go">        [0, 1],</span>
<span class="go">        [0, 0]], dtype=torch.uint8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">tensor([1, 0, 1, 0], dtype=torch.uint8)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<span id="document-tensor_attributes"></span><div class="section" id="tensor-attributes">
<span id="tensor-attributes-doc"></span><h2>Tensor Attributes<a class="headerlink" href="#tensor-attributes" title="Permalink to this headline">¶</a></h2>
<p>Each <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> has a <a class="reference internal" href="#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, <a class="reference internal" href="#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>, and <a class="reference internal" href="#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a>.</p>
<div class="section" id="torch-dtype">
<span id="dtype-doc"></span><h3>torch.dtype<a class="headerlink" href="#torch-dtype" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.torch.dtype">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">dtype</code><a class="headerlink" href="#torch.torch.dtype" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>A <a class="reference internal" href="#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> is an object that represents the data type of a
<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>. PyTorch has eight different data types:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 26%" />
<col style="width: 46%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Data type</p></th>
<th class="head"><p>dtype</p></th>
<th class="head"><p>Tensor types</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>32-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.float</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.*.FloatTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.double</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.*.DoubleTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>16-bit floating point</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.float16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.half</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.*.HalfTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>8-bit integer (unsigned)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.*.ByteTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>8-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int8</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.*.CharTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>16-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.short</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.*.ShortTensor</span></code></p></td>
</tr>
<tr class="row-even"><td><p>32-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int32</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.int</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.*.IntTensor</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64-bit integer (signed)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.int64</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.long</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.*.LongTensor</span></code></p></td>
</tr>
</tbody>
</table>
<p>To find out if a <a class="reference internal" href="#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> is a floating point data type, the property <a class="reference internal" href="index.html#torch.is_floating_point" title="torch.is_floating_point"><code class="xref py py-attr docutils literal notranslate"><span class="pre">is_floating_point</span></code></a>
can be used, which returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if the data type is a floating point data type.</p>
</div>
<div class="section" id="torch-device">
<span id="device-doc"></span><h3>torch.device<a class="headerlink" href="#torch-device" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.torch.device">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">device</code><a class="headerlink" href="#torch.torch.device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>A <a class="reference internal" href="#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> is an object representing the device on which a <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> is
or will be allocated.</p>
<p>The <a class="reference internal" href="#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> contains a device type (<code class="docutils literal notranslate"><span class="pre">'cpu'</span></code> or <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code>) and optional device ordinal for the
device type.  If the device ordinal is not present, this represents the current device for the device type;
e.g. a <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> constructed with device <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">'cuda:X'</span></code> where X is the result of
<a class="reference internal" href="index.html#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cuda.current_device()</span></code></a>.</p>
<p>A <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>’s device can be accessed via the <a class="reference internal" href="index.html#torch.Tensor.device" title="torch.Tensor.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Tensor.device</span></code></a> property.</p>
<p>A <a class="reference internal" href="#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> can be constructed via a string or via a string and device ordinal</p>
<p>Via a string:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="go">device(type=&#39;cpu&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>  <span class="c1"># current cuda device</span>
<span class="go">device(type=&#39;cuda&#39;)</span>
</pre></div>
</div>
<p>Via a string and device ordinal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">device(type=&#39;cuda&#39;, index=0)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">device(type=&#39;cpu&#39;, index=0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <a class="reference internal" href="#torch.torch.device" title="torch.torch.device"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a> argument in functions can generally be substituted with a string.
This allows for fast prototyping of code.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example of a function that takes in a torch.device</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cuda1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># You can substitute the torch.device with a string</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For legacy reasons, a device can be constructed via a single device ordinal, which is treated
as a cuda device.  This matches <a class="reference internal" href="index.html#torch.Tensor.get_device" title="torch.Tensor.get_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.get_device()</span></code></a>, which returns an ordinal for cuda
tensors and is not supported for cpu tensors.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">device(type=&#39;cuda&#39;, index=1)</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Methods which take a device will generally accept a (properly formatted) string
or (legacy) integer device ordinal, i.e. the following are all equivalent:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># legacy</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="torch-layout">
<span id="layout-doc"></span><h3>torch.layout<a class="headerlink" href="#torch-layout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.torch.layout">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">layout</code><a class="headerlink" href="#torch.torch.layout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>A <a class="reference internal" href="#torch.torch.layout" title="torch.torch.layout"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code></a> is an object that represents the memory layout of a
<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a>. Currently, we support <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code> (dense Tensors)
and have experimental support for <code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code> (sparse COO Tensors).</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.strided</span></code> represents dense Tensors and is the memory layout that
is most commonly used. Each strided tensor has an associated
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Storage</span></code>, which holds its data. These tensors provide
multi-dimensional, <a class="reference external" href="https://en.wikipedia.org/wiki/Stride_of_an_array">strided</a>
view of a storage. Strides are a list of integers: the k-th stride
represents the jump in the memory necessary to go from one element to the
next one in the k-th dimension of the Tensor. This concept makes it possible
to perform many tensor operations efficiently.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(5, 1)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
<span class="go">(1, 5)</span>
</pre></div>
</div>
<p>For more information on <code class="docutils literal notranslate"><span class="pre">torch.sparse_coo</span></code> tensors, see <a class="reference internal" href="index.html#sparse-docs"><span class="std std-ref">torch.sparse</span></a>.</p>
</div>
</div>
<span id="document-type_info"></span><div class="section" id="type-info">
<span id="type-info-doc"></span><h2>Type Info<a class="headerlink" href="#type-info" title="Permalink to this headline">¶</a></h2>
<p>The numerical properties of a <a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> can be accessed through either the <a class="reference internal" href="#torch.torch.finfo" title="torch.torch.finfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.finfo</span></code></a> or the <a class="reference internal" href="#torch.torch.iinfo" title="torch.torch.iinfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.iinfo</span></code></a>.</p>
<div class="section" id="torch-finfo">
<span id="finfo-doc"></span><h3>torch.finfo<a class="headerlink" href="#torch-finfo" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.torch.finfo">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">finfo</code><a class="headerlink" href="#torch.torch.finfo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>A <a class="reference internal" href="#torch.torch.finfo" title="torch.torch.finfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.finfo</span></code></a> is an object that represents the numerical properties of a floating point
<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a>, (i.e. <code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.float64</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>). This is similar to <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.finfo.html">numpy.finfo</a>.</p>
<p>A <a class="reference internal" href="#torch.torch.finfo" title="torch.torch.finfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.finfo</span></code></a> provides the following attributes:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 11%" />
<col style="width: 6%" />
<col style="width: 82%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bits</p></td>
<td><p>int</p></td>
<td><p>The number of bits occupied by the type.</p></td>
</tr>
<tr class="row-odd"><td><p>eps</p></td>
<td><p>float</p></td>
<td><p>The smallest representable number such that <code class="docutils literal notranslate"><span class="pre">1.0</span> <span class="pre">+</span> <span class="pre">eps</span> <span class="pre">!=</span> <span class="pre">1.0</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p>max</p></td>
<td><p>float</p></td>
<td><p>The largest representable number.</p></td>
</tr>
<tr class="row-odd"><td><p>min</p></td>
<td><p>float</p></td>
<td><p>The smallest representable number (typically <code class="docutils literal notranslate"><span class="pre">-max</span></code>).</p></td>
</tr>
<tr class="row-even"><td><p>tiny</p></td>
<td><p>float</p></td>
<td><p>The smallest positive representable number.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The constructor of <a class="reference internal" href="#torch.torch.finfo" title="torch.torch.finfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.finfo</span></code></a> can be called without argument, in which case the class is created for the pytorch default dtype (as returned by <a class="reference internal" href="index.html#torch.get_default_dtype" title="torch.get_default_dtype"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.get_default_dtype()</span></code></a>).</p>
</div>
</div>
<div class="section" id="torch-iinfo">
<span id="iinfo-doc"></span><h3>torch.iinfo<a class="headerlink" href="#torch-iinfo" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.torch.iinfo">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">iinfo</code><a class="headerlink" href="#torch.torch.iinfo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>A <a class="reference internal" href="#torch.torch.iinfo" title="torch.torch.iinfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.iinfo</span></code></a> is an object that represents the numerical properties of a integer
<a class="reference internal" href="index.html#torch.torch.dtype" title="torch.torch.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code></a> (i.e. <code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.int8</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.int16</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.int32</span></code>, and <code class="docutils literal notranslate"><span class="pre">torch.int64</span></code>). This is similar to <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.iinfo.html">numpy.iinfo</a>.</p>
<p>A <a class="reference internal" href="#torch.torch.iinfo" title="torch.torch.iinfo"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.iinfo</span></code></a> provides the following attributes:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 17%" />
<col style="width: 9%" />
<col style="width: 74%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bits</p></td>
<td><p>int</p></td>
<td><p>The number of bits occupied by the type.</p></td>
</tr>
<tr class="row-odd"><td><p>max</p></td>
<td><p>int</p></td>
<td><p>The largest representable number.</p></td>
</tr>
<tr class="row-even"><td><p>min</p></td>
<td><p>int</p></td>
<td><p>The smallest representable number.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<span id="document-sparse"></span><div class="section" id="torch-sparse">
<span id="sparse-docs"></span><h2>torch.sparse<a class="headerlink" href="#torch-sparse" title="Permalink to this headline">¶</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This API is currently experimental and may change in the near future.</p>
</div>
<p>Torch supports sparse tensors in COO(rdinate) format, which can
efficiently store and process tensors for which the majority of elements
are zeros.</p>
<p>A sparse tensor is represented as a pair of dense tensors: a tensor
of values and a 2D tensor of indices.  A sparse tensor can be constructed
by providing these two tensors, as well as the size of the sparse tensor
(which cannot be inferred from these tensors!)  Suppose we want to define
a sparse tensor with the entry 3 at location (0, 2), entry 4 at
location (1, 0), and entry 5 at location (1, 2).  We would then write:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="go">                          [2, 0, 2]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]))</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go"> 0  0  3</span>
<span class="go"> 4  0  5</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
<p>Note that the input to LongTensor is NOT a list of index tuples.  If you want
to write your indices this way, you should transpose before passing them to
the sparse constructor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span>      <span class="mi">4</span><span class="p">,</span>      <span class="mi">5</span>    <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]))</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go"> 0  0  3</span>
<span class="go"> 4  0  5</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
<p>You can also construct hybrid sparse tensors, where only the first n
dimensions are sparse, and the rest of the dimensions are dense.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span>
<span class="go"> 0  0</span>
<span class="go"> 0  0</span>
<span class="go"> 1  3</span>
<span class="go"> 0  0</span>
<span class="go"> 5  7</span>
<span class="go">[torch.FloatTensor of size 5x2]</span>
</pre></div>
</div>
<p>An empty sparse tensor can be constructed by specifying its size:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">SparseFloatTensor of size 2x3 with indices:</span>
<span class="go">[torch.LongTensor with no dimension]</span>
<span class="go">and values:</span>
<span class="go">[torch.FloatTensor with no dimension]</span>
</pre></div>
</div>
<dl class="simple">
<dt>SparseTensor has the following invariants:</dt><dd><ol class="arabic simple">
<li><p>sparse_dim + dense_dim = len(SparseTensor.shape)</p></li>
<li><p>SparseTensor._indices().shape = (sparse_dim, nnz)</p></li>
<li><p>SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])</p></li>
</ol>
</dd>
</dl>
<p>Since SparseTensor._indices() is always a 2D tensor, the smallest sparse_dim = 1.
Therefore, representation of a SparseTensor of sparse_dim = 0 is simply a dense tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Our sparse tensor format permits <em>uncoalesced</em> sparse tensors, where
there may be duplicate coordinates in the indices; in this case,
the interpretation is that the value at that index is the sum of all
duplicate value entries. Uncoalesced tensors permit us to implement
certain operators more efficiently.</p>
<p>For the most part, you shouldn’t have to care whether or not a
sparse tensor is coalesced or not, as most operations will work
identically given a coalesced or uncoalesced sparse tensor.
However, there are two cases in which you may need to care.</p>
<p>First, if you repeatedly perform an operation that can produce
duplicate entries (e.g., <a class="reference internal" href="#torch.sparse.FloatTensor.add" title="torch.sparse.FloatTensor.add"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse.FloatTensor.add()</span></code></a>), you
should occasionally coalesce your sparse tensors to prevent
them from growing too large.</p>
<p>Second, some operators will produce different values depending on
whether or not they are coalesced or not (e.g.,
<a class="reference internal" href="#torch.sparse.FloatTensor._values" title="torch.sparse.FloatTensor._values"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse.FloatTensor._values()</span></code></a> and
<a class="reference internal" href="#torch.sparse.FloatTensor._indices" title="torch.sparse.FloatTensor._indices"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse.FloatTensor._indices()</span></code></a>, as well as
<a class="reference internal" href="index.html#torch.Tensor.sparse_mask" title="torch.Tensor.sparse_mask"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.sparse_mask()</span></code></a>).  These operators are
prefixed by an underscore to indicate that they reveal internal
implementation details and should be used with care, since code
that works with coalesced sparse tensors may not work with
uncoalesced sparse tensors; generally speaking, it is safest
to explicitly coalesce before working with these operators.</p>
<p>For example, suppose that we wanted to implement an operator
by operating directly on <a class="reference internal" href="#torch.sparse.FloatTensor._values" title="torch.sparse.FloatTensor._values"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.sparse.FloatTensor._values()</span></code></a>.
Multiplication by a scalar can be implemented in the obvious way,
as multiplication distributes over addition; however, square root
cannot be implemented directly, since <code class="docutils literal notranslate"><span class="pre">sqrt(a</span> <span class="pre">+</span> <span class="pre">b)</span> <span class="pre">!=</span> <span class="pre">sqrt(a)</span> <span class="pre">+</span>
<span class="pre">sqrt(b)</span></code> (which is what would be computed if you were given an
uncoalesced tensor.)</p>
</div>
<dl class="class">
<dt id="torch.sparse.FloatTensor">
<em class="property">class </em><code class="descclassname">torch.sparse.</code><code class="descname">FloatTensor</code><a class="headerlink" href="#torch.sparse.FloatTensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="torch.sparse.FloatTensor.add">
<code class="descname">add</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.add" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.add_">
<code class="descname">add_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.add_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.clone" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.dim">
<code class="descname">dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.div">
<code class="descname">div</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.div" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.div_">
<code class="descname">div_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.div_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.get_device">
<code class="descname">get_device</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.get_device" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.hspmm">
<code class="descname">hspmm</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.hspmm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.mm">
<code class="descname">mm</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.mm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.mul">
<code class="descname">mul</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.mul" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.mul_">
<code class="descname">mul_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.mul_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.narrow_copy">
<code class="descname">narrow_copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.narrow_copy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.resizeAs_">
<code class="descname">resizeAs_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.resizeAs_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.size">
<code class="descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.spadd">
<code class="descname">spadd</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.spadd" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.spmm">
<code class="descname">spmm</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.spmm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.sspaddmm">
<code class="descname">sspaddmm</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.sspaddmm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.sspmm">
<code class="descname">sspmm</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.sspmm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.sub">
<code class="descname">sub</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.sub" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.sub_">
<code class="descname">sub_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.sub_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.t_">
<code class="descname">t_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.t_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.toDense">
<code class="descname">toDense</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.toDense" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.transpose">
<code class="descname">transpose</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.transpose" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.transpose_">
<code class="descname">transpose_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.transpose_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.zero_">
<code class="descname">zero_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.zero_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.coalesce">
<code class="descname">coalesce</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.coalesce" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor.is_coalesced">
<code class="descname">is_coalesced</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor.is_coalesced" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor._indices">
<code class="descname">_indices</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor._indices" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor._values">
<code class="descname">_values</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor._values" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.sparse.FloatTensor._nnz">
<code class="descname">_nnz</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.FloatTensor._nnz" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<div class="section" id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.sparse.addmm">
<code class="descclassname">torch.sparse.</code><code class="descname">addmm</code><span class="sig-paren">(</span><em>mat</em>, <em>mat1</em>, <em>mat2</em>, <em>beta=1</em>, <em>alpha=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.addmm" title="Permalink to this definition">¶</a></dt>
<dd><p>This function does exact same thing as <a class="reference internal" href="index.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.addmm()</span></code></a> in the forward,
except that it supports backward for sparse matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code>
need to have <cite>sparse_dim = 2</cite>. Note that the gradients of <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> is a
coalesced sparse tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a dense matrix to be added</p></li>
<li><p><strong>mat1</strong> (<em>SparseTensor</em>) – a sparse matrix to be multiplied</p></li>
<li><p><strong>mat2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a dense matrix be multiplied</p></li>
<li><p><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat</span></code> (<span class="math">\(\beta\)</span>)</p></li>
<li><p><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) – multiplier for <span class="math">\(mat1 &#64; mat2\)</span> (<span class="math">\(\alpha\)</span>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.sparse.mm">
<code class="descclassname">torch.sparse.</code><code class="descname">mm</code><span class="sig-paren">(</span><em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.mm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix multiplication of the sparse matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code>
and dense matrix <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>. Similar to <a class="reference internal" href="index.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.mm()</span></code></a>, If <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> is a
<span class="math">\((n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code> is a <span class="math">\((m \times p)\)</span> tensor, out will be a
<span class="math">\((n \times p)\)</span> dense tensor. <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> need to have <cite>sparse_dim = 2</cite>.
This function also supports backward for both matrices. Note that the gradients of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">mat1</span></code> is a coalesced sparse tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mat1</strong> (<em>SparseTensor</em>) – the first sparse matrix to be multiplied</p></li>
<li><p><strong>mat2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the second dense matrix to be multiplied</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor(indices=tensor([[0, 0, 0, 1, 1, 1],</span>
<span class="go">                       [0, 1, 2, 0, 1, 2]]),</span>
<span class="go">       values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),</span>
<span class="go">       size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([[-0.6479,  0.7874],</span>
<span class="go">        [-1.2056,  0.5641],</span>
<span class="go">        [-1.1716, -0.9923]], requires_grad=True)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">tensor([[-0.3323,  1.8723],</span>
<span class="go">        [-1.8951,  0.7904]], grad_fn=&lt;SparseAddmmBackward&gt;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor(indices=tensor([[0, 0, 0, 1, 1, 1],</span>
<span class="go">                       [0, 1, 2, 0, 1, 2]]),</span>
<span class="go">       values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),</span>
<span class="go">       size=(2, 3), nnz=6, layout=torch.sparse_coo)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sparse.sum">
<code class="descclassname">torch.sparse.</code><code class="descname">sum</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.sparse.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the sum of each row of SparseTensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> in the given
dimensions <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is a list of dimensions,
reduce over all of them. When sum over all <code class="docutils literal notranslate"><span class="pre">sparse_dim</span></code>, this method
returns a Tensor instead of SparseTensor.</p>
<p>All summed <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> are squeezed (see <a class="reference internal" href="index.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code></a>), resulting an output
tensor having <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> fewer dimensions than <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>During backward, only gradients at <code class="docutils literal notranslate"><span class="pre">nnz</span></code> locations of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
will propagate back. Note that the gradients of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is coalesced.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input SparseTensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>tuple of python:ints</em>) – a dimension or a list of dimensions to reduce. Default: reduce
over all dims.</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned Tensor.
Default: dtype of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nnz</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">nnz</span><span class="p">,)),</span>
<span class="go">                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nnz</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span>
<span class="go">tensor(indices=tensor([[2, 0, 3],</span>
<span class="go">                       [2, 4, 1]]),</span>
<span class="go">       values=tensor([[[-0.6438, -1.6467,  1.4004],</span>
<span class="go">                       [ 0.3411,  0.0918, -0.2312]],</span>

<span class="go">                      [[ 0.5348,  0.0634, -2.0494],</span>
<span class="go">                       [-0.7125, -1.0646,  2.1844]],</span>

<span class="go">                      [[ 0.1276,  0.1874, -0.6334],</span>
<span class="go">                       [-1.9682, -0.5340,  0.7483]]]),</span>
<span class="go">       size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)</span>

<span class="go"># when sum over only part of sparse_dims, return a SparseTensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">tensor(indices=tensor([[0, 2, 3]]),</span>
<span class="go">       values=tensor([[-1.4512,  0.4073],</span>
<span class="go">                      [-0.8901,  0.2017],</span>
<span class="go">                      [-0.3183, -1.7539]]),</span>
<span class="go">       size=(5, 2), nnz=3, layout=torch.sparse_coo)</span>

<span class="go"># when sum over all sparse dim, return a dense Tensor</span>
<span class="go"># with summed dims squeezed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">tensor([-2.6596, -1.1450])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<span id="document-cuda"></span><div class="section" id="module-torch.cuda">
<span id="torch-cuda"></span><h2>torch.cuda<a class="headerlink" href="#module-torch.cuda" title="Permalink to this headline">¶</a></h2>
<p>This package adds support for CUDA tensor types, that implement the same
function as CPU tensors, but they utilize GPUs for computation.</p>
<p>It is lazily initialized, so you can always import it, and use
<a class="reference internal" href="#torch.cuda.is_available" title="torch.cuda.is_available"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_available()</span></code></a> to determine if your system supports CUDA.</p>
<p><span class="xref std std-ref">cuda-semantics</span> has more details about working with CUDA.</p>
<dl class="function">
<dt id="torch.cuda.current_blas_handle">
<code class="descclassname">torch.cuda.</code><code class="descname">current_blas_handle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.current_blas_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cublasHandle_t pointer to current cuBLAS handle</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.current_device">
<code class="descclassname">torch.cuda.</code><code class="descname">current_device</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.current_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the index of a currently selected device.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.current_stream">
<code class="descclassname">torch.cuda.</code><code class="descname">current_stream</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.current_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the currently selected <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
the currently selected <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for the current device, given
by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code>
(default).</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.default_stream">
<code class="descclassname">torch.cuda.</code><code class="descname">default_stream</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.default_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the default <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
the default <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for the current device, given by
<a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code>
(default).</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.cuda.device">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">device</code><span class="sig-paren">(</span><em>device</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that changes the selected device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – device index to select. It’s a no-op if
this argument is a negative integer or <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.device_count">
<code class="descclassname">torch.cuda.</code><code class="descname">device_count</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of GPUs available.</p>
</dd></dl>

<dl class="class">
<dt id="torch.cuda.device_of">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">device_of</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.device_of" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that changes the current device to that of given object.</p>
<p>You can use both tensors and storages as arguments. If a given object is
not allocated on a GPU, this is a no-op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>obj</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) – object allocated on the selected device.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.empty_cache">
<code class="descclassname">torch.cuda.</code><code class="descname">empty_cache</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.empty_cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Releases all unoccupied cached memory currently held by the caching
allocator so that those can be used in other GPU application and visible in
<cite>nvidia-smi</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code class="xref py py-meth docutils literal notranslate"><span class="pre">empty_cache()</span></code></a> doesn’t increase the amount of GPU
memory available for PyTorch. See <span class="xref std std-ref">cuda-memory-management</span> for
more details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_device_capability">
<code class="descclassname">torch.cuda.</code><code class="descname">get_device_capability</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.get_device_capability" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the cuda capability of a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – device for which to return the
device capability. This function is a no-op if this argument is
a negative integer. Uses the current device, given by
<a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code>
(default).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the major and minor cuda capability of the device</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)">tuple</a>(<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)">int</a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_device_name">
<code class="descclassname">torch.cuda.</code><code class="descname">get_device_name</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.get_device_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the name of a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – device for which to return the
name. This function is a no-op if this argument is a negative
integer. Uses the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.init">
<code class="descclassname">torch.cuda.</code><code class="descname">init</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.init" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize PyTorch’s CUDA state.  You may need to call
this explicitly if you are interacting with PyTorch via
its C API, as Python bindings for CUDA functionality will not
be until this initialization takes place.  Ordinary users
should not need this, as all of PyTorch’s CUDA methods
automatically initialize CUDA state on-demand.</p>
<p>Does nothing if the CUDA state is already initialized.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.is_available">
<code class="descclassname">torch.cuda.</code><code class="descname">is_available</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.is_available" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a bool indicating if CUDA is currently available.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.max_memory_allocated">
<code class="descclassname">torch.cuda.</code><code class="descname">max_memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.max_memory_allocated" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the maximum GPU memory occupied by tensors in bytes for a given
device.</p>
<p>By default, this returns the peak allocated memory since the beginning of
this program. <a class="reference internal" href="#torch.cuda.reset_max_memory_allocated" title="torch.cuda.reset_max_memory_allocated"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_max_memory_allocated()</span></code></a> can be used to
reset the starting point in tracking this metric. For example, these two
functions can measure the peak allocated memory usage of each iteration in a
training loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.max_memory_cached">
<code class="descclassname">torch.cuda.</code><code class="descname">max_memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.max_memory_cached" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the maximum GPU memory managed by the caching allocator in bytes
for a given device.</p>
<p>By default, this returns the peak cached memory since the beginning of this
program. <a class="reference internal" href="#torch.cuda.reset_max_memory_cached" title="torch.cuda.reset_max_memory_cached"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_max_memory_cached()</span></code></a> can be used to reset
the starting point in tracking this metric. For example, these two functions
can measure the peak cached memory amount of each iteration in a training
loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_allocated">
<code class="descclassname">torch.cuda.</code><code class="descname">memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.memory_allocated" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current GPU memory occupied by tensors in bytes for a given
device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is likely less than the amount shown in <cite>nvidia-smi</cite> since some
unused memory can be held by the caching allocator and some context
needs to be created on GPU. See <span class="xref std std-ref">cuda-memory-management</span> for more
details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_cached">
<code class="descclassname">torch.cuda.</code><code class="descname">memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.memory_cached" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current GPU memory managed by the caching allocator in bytes
for a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.reset_max_memory_allocated">
<code class="descclassname">torch.cuda.</code><code class="descname">reset_max_memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.reset_max_memory_allocated" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the starting point in tracking maximum GPU memory occupied by
tensors for a given device.</p>
<p>See <a class="reference internal" href="#torch.cuda.max_memory_allocated" title="torch.cuda.max_memory_allocated"><code class="xref py py-func docutils literal notranslate"><span class="pre">max_memory_allocated()</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.reset_max_memory_cached">
<code class="descclassname">torch.cuda.</code><code class="descname">reset_max_memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.reset_max_memory_cached" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the starting point in tracking maximum GPU memory managed by the
caching allocator for a given device.</p>
<p>See <a class="reference internal" href="#torch.cuda.max_memory_cached" title="torch.cuda.max_memory_cached"><code class="xref py py-func docutils literal notranslate"><span class="pre">max_memory_cached()</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_device">
<code class="descclassname">torch.cuda.</code><code class="descname">set_device</code><span class="sig-paren">(</span><em>device</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.set_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the current device.</p>
<p>Usage of this function is discouraged in favor of <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref any py py-class docutils literal notranslate"><span class="pre">device</span></code></a>. In most
cases it’s better to use <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environmental variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – selected device. This function is a no-op
if this argument is negative.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.stream">
<code class="descclassname">torch.cuda.</code><code class="descname">stream</code><span class="sig-paren">(</span><em>stream</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that selects a given stream.</p>
<p>All CUDA kernels queued within its context will be enqueued on a selected
stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stream</strong> (<a class="reference internal" href="index.html#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a>) – selected stream. This manager is a no-op if it’s
<code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Streams are per-device. If the selected stream is not on the
current device, this function will also change the current device to
match the stream.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.synchronize">
<code class="descclassname">torch.cuda.</code><code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all kernels in all streams on current device to complete.</p>
</dd></dl>

<div class="section" id="random-number-generator">
<h3>Random Number Generator<a class="headerlink" href="#random-number-generator" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.cuda.get_rng_state">
<code class="descclassname">torch.cuda.</code><code class="descname">get_rng_state</code><span class="sig-paren">(</span><em>device=device(type='cuda')</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.get_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the random number generator state of the current
GPU as a ByteTensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – The device to return the RNG state of.
Default: <code class="docutils literal notranslate"><span class="pre">torch.device('cuda')</span></code> (i.e., the current CUDA device).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function eagerly initializes CUDA.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_rng_state_all">
<code class="descclassname">torch.cuda.</code><code class="descname">get_rng_state_all</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.get_rng_state_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tuple of ByteTensor representing the random number states of all devices.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_rng_state">
<code class="descclassname">torch.cuda.</code><code class="descname">set_rng_state</code><span class="sig-paren">(</span><em>new_state</em>, <em>device=device(type='cuda')</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.set_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state of the current GPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>new_state</strong> (<a class="reference internal" href="index.html#torch.ByteTensor" title="torch.ByteTensor"><em>torch.ByteTensor</em></a>) – The desired state</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – The device to set the RNG state.
Default: <code class="docutils literal notranslate"><span class="pre">torch.device('cuda')</span></code> (i.e., the current CUDA device).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_rng_state_all">
<code class="descclassname">torch.cuda.</code><code class="descname">set_rng_state_all</code><span class="sig-paren">(</span><em>new_states</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.set_rng_state_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state of all devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_state</strong> (<em>tuple of torch.ByteTensor</em>) – The desired state for each device</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.manual_seed">
<code class="descclassname">torch.cuda.</code><code class="descname">manual_seed</code><span class="sig-paren">(</span><em>seed</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.manual_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers for the current GPU.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The desired seed.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you are working with a multi-GPU model, this function is insufficient
to get determinism.  To seed all GPUs, use <a class="reference internal" href="#torch.cuda.manual_seed_all" title="torch.cuda.manual_seed_all"><code class="xref py py-func docutils literal notranslate"><span class="pre">manual_seed_all()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.manual_seed_all">
<code class="descclassname">torch.cuda.</code><code class="descname">manual_seed_all</code><span class="sig-paren">(</span><em>seed</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.manual_seed_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers on all GPUs.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The desired seed.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.seed">
<code class="descclassname">torch.cuda.</code><code class="descname">seed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers to a random number for the current GPU.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you are working with a multi-GPU model, this function will only initialize
the seed on one GPU.  To initialize all GPUs, use <a class="reference internal" href="#torch.cuda.seed_all" title="torch.cuda.seed_all"><code class="xref py py-func docutils literal notranslate"><span class="pre">seed_all()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.seed_all">
<code class="descclassname">torch.cuda.</code><code class="descname">seed_all</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.seed_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers to a random number on all GPUs.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.initial_seed">
<code class="descclassname">torch.cuda.</code><code class="descname">initial_seed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.initial_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current random seed of the current GPU.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function eagerly initializes CUDA.</p>
</div>
</dd></dl>

</div>
<div class="section" id="communication-collectives">
<h3>Communication collectives<a class="headerlink" href="#communication-collectives" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.cuda.comm.broadcast">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">broadcast</code><span class="sig-paren">(</span><em>tensor</em>, <em>devices</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts a tensor to a number of GPUs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor to broadcast.</p></li>
<li><p><strong>devices</strong> (<em>Iterable</em>) – an iterable of devices among which to broadcast.
Note that it should be like (src, dst1, dst2, …), the first element
of which is the source device to broadcast from.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing copies of the <code class="docutils literal notranslate"><span class="pre">tensor</span></code>, placed on devices
corresponding to indices from <code class="docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.broadcast_coalesced">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">broadcast_coalesced</code><span class="sig-paren">(</span><em>tensors</em>, <em>devices</em>, <em>buffer_size=10485760</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.broadcast_coalesced" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts a sequence tensors to the specified GPUs.
Small tensors are first coalesced into a buffer to reduce the number
of synchronizations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> (<em>sequence</em>) – tensors to broadcast.</p></li>
<li><p><strong>devices</strong> (<em>Iterable</em>) – an iterable of devices among which to broadcast.
Note that it should be like (src, dst1, dst2, …), the first element
of which is the source device to broadcast from.</p></li>
<li><p><strong>buffer_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – maximum size of the buffer used for coalescing</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing copies of the <code class="docutils literal notranslate"><span class="pre">tensor</span></code>, placed on devices
corresponding to indices from <code class="docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.reduce_add">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">reduce_add</code><span class="sig-paren">(</span><em>inputs</em>, <em>destination=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.reduce_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Sums tensors from multiple GPUs.</p>
<p>All inputs should have matching shapes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterable of tensors to add.</p></li>
<li><p><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – a device on which the output will be
placed (default: current device).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor containing an elementwise sum of all inputs, placed on the
<code class="docutils literal notranslate"><span class="pre">destination</span></code> device.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.scatter">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">scatter</code><span class="sig-paren">(</span><em>tensor</em>, <em>devices</em>, <em>chunk_sizes=None</em>, <em>dim=0</em>, <em>streams=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatters tensor across multiple GPUs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor to scatter.</p></li>
<li><p><strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – iterable of ints, specifying among which
devices the tensor should be scattered.</p></li>
<li><p><strong>chunk_sizes</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – sizes of chunks to be placed on
each device. It should match <code class="docutils literal notranslate"><span class="pre">devices</span></code> in length and sum to
<code class="docutils literal notranslate"><span class="pre">tensor.size(dim)</span></code>. If not specified, the tensor will be divided
into equal chunks.</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – A dimension along which to chunk the tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing chunks of the <code class="docutils literal notranslate"><span class="pre">tensor</span></code>, spread across given
<code class="docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.gather">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">gather</code><span class="sig-paren">(</span><em>tensors</em>, <em>dim=0</em>, <em>destination=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers tensors from multiple GPUs.</p>
<p>Tensor sizes in all dimension different than <code class="docutils literal notranslate"><span class="pre">dim</span></code> have to match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – iterable of tensors to gather.</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – a dimension along which the tensors will be concatenated.</p></li>
<li><p><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – output device (-1 means CPU, default:
current device)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor located on <code class="docutils literal notranslate"><span class="pre">destination</span></code> device, that is a result of
concatenating <code class="docutils literal notranslate"><span class="pre">tensors</span></code> along <code class="docutils literal notranslate"><span class="pre">dim</span></code>.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="streams-and-events">
<h3>Streams and events<a class="headerlink" href="#streams-and-events" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.cuda.Stream">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">Stream</code><a class="headerlink" href="#torch.cuda.Stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around a CUDA stream.</p>
<p>A CUDA stream is a linear sequence of execution that belongs to a specific
device, independent from other streams.  See <span class="xref std std-ref">cuda-semantics</span> for
details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – a device on which to allocate
the stream. If <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default) or a negative
integer, this will use the current device.</p></li>
<li><p><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – priority of the stream. Lower numbers
represent higher priorities.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.cuda.Stream.query">
<code class="descname">query</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.query" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if all the work submitted has been completed.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A boolean indicating if all kernels in this stream are completed.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.record_event">
<code class="descname">record_event</code><span class="sig-paren">(</span><em>event=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.record_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Records an event.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>event</strong> (<a class="reference internal" href="index.html#torch.cuda.Event" title="torch.cuda.Event"><em>Event</em></a><em>, </em><em>optional</em>) – event to record. If not given, a new one
will be allocated.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Recorded event.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.synchronize">
<code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Wait for all the kernels in this stream to complete.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a wrapper around <code class="docutils literal notranslate"><span class="pre">cudaStreamSynchronize()</span></code>: see
<a href="#id4"><span class="problematic" id="id5">`CUDA documentation`_</span></a> for more info.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.wait_event">
<code class="descname">wait_event</code><span class="sig-paren">(</span><em>event</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.wait_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes all future work submitted to the stream wait for an event.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>event</strong> (<a class="reference internal" href="index.html#torch.cuda.Event" title="torch.cuda.Event"><em>Event</em></a>) – an event to wait for.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a wrapper around <code class="docutils literal notranslate"><span class="pre">cudaStreamWaitEvent()</span></code>: see <a href="#id6"><span class="problematic" id="id7">`CUDA
documentation`_</span></a> for more info.</p>
<p>This function returns without waiting for <code class="xref py py-attr docutils literal notranslate"><span class="pre">event</span></code>: only future
operations are affected.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.wait_stream">
<code class="descname">wait_stream</code><span class="sig-paren">(</span><em>stream</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.wait_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Synchronizes with another stream.</p>
<p>All future work submitted to this stream will wait until all kernels
submitted to a given stream at the time of call complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stream</strong> (<a class="reference internal" href="index.html#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a>) – a stream to synchronize.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function returns without waiting for currently enqueued
kernels in <a class="reference internal" href="#torch.cuda.stream" title="torch.cuda.stream"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stream</span></code></a>: only future operations are affected.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.cuda.Event">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">Event</code><a class="headerlink" href="#torch.cuda.Event" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around a CUDA event.</p>
<p>CUDA events are synchronization markers that can be used to monitor the
device’s progress, to accurately measure timing, and to synchronize CUDA
streams.</p>
<p>The underlying CUDA events are lazily initialized when the event is first
recorded or exported to another process. After creation, only streams on the
same device may record the event. However, streams on any device can wait on
the event.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>enable_timing</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – indicates if the event should measure time
(default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, <a class="reference internal" href="#torch.cuda.Event.wait" title="torch.cuda.Event.wait"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wait()</span></code></a> will be blocking (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>interprocess</strong> (<span class="target" id="id2"></span>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the event can be shared between processes
(default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.cuda.Event.elapsed_time">
<code class="descname">elapsed_time</code><span class="sig-paren">(</span><em>end_event</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.elapsed_time" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the time elapsed in milliseconds after the event was
recorded and before the end_event was recorded.</p>
</dd></dl>

<dl class="classmethod">
<dt id="torch.cuda.Event.from_ipc_handle">
<em class="property">classmethod </em><code class="descname">from_ipc_handle</code><span class="sig-paren">(</span><em>device</em>, <em>handle</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.from_ipc_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Reconstruct an event from an IPC handle on the given device.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.ipc_handle">
<code class="descname">ipc_handle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.ipc_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an IPC handle of this event. If not recorded yet, the event
will use the current device.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.query">
<code class="descname">query</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.query" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if all work currently captured by event has completed.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A boolean indicating if all work currently captured by event has
completed.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.record">
<code class="descname">record</code><span class="sig-paren">(</span><em>stream=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.record" title="Permalink to this definition">¶</a></dt>
<dd><p>Records the event in a given stream.</p>
<p>Uses <code class="docutils literal notranslate"><span class="pre">torch.cuda.current_stream()</span></code> if no stream is specified. The
stream’s device must match the event’s device.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.synchronize">
<code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for the event to complete.</p>
<p>Waits until the completion of all work currently captured in this event.
This prevents the CPU thread from proceeding until the event completes.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a wrapper around <code class="docutils literal notranslate"><span class="pre">cudaEventSynchronize()</span></code>: see <a href="#id8"><span class="problematic" id="id9">`CUDA
documentation`_</span></a> for more info.</p>
</div>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.wait">
<code class="descname">wait</code><span class="sig-paren">(</span><em>stream=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.wait" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes all future work submitted to the given stream wait for this
event.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">torch.cuda.current_stream()</span></code> if no stream is specified.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="memory-management">
<h3>Memory management<a class="headerlink" href="#memory-management" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">empty_cache</code><span class="sig-paren">(</span><span class="sig-paren">)</span></dt>
<dd><p>Releases all unoccupied cached memory currently held by the caching
allocator so that those can be used in other GPU application and visible in
<cite>nvidia-smi</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code class="xref py py-meth docutils literal notranslate"><span class="pre">empty_cache()</span></code></a> doesn’t increase the amount of GPU
memory available for PyTorch. See <span class="xref std std-ref">cuda-memory-management</span> for
more details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span></dt>
<dd><p>Returns the current GPU memory occupied by tensors in bytes for a given
device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is likely less than the amount shown in <cite>nvidia-smi</cite> since some
unused memory can be held by the caching allocator and some context
needs to be created on GPU. See <span class="xref std std-ref">cuda-memory-management</span> for more
details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">max_memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span></dt>
<dd><p>Returns the maximum GPU memory occupied by tensors in bytes for a given
device.</p>
<p>By default, this returns the peak allocated memory since the beginning of
this program. <a class="reference internal" href="#torch.cuda.reset_max_memory_allocated" title="torch.cuda.reset_max_memory_allocated"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_max_memory_allocated()</span></code></a> can be used to
reset the starting point in tracking this metric. For example, these two
functions can measure the peak allocated memory usage of each iteration in a
training loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">reset_max_memory_allocated</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span></dt>
<dd><p>Resets the starting point in tracking maximum GPU memory occupied by
tensors for a given device.</p>
<p>See <a class="reference internal" href="#torch.cuda.max_memory_allocated" title="torch.cuda.max_memory_allocated"><code class="xref py py-func docutils literal notranslate"><span class="pre">max_memory_allocated()</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span></dt>
<dd><p>Returns the current GPU memory managed by the caching allocator in bytes
for a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">max_memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span></dt>
<dd><p>Returns the maximum GPU memory managed by the caching allocator in bytes
for a given device.</p>
<p>By default, this returns the peak cached memory since the beginning of this
program. <a class="reference internal" href="#torch.cuda.reset_max_memory_cached" title="torch.cuda.reset_max_memory_cached"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_max_memory_cached()</span></code></a> can be used to reset
the starting point in tracking this metric. For example, these two functions
can measure the peak cached memory amount of each iteration in a training
loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt>
<code class="descclassname">torch.cuda.</code><code class="descname">reset_max_memory_cached</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span></dt>
<dd><p>Resets the starting point in tracking maximum GPU memory managed by the
caching allocator for a given device.</p>
<p>See <a class="reference internal" href="#torch.cuda.max_memory_cached" title="torch.cuda.max_memory_cached"><code class="xref py py-func docutils literal notranslate"><span class="pre">max_memory_cached()</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <span class="xref std std-ref">cuda-memory-management</span> for more details about GPU memory
management.</p>
</div>
</dd></dl>

</div>
<div class="section" id="nvidia-tools-extension-nvtx">
<h3>NVIDIA Tools Extension (NVTX)<a class="headerlink" href="#nvidia-tools-extension-nvtx" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.cuda.nvtx.mark">
<code class="descclassname">torch.cuda.nvtx.</code><code class="descname">mark</code><span class="sig-paren">(</span><em>msg</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.nvtx.mark" title="Permalink to this definition">¶</a></dt>
<dd><p>Describe an instantaneous event that occurred at some point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>msg</strong> (<em>string</em>) – ASCII message to associate with the event.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.nvtx.range_push">
<code class="descclassname">torch.cuda.nvtx.</code><code class="descname">range_push</code><span class="sig-paren">(</span><em>msg</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.nvtx.range_push" title="Permalink to this definition">¶</a></dt>
<dd><p>Pushes a range onto a stack of nested range span.  Returns zero-based
depth of the range that is started.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>msg</strong> (<em>string</em>) – ASCII message to associate with range</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.nvtx.range_pop">
<code class="descclassname">torch.cuda.nvtx.</code><code class="descname">range_pop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.nvtx.range_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Pops a range off of a stack of nested range spans.  Returns the
zero-based depth of the range that is ended.</p>
</dd></dl>

</div>
</div>
<span id="document-storage"></span><div class="section" id="torch-storage">
<h2>torch.Storage<a class="headerlink" href="#torch-storage" title="Permalink to this headline">¶</a></h2>
<p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Storage</span></code> is a contiguous, one-dimensional array of a single
data type.</p>
<p>Every <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> has a corresponding storage of the same data type.</p>
<dl class="class">
<dt id="torch.FloatStorage">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">FloatStorage</code><a class="headerlink" href="#torch.FloatStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="torch.FloatStorage.bool">
<code class="descname">bool</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.bool" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to bool type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.byte">
<code class="descname">byte</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.byte" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to byte type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.char">
<code class="descname">char</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.char" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to char type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.copy_">
<code class="descname">copy_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.copy_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a CPU copy of this storage if it’s not already on the CPU</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em>, <em>non_blocking=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The destination GPU id. Defaults to the current device.</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the source is in pinned memory,
the copy will be asynchronous with respect to the host. Otherwise,
the argument has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.data_ptr">
<code class="descname">data_ptr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to double type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.element_size">
<code class="descname">element_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.element_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.fill_">
<code class="descname">fill_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.fill_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to float type</p>
</dd></dl>

<dl class="staticmethod">
<dt id="torch.FloatStorage.from_buffer">
<em class="property">static </em><code class="descname">from_buffer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.from_buffer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="staticmethod">
<dt id="torch.FloatStorage.from_file">
<em class="property">static </em><code class="descname">from_file</code><span class="sig-paren">(</span><em>filename</em>, <em>shared=False</em>, <em>size=0</em><span class="sig-paren">)</span> &#x2192; Storage<a class="headerlink" href="#torch.FloatStorage.from_file" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>shared</cite> is <cite>True</cite>, then memory is shared between all processes.
All changes are written to the file. If <cite>shared</cite> is <cite>False</cite>, then the changes on
the storage do not affect the file.</p>
<p><cite>size</cite> is the number of elements in the storage. If <cite>shared</cite> is <cite>False</cite>,
then the file must contain at least <cite>size * sizeof(Type)</cite> bytes
(<cite>Type</cite> is the type of storage). If <cite>shared</cite> is <cite>True</cite> the file will be
created if needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – file name to map</p></li>
<li><p><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to share memory</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of elements in the storage</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to half type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.int">
<code class="descname">int</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.int" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to int type</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.FloatStorage.is_cuda">
<code class="descname">is_cuda</code><em class="property"> = False</em><a class="headerlink" href="#torch.FloatStorage.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.is_pinned">
<code class="descname">is_pinned</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.is_shared">
<code class="descname">is_shared</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.is_shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.FloatStorage.is_sparse">
<code class="descname">is_sparse</code><em class="property"> = False</em><a class="headerlink" href="#torch.FloatStorage.is_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.long">
<code class="descname">long</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.long" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to long type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.new">
<code class="descname">new</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.new" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.pin_memory">
<code class="descname">pin_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the storage to pinned memory, if it’s not already pinned.</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.resize_">
<code class="descname">resize_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.resize_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.share_memory_">
<code class="descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the storage to shared memory.</p>
<p>This is a no-op for storages already in shared memory and for CUDA
storages, which do not need to be moved for sharing across processes.
Storages in shared memory cannot be resized.</p>
<p>Returns: self</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.short">
<code class="descname">short</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.short" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to short type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.size">
<code class="descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.tolist">
<code class="descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list containing the elements of this storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>dtype=None</em>, <em>non_blocking=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the type if <cite>dtype</cite> is not provided, else casts this object to
the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a><em> or </em><em>string</em>) – The desired type</p></li>
<li><p><strong>non_blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, and the source is in pinned memory
and destination is on the GPU or vice versa, the copy is performed
asynchronously with respect to the host. Otherwise, the argument
has no effect.</p></li>
<li><p><strong>**kwargs</strong> – For compatibility, may contain the key <code class="docutils literal notranslate"><span class="pre">async</span></code> in place of
the <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> argument. The <code class="docutils literal notranslate"><span class="pre">async</span></code> arg is deprecated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<span id="document-nn"></span><div class="section" id="module-torch.nn">
<span id="torch-nn"></span><h2>torch.nn<a class="headerlink" href="#module-torch.nn" title="Permalink to this headline">¶</a></h2>
<div class="section" id="parameters">
<h3>Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Parameter">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Parameter</code><a class="headerlink" href="#torch.nn.Parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>A kind of Tensor that is to be considered a module parameter.</p>
<p>Parameters are <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> subclasses, that have a
very special property when used with <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> s - when they’re
assigned as Module attributes they are automatically added to the list of
its parameters, and will appear e.g. in <a class="reference internal" href="#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parameters()</span></code></a> iterator.
Assigning a Tensor doesn’t have such effect. This is because one might
want to cache some temporary state, like last hidden state of the RNN, in
the model. If there was no such class as <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>, these
temporaries would get registered too.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – parameter tensor.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if the parameter requires gradient. See
<span class="xref std std-ref">excluding-subgraphs</span> for more details. Default: <cite>True</cite></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="containers">
<h3>Containers<a class="headerlink" href="#containers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="module">
<h4><span class="hidden-section">Module</span><a class="headerlink" href="#module" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Module">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Module</code><a class="headerlink" href="#torch.nn.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
       <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
       <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call <a class="reference internal" href="#torch.nn.Module.to" title="torch.nn.Module.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code></a>, etc.</p>
<dl class="method">
<dt id="torch.nn.Module.add_module">
<code class="descname">add_module</code><span class="sig-paren">(</span><em>name</em>, <em>module</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>parameter</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – child module to be added to the module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.apply">
<code class="descname">apply</code><span class="sig-paren">(</span><em>fn</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">torch-nn-init</span>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="go">        print(m)</span>
<span class="go">        if type(m) == nn.Linear:</span>
<span class="go">            m.weight.data.fill_(1.0)</span>
<span class="go">            print(m.weight)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.buffers">
<code class="descname">buffers</code><span class="sig-paren">(</span><em>recurse=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>torch.Tensor</em> – module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.children">
<code class="descname">children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a child module</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="torch.nn.Module.dump_patches">
<code class="descname">dump_patches</code><em class="property"> = False</em><a class="headerlink" href="#torch.nn.Module.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.eval">
<code class="descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <a class="reference internal" href="#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.extra_repr">
<code class="descname">extra_repr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should reimplement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to float datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>*input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.load_state_dict">
<code class="descname">load_state_dict</code><span class="sig-paren">(</span><em>state_dict</em>, <em>strict=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.modules">
<code class="descname">modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_buffers">
<code class="descname">named_buffers</code><span class="sig-paren">(</span><em>prefix=''</em>, <em>recurse=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_children">
<code class="descname">named_children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_modules">
<code class="descname">named_modules</code><span class="sig-paren">(</span><em>memo=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_parameters">
<code class="descname">named_parameters</code><span class="sig-paren">(</span><em>prefix=''</em>, <em>recurse=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – prefix to prepend to all parameter names.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><em>recurse=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Parameter</em> – module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_backward_hook">
<code class="descname">register_backward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> may be tuples if the
module has multiple inputs or outputs. The hook should not modify its
arguments, but it can optionally return a new gradient with respect to
input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in subsequent
computations.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The current implementation will not have the presented behavior
for complex <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> that perform many operations.
In some failure cases, <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will only
contain the gradients for a subset of the inputs and outputs.
For such <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a>, you should use <a class="reference internal" href="index.html#torch.Tensor.register_hook" title="torch.Tensor.register_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.register_hook()</span></code></a>
directly on a specific input or output to get the required gradients.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_buffer">
<code class="descname">register_buffer</code><span class="sig-paren">(</span><em>name</em>, <em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a persistent buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the persistent state.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – buffer to be registered.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_hook">
<code class="descname">register_forward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input or output.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_pre_hook">
<code class="descname">register_forward_pre_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_parameter">
<code class="descname">register_parameter</code><span class="sig-paren">(</span><em>name</em>, <em>param</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>parameter</strong> (<a class="reference internal" href="index.html#torch.nn.Parameter" title="torch.nn.Parameter"><em>Parameter</em></a>) – parameter to be added to the module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.state_dict">
<code class="descname">state_dict</code><span class="sig-paren">(</span><em>destination=None</em>, <em>prefix=''</em>, <em>keep_vars=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)">dict</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.to">
<code class="descname">to</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as</p>
<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>device=None</em>, <em>dtype=None</em>, <em>non_blocking=False</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>dtype</em>, <em>non_blocking=False</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<dl class="function">
<dt>
<code class="descname">to</code><span class="sig-paren">(</span><em>tensor</em>, <em>non_blocking=False</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Its signature is similar to <a class="reference internal" href="index.html#torch.Tensor.to" title="torch.Tensor.to"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code></a>, but only accepts
floating point desired <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> s. In addition, this method will
only cast the floating point parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>
(if given). The integral parameters and buffers will be moved
<code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.</p>
<p>See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>) – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>torch.Tensor</em></a>) – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1913, -0.3420],</span>
<span class="go">        [-0.5113, -0.2325]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gpu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">gpu1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16, device=&#39;cuda:1&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 0.1914, -0.3420],</span>
<span class="go">        [-0.5112, -0.2324]], dtype=torch.float16)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>mode=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <a class="reference internal" href="#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>dst_type</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a><em> or </em><em>string</em>) – the desired type</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.zero_grad">
<code class="descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sequential">
<h4><span class="hidden-section">Sequential</span><a class="headerlink" href="#sequential" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Sequential">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sequential</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, here is a small example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example of using Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

<span class="c1"># Example of using Sequential with OrderedDict</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
          <span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
          <span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="p">]))</span>
</pre></div>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((*)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="modulelist">
<h4><span class="hidden-section">ModuleList</span><a class="headerlink" href="#modulelist" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ModuleList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ModuleList</code><span class="sig-paren">(</span><em>modules=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a list.</p>
<p><a class="reference internal" href="#torch.nn.ModuleList" title="torch.nn.ModuleList"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleList</span></code></a> can be indexed like a regular Python list, but
modules it contains are properly registered, and will be visible by all
<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modules</strong> (<em>iterable</em><em>, </em><em>optional</em>) – an iterable of modules to add</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ModuleList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ModuleList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given module to the end of the list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module to append</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>modules</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends modules from a Python iterable to the end of the list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modules</strong> (<em>iterable</em>) – iterable of modules to append</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleList.insert">
<code class="descname">insert</code><span class="sig-paren">(</span><em>index</em>, <em>module</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleList.insert" title="Permalink to this definition">¶</a></dt>
<dd><p>Insert a given module before a given index in the list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – index to insert.</p></li>
<li><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – module to insert</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="moduledict">
<h4><span class="hidden-section">ModuleDict</span><a class="headerlink" href="#moduledict" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ModuleDict">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ModuleDict</code><span class="sig-paren">(</span><em>modules=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a dictionary.</p>
<p><a class="reference internal" href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDict</span></code></a> can be indexed like a regular Python dictionary,
but modules it contains are properly registered, and will be visible by all
<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> methods.</p>
<p><a class="reference internal" href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDict</span></code></a> is an <strong>ordered</strong> dictionary that respects</p>
<ul class="simple">
<li><p>the order of insertion, and</p></li>
<li><p>in <a class="reference internal" href="#torch.nn.ModuleDict.update" title="torch.nn.ModuleDict.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a>, the order of the merged <code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code>
or another <a class="reference internal" href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDict</span></code></a> (the argument to <a class="reference internal" href="#torch.nn.ModuleDict.update" title="torch.nn.ModuleDict.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a>).</p></li>
</ul>
<p>Note that <a class="reference internal" href="#torch.nn.ModuleDict.update" title="torch.nn.ModuleDict.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a> with other unordered mapping
types (e.g., Python’s plain <code class="docutils literal notranslate"><span class="pre">dict</span></code>) does not preserve the order of the
merged mapping.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modules</strong> (<em>iterable</em><em>, </em><em>optional</em>) – a mapping (dictionary) of (string: module)
or an iterable of key-value pairs of type (string, module)</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">choices</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span>
                <span class="s1">&#39;conv&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
                <span class="s1">&#39;pool&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activations</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">([</span>
                <span class="p">[</span><span class="s1">&#39;lrelu&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">()],</span>
                <span class="p">[</span><span class="s1">&#39;prelu&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()]</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">choice</span><span class="p">,</span> <span class="n">act</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="n">choice</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activations</span><span class="p">[</span><span class="n">act</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ModuleDict.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleDict.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all items from the ModuleDict.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.items">
<code class="descname">items</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleDict.items" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict key/value pairs.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.keys">
<code class="descname">keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleDict.keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict keys.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.pop">
<code class="descname">pop</code><span class="sig-paren">(</span><em>key</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleDict.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove key from the ModuleDict and return its module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> (<em>string</em>) – key to pop from the ModuleDict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>modules</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleDict.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the <a class="reference internal" href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDict</span></code></a> with the key-value pairs from a
mapping or an iterable, overwriting existing keys.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">modules</span></code> is an <code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code>, a <a class="reference internal" href="#torch.nn.ModuleDict" title="torch.nn.ModuleDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModuleDict</span></code></a>, or
an iterable of key-value pairs, the order of new elements in it is preserved.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modules</strong> (<em>iterable</em>) – a mapping (dictionary) from string to <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a>,
or an iterable of key-value pairs of type (string, <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleDict.values">
<code class="descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleDict.values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ModuleDict values.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="parameterlist">
<h4><span class="hidden-section">ParameterList</span><a class="headerlink" href="#parameterlist" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ParameterList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ParameterList</code><span class="sig-paren">(</span><em>parameters=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds parameters in a list.</p>
<p><a class="reference internal" href="#torch.nn.ParameterList" title="torch.nn.ParameterList"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParameterList</span></code></a> can be indexed like a regular Python
list, but parameters it contains are properly registered, and will be
visible by all <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>parameters</strong> (<em>iterable</em><em>, </em><em>optional</em>) – an iterable of <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a> to add</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ParameterList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ParameterList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>parameter</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given parameter at the end of the list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>parameter</strong> (<a class="reference internal" href="index.html#torch.nn.Parameter" title="torch.nn.Parameter"><em>nn.Parameter</em></a>) – parameter to append</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends parameters from a Python iterable to the end of the list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>parameters</strong> (<em>iterable</em>) – iterable of parameters to append</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="parameterdict">
<h4><span class="hidden-section">ParameterDict</span><a class="headerlink" href="#parameterdict" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ParameterDict">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ParameterDict</code><span class="sig-paren">(</span><em>parameters=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds parameters in a dictionary.</p>
<p>ParameterDict can be indexed like a regular Python dictionary, but parameters it
contains are properly registered, and will be visible by all Module methods.</p>
<p><a class="reference internal" href="#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParameterDict</span></code></a> is an <strong>ordered</strong> dictionary that respects</p>
<ul class="simple">
<li><p>the order of insertion, and</p></li>
<li><p>in <a class="reference internal" href="#torch.nn.ParameterDict.update" title="torch.nn.ParameterDict.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a>, the order of the merged <code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code>
or another <a class="reference internal" href="#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParameterDict</span></code></a> (the argument to
<a class="reference internal" href="#torch.nn.ParameterDict.update" title="torch.nn.ParameterDict.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a>).</p></li>
</ul>
<p>Note that <a class="reference internal" href="#torch.nn.ParameterDict.update" title="torch.nn.ParameterDict.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a> with other unordered mapping
types (e.g., Python’s plain <code class="docutils literal notranslate"><span class="pre">dict</span></code>) does not preserve the order of the
merged mapping.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>parameters</strong> (<em>iterable</em><em>, </em><em>optional</em>) – a mapping (dictionary) of
(string : <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>) or an iterable of key-value pairs
of type (string, <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>)</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterDict</span><span class="p">({</span>
                <span class="s1">&#39;left&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
                <span class="s1">&#39;right&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        <span class="p">})</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">choice</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">choice</span><span class="p">]</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ParameterDict.clear">
<code class="descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterDict.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all items from the ParameterDict.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.items">
<code class="descname">items</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterDict.items" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict key/value pairs.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.keys">
<code class="descname">keys</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterDict.keys" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict keys.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.pop">
<code class="descname">pop</code><span class="sig-paren">(</span><em>key</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterDict.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove key from the ParameterDict and return its parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> (<em>string</em>) – key to pop from the ParameterDict</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterDict.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the <a class="reference internal" href="#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParameterDict</span></code></a> with the key-value pairs from a
mapping or an iterable, overwriting existing keys.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">parameters</span></code> is an <code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code>, a <a class="reference internal" href="#torch.nn.ParameterDict" title="torch.nn.ParameterDict"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParameterDict</span></code></a>, or
an iterable of key-value pairs, the order of new elements in it is preserved.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>parameters</strong> (<em>iterable</em>) – a mapping (dictionary) from string to
<a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>, or an iterable of
key-value pairs of type (string, <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterDict.values">
<code class="descname">values</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterDict.values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterable of the ParameterDict values.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="convolution-layers">
<h3>Convolution layers<a class="headerlink" href="#convolution-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="conv1d">
<h4><span class="hidden-section">Conv1d</span><a class="headerlink" href="#conv1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Conv1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math">\((N, C_{\text{in}}, L)\)</span> and output <span class="math">\((N, C_{\text{out}}, L_{\text{out}})\)</span> can be
precisely described as:</p>
<div class="math">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
\star \text{input}(N_i, k)

\]</div>
<p>where <span class="math">\(\star\)</span> is the valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math">\(N\)</span> is a batch size, <span class="math">\(C\)</span> denotes a number of channels,
<span class="math">\(L\)</span> is a length of signal sequence.</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a one-element tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters,
of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid
<a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>, and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also termed in
literature as depthwise convolution.</p>
<p>In other words, for an input of size <span class="math">\((N, C_{in}, L_{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite>, can be constructed by arguments
<span class="math">\((C_\text{in}=C_{in}, C_\text{out}=C_{in} \times K, ..., \text{groups}=C_{in})\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to both sides of
the input. Default: 0</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <cite>zeros</cite></p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel
elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C_{in}, L_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C_{out}, L_{out})\)</span> where</p>
<div class="math">
\[L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
          \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~Conv1d.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape
<span class="math">\((\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})\)</span>.
The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \text{kernel\_size}}\)</span></p></li>
<li><p><strong>~Conv1d.bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape
(out_channels). If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \text{kernel\_size}}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv2d">
<h4><span class="hidden-section">Conv2d</span><a class="headerlink" href="#conv2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Conv2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math">\((N, C_{\text{in}}, H, W)\)</span> and output <span class="math">\((N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)</span>
can be precisely described as:</p>
<div class="math">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)

\]</div>
<p>where <span class="math">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math">\(N\)</span> is a batch size, <span class="math">\(C\)</span> denotes a number of channels,
<span class="math">\(H\)</span> is a height of input planes in pixels, and <span class="math">\(W\)</span> is
width in pixels.</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters, of size:
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also termed in
literature as depthwise convolution.</p>
<p>In other words, for an input of size <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite>, can be constructed by arguments
<span class="math">\((in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to both sides of the input. Default: 0</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <cite>zeros</cite></p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
          \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
          \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~Conv2d.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <p>the learnable weights of the module of shape
:math:<a href="#id1"><span class="problematic" id="id2">`</span></a>(text{out_channels}, frac{text{in_channels}}{text{groups}},</p>
<blockquote>
<div><p>text{kernel_size[0]}, text{kernel_size[1]})`.</p>
</div></blockquote>
<p>The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}\)</span></p>
</p></li>
<li><p><strong>~Conv2d.bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels). If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv3d">
<h4><span class="hidden-section">Conv3d</span><a class="headerlink" href="#conv3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Conv3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C_{in}, D, H, W)\)</span>
and output <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[out(N_i, C_{out_j}) = bias(C_{out_j}) +
                        \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)

\]</div>
<p>where <span class="math">\(\star\)</span> is the valid 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters, of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also termed in
literature as depthwise convolution.</p>
<blockquote>
<div><p>In other words, for an input of size <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite>, can be constructed by arguments
<span class="math">\((in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})\)</span>.</p>
</div></blockquote>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Zero-padding added to all three sides of the input. Default: 0</p></li>
<li><p><strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <cite>zeros</cite></p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where</p>
<div class="math">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
      \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
      \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
      \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~Conv3d.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <p>the learnable weights of the module of shape
:math:<a href="#id5"><span class="problematic" id="id6">`</span></a>(text{out_channels}, frac{text{in_channels}}{text{groups}},</p>
<blockquote>
<div><p>text{kernel_size[0]}, text{kernel_size[1]}, text{kernel_size[2]})`.</p>
</div></blockquote>
<p>The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></p>
</p></li>
<li><p><strong>~Conv3d.bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels). If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose1d">
<h4><span class="hidden-section">ConvTranspose1d</span><a class="headerlink" href="#convtranspose1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConvTranspose1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv1d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C_{in}, L_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C_{out}, L_{out})\)</span> where</p>
<div class="math">
\[L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
          \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1

\]</div>
</li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~ConvTranspose1d.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <p>the learnable weights of the module of shape
:math:<a href="#id9"><span class="problematic" id="id10">`</span></a>(text{in_channels}, frac{text{out_channels}}{text{groups}},</p>
<blockquote>
<div><p>text{kernel_size})`. The values of these weights are sampled from</p>
</div></blockquote>
<p><span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \text{kernel\_size}}\)</span></p>
</p></li>
<li><p><strong>~ConvTranspose1d.bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels).
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \text{kernel\_size}}\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="convtranspose2d">
<h4><span class="hidden-section">ConvTranspose2d</span><a class="headerlink" href="#convtranspose2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConvTranspose2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimensions</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of each dimension in the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> where</p></li>
</ul>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
          \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1

\]</div>
<div class="math">
\[W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
          \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1

\]</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~ConvTranspose2d.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <p>the learnable weights of the module of shape
:math:<a href="#id11"><span class="problematic" id="id12">`</span></a>(text{in_channels}, frac{text{out_channels}}{text{groups}},</p>
<blockquote>
<div><p>text{kernel_size[0]}, text{kernel_size[1]})`.</p>
</div></blockquote>
<p>The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}\)</span></p>
</p></li>
<li><p><strong>~ConvTranspose2d.bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># exact output size can be also specified as an argument</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 6, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose3d">
<h4><span class="hidden-section">ConvTranspose3d</span><a class="headerlink" href="#convtranspose3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConvTranspose3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image composed of several input
planes.
The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.</p>
<p>This module can be seen as the gradient of Conv3d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math">\(\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimensions</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> and a <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the convolving kernel</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of each dimension in the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Additional size added to one side
of each dimension in the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where</p></li>
</ul>
<div class="math">
\[D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
          \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1

\]</div>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
          \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1

\]</div>
<div class="math">
\[W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]
          \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1

\]</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~ConvTranspose3d.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <p>the learnable weights of the module of shape
:math:<a href="#id15"><span class="problematic" id="id16">`</span></a>(text{in_channels}, frac{text{out_channels}}{text{groups}},</p>
<blockquote>
<div><p>text{kernel_size[0]}, text{kernel_size[1]}, text{kernel_size[2]})`.</p>
</div></blockquote>
<p>The values of these weights are sampled from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></p>
</p></li>
<li><p><strong>~ConvTranspose3d.bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable bias of the module of shape (out_channels)
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="unfold">
<h4><span class="hidden-section">Unfold</span><a class="headerlink" href="#unfold" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Unfold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Unfold</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts sliding local blocks from a batched input tensor.</p>
<p>Consider an batched <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor of shape <span class="math">\((N, C, *)\)</span>,
where <span class="math">\(N\)</span> is the batch dimension, <span class="math">\(C\)</span> is the channel dimension,
and <span class="math">\(*\)</span> represent arbitrary spatial dimensions. This operation flattens
each sliding <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>-sized block within the spatial dimensions
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> into a column (i.e., last dimension) of a 3-D <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code>
tensor of shape <span class="math">\((N, C \times \prod(\text{kernel\_size}), L)\)</span>, where
<span class="math">\(C \times \prod(\text{kernel\_size})\)</span> is the total number of values
with in each block (a block has <span class="math">\(\prod(\text{kernel\_size})\)</span> spatial
locations each containing a <span class="math">\(C\)</span>-channeled vector), and <span class="math">\(L\)</span> is
the total number of such blocks:</p>
<div class="math">
\[L = \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times \text{padding}[d] %
    - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,

\]</div>
<p>where <span class="math">\(\text{spatial\_size}\)</span> is formed by the spatial dimensions
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (<span class="math">\(*\)</span> above), and <span class="math">\(d\)</span> is over all spatial
dimensions.</p>
<p>Therefore, indexing <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code> at the last dimension (column dimension)
gives all values within a certain block.</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> arguments specify
how the sliding blocks are retrieved.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the sliding blocks.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension before
reshaping.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the sliding blocks</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – the stride of the sliding blocks in the input
spatial dimensions. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – implicit zero padding to be added on
both sides of input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a parameter that controls the
stride of elements within the
neighborhood. Default: 1</p></li>
</ul>
</dd>
</dl>
<ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> or
<code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> is an int or a tuple of length 1, their values will be
replicated across all spatial dimensions.</p></li>
<li><p>For the case of two input spatial dimensions this operation is sometimes
called <code class="docutils literal notranslate"><span class="pre">im2col</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Fold</span></code></a> calculates each combined value in the resulting
large tensor by summing all values from all containing blocks.
<a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a> extracts the values in the local blocks by
copying from the large tensor. So, if the blocks overlap, they are not
inverses of each other.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently, only 4-D input tensors (batched image-like tensors) are
supported.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, *)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C \times \prod(\text{kernel\_size}), L)\)</span> as described above</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unfold</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Unfold</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">unfold</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each patch contains 30 values (2x3=6 vectors, each of 5 channels)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 4 blocks (2x3 kernels) in total in the 3x4 input</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 30, 4])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp_unf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_unf</span> <span class="o">=</span> <span class="n">inp_unf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">())</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">fold</span><span class="p">(</span><span class="n">out_unf</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or equivalently (and avoiding a copy),</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># out = out_unf.view(1, 2, 7, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">out</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="go">tensor(1.9073e-06)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fold">
<h4><span class="hidden-section">Fold</span><a class="headerlink" href="#fold" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Fold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Fold</code><span class="sig-paren">(</span><em>output_size</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines an array of sliding local blocks into a large containing
tensor.</p>
<p>Consider a batched <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor containing sliding local blocks,
e.g., patches of images, of shape <span class="math">\((N, C \times  \prod(\text{kernel\_size}), L)\)</span>,
where <span class="math">\(N\)</span> is batch dimension, <span class="math">\(C \times \prod(\text{kernel\_size})\)</span>
is the number of values with in a block (a block has <span class="math">\(\prod(\text{kernel\_size})\)</span>
spatial locations each containing a <span class="math">\(C\)</span>-channeled vector), and
<span class="math">\(L\)</span> is the total number of blocks. (This is exacly the
same specification as the output shape of <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a>.) This
operation combines these local blocks into the large <code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code> tensor
of shape <span class="math">\((N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)\)</span>
by summing the overlapping values. Similar to <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a>, the
arguments must satisfy</p>
<div class="math">
\[L = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] %
    - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,

\]</div>
<p>where <span class="math">\(d\)</span> is over all spatial dimensions.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> describes the spatial shape of the large containing
tensor of the sliding local blocks. It is useful to resolve the ambiguity
when multiple input shapes map to same number of sliding blocks, e.g.,
with <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>.</p></li>
</ul>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> arguments specify
how the sliding blocks are retrieved.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the sliding blocks.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero-paddings on both
sides for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points for each dimension before
reshaping.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the shape of the spatial dimensions of the
output (i.e., <code class="docutils literal notranslate"><span class="pre">output.sizes()[2:]</span></code>)</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the sliding blocks</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the stride of the sliding blocks in the input
spatial dimensions. Default: 1</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – implicit zero padding to be added on
both sides of input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a><em>, </em><em>optional</em>) – a parameter that controls the
stride of elements within the
neighborhood. Default: 1</p></li>
</ul>
</dd>
</dl>
<ul class="simple">
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code>,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> or <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> is an int or a tuple of length 1 then
their values will be replicated across all spatial dimensions.</p></li>
<li><p>For the case of two output spatial dimensions this operation is sometimes
called <code class="docutils literal notranslate"><span class="pre">col2im</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Fold</span></code></a> calculates each combined value in the resulting
large tensor by summing all values from all containing blocks.
<a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Unfold</span></code></a> extracts the values in the local blocks by
copying from the large tensor. So, if the blocks overlap, they are not
inverses of each other.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently, only 4-D output tensors (batched image-like tensors) are
supported.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C \times \prod(\text{kernel\_size}), L)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)\)</span> as described above</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fold</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Fold</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">fold</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 3, 4, 5])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-layers">
<h3>Pooling layers<a class="headerlink" href="#pooling-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="maxpool1d">
<h4><span class="hidden-section">MaxPool1d</span><a class="headerlink" href="#maxpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, L)\)</span>
and output <span class="math">\((N, C, L_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[out(N_i, C_j, k) = \max_{m=0, \ldots, \text{kernel\_size} - 1}
        input(N_i, C_j, stride \times k + m)

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window to take a max over</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero padding to be added on both sides</p></li>
<li><p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool1d</span></code></a> later</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, L_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, L_{out})\)</span>, where</p>
<div class="math">
\[L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation}
      \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool2d">
<h4><span class="hidden-section">MaxPool2d</span><a class="headerlink" href="#maxpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, H, W)\)</span>,
output <span class="math">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{aligned}
    out(N_i, C_j, h, w) ={} & \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                            & \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
                                           \text{stride[1]} \times w + n)
\end{aligned}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window to take a max over</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero padding to be added on both sides</p></li>
<li><p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool2d</span></code></a> later</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}
      \times (\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}
      \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool3d">
<h4><span class="hidden-section">MaxPool3d</span><a class="headerlink" href="#maxpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, D, H, W)\)</span>,
output <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{aligned}
    \text{out}(N_i, C_j, d, h, w) ={} & \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                      & \text{input}(N_i, C_j, \text{stride[0]} \times d + k,
                                                     \text{stride[1]} \times h + m, \text{stride[2]} \times w + n)
\end{aligned}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points. <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points.
It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window to take a max over</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero padding to be added on all three sides</p></li>
<li><p><strong>dilation</strong> – a parameter that controls the stride of elements in the window</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the max indices along with the outputs.
Useful for <a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.MaxUnpool3d</span></code></a> later</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times
  (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times
  (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times
  (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool1d">
<h4><span class="hidden-section">MaxUnpool1d</span><a class="headerlink" href="#maxunpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxUnpool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxUnpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool1d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the forward call.
See the Inputs and Example below.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><cite>input</cite>: the input Tensor to invert</p></li>
<li><p><cite>indices</cite>: the indices given out by <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a></p></li>
<li><p><cite>output_size</cite> (optional): the targeted output size</p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]

\]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example showcasing the use of output_size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool2d">
<h4><span class="hidden-section">MaxUnpool2d</span><a class="headerlink" href="#maxunpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxUnpool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxUnpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool2d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the forward call.
See the Inputs and Example below.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><cite>input</cite>: the input Tensor to invert</p></li>
<li><p><cite>indices</cite>: the indices given out by <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a></p></li>
<li><p><cite>output_size</cite> (optional): the targeted output size</p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}

\]</div>
<div class="math">
\[W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}

\]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
<span class="go">                            [ 5,  6,  7,  8],</span>
<span class="go">                            [ 9, 10, 11, 12],</span>
<span class="go">                            [13, 14, 15, 16]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">tensor([[[[  0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,   6.,   0.,   8.],</span>
<span class="go">          [  0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,  14.,   0.,  16.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># specify a different output size than input size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="go">tensor([[[[  0.,   0.,   0.,   0.,   0.],</span>
<span class="go">          [  6.,   0.,   8.,   0.,   0.],</span>
<span class="go">          [  0.,   0.,   0.,  14.,   0.],</span>
<span class="go">          [ 16.,   0.,   0.,   0.,   0.],</span>
<span class="go">          [  0.,   0.,   0.,   0.,   0.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool3d">
<h4><span class="hidden-section">MaxUnpool3d</span><a class="headerlink" href="#maxunpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxUnpool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxUnpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> is not fully invertible, since the non-maximal values are lost.
<a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool3d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> can map several input sizes to the same output
sizes. Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the forward call.
See the Inputs section below.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Size of the max pooling window.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Stride of the max pooling window.
It is set to <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> by default.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Padding that was added to the input</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><cite>input</cite>: the input Tensor to invert</p></li>
<li><p><cite>indices</cite>: the indices given out by <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a></p></li>
<li><p><cite>output_size</cite> (optional): the targeted output size</p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[D_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}

\]</div>
<div class="math">
\[H_{out} = (H_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}

\]</div>
<div class="math">
\[W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}

\]</div>
<p>or as given by <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_size</span></code> in the call operator</p>
</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([20, 16, 51, 33, 15])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool1d">
<h4><span class="hidden-section">AvgPool1d</span><a class="headerlink" href="#avgpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, L)\)</span>,
output <span class="math">\((N, C, L_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\(k\)</span>
can be precisely described as:</p>
<div class="math">
\[\text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1}
                       \text{input}(N_i, C_j, \text{stride} \times l + m)\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> can each be
an <code class="docutils literal notranslate"><span class="pre">int</span></code> or a one-element tuple.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero padding to be added on both sides</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, L_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, L_{out})\)</span>, where</p>
<div class="math">
\[L_{out} = \left\lfloor \frac{L_{in} +
2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool with window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]]))</span>
<span class="go">tensor([[[ 2.,  4.,  6.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool2d">
<h4><span class="hidden-section">AvgPool2d</span><a class="headerlink" href="#avgpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, H, W)\)</span>,
output <span class="math">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero padding to be added on both sides</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] -
  \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] -
  \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool3d">
<h4><span class="hidden-section">AvgPool3d</span><a class="headerlink" href="#avgpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, D, H, W)\)</span>,
output <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{aligned}
    \text{out}(N_i, C_j, d, h, w) ={} & \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\
                                      & \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k,
                                              \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)}
                                             {kD \times kH \times kW}
\end{aligned}

\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on all three sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero padding to be added on all three sides</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] -
      \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] -
      \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] -
      \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fractionalmaxpool2d">
<h4><span class="hidden-section">FractionalMaxPool2d</span><a class="headerlink" href="#fractionalmaxpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.FractionalMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">FractionalMaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>output_size=None</em>, <em>output_ratio=None</em>, <em>return_indices=False</em>, <em>_random_samples=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.FractionalMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D fractional max pooling over an input signal composed of several input planes.</p>
<p>Fractional MaxPooling is described in detail in the paper <a class="reference external" href="http://arxiv.org/abs/1412.6071">Fractional MaxPooling</a> by Ben Graham</p>
<p>The max-pooling operation is applied in <span class="math">\(kH \times kW\)</span> regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window to take a max over.
Can be a single number k (for a square kernel of k x k) or a tuple <cite>(kh, kw)</cite></p></li>
<li><p><strong>output_size</strong> – the target output size of the image of the form <cite>oH x oW</cite>.
Can be a tuple <cite>(oH, oW)</cite> or a single number oH for a square image <cite>oH x oH</cite></p></li>
<li><p><strong>output_ratio</strong> – If one wants to have an output size as a ratio of the input size, this option can be given.
This has to be a number or tuple in the range (0, 1)</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to <code class="xref py py-meth docutils literal notranslate"><span class="pre">nn.MaxUnpool2d()</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, and target output size 13x12</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window and target output size being half of input image size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lppool1d">
<h4><span class="hidden-section">LPPool1d</span><a class="headerlink" href="#lppool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LPPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LPPool1d</code><span class="sig-paren">(</span><em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LPPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D power-average pooling over an input signal composed of several input
planes.</p>
<p>On each window, the function computed is:</p>
<div class="math">
\[f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}

\]</div>
<ul class="simple">
<li><p>At p = <span class="math">\(\infty\)</span>, one gets Max Pooling</p></li>
<li><p>At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the sum to the power of <cite>p</cite> is zero, the gradient of this function is
not defined. This implementation will set the gradient to zero in this case.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – a single int, the size of the window</p></li>
<li><p><strong>stride</strong> – a single int, the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, L_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, L_{out})\)</span>, where</p>
<div class="math">
\[L_{out} = \left\lfloor\frac{L_{in} +
2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
<dt>Examples::</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># power-2 pool of window of length 3, with stride 2.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="lppool2d">
<h4><span class="hidden-section">LPPool2d</span><a class="headerlink" href="#lppool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LPPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LPPool2d</code><span class="sig-paren">(</span><em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LPPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of several input
planes.</p>
<p>On each window, the function computed is:</p>
<div class="math">
\[f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}

\]</div>
<ul class="simple">
<li><p>At p = <span class="math">\(\infty\)</span>, one gets Max Pooling</p></li>
<li><p>At p = 1, one gets Sum Pooling (which is proportional to average pooling)</p></li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the sum to the power of <cite>p</cite> is zero, the gradient of this function is
not defined. This implementation will set the gradient to zero in this case.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> – the size of the window</p></li>
<li><p><strong>stride</strong> – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span>, where</p>
<div class="math">
\[H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0] \times
      (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1] \times
      (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

\]</div>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># power-2 pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window of power 1.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool1d">
<h4><span class="hidden-section">AdaptiveMaxPool1d</span><a class="headerlink" href="#adaptivemaxpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool1d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveMaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size H</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool1d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool2d">
<h4><span class="hidden-section">AdaptiveMaxPool2d</span><a class="headerlink" href="#adaptivemaxpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool2d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H.
H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool2d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 10x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool3d">
<h4><span class="hidden-section">AdaptiveMaxPool3d</span><a class="headerlink" href="#adaptivemaxpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool3d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveMaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size of the image of the form D x H x W.
Can be a tuple (D, H, W) or a single D for a cube D x D x D.
D, H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</p></li>
<li><p><strong>return_indices</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool3d. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7x9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7x7 (cube)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x9x8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool1d">
<h4><span class="hidden-section">AdaptiveAvgPool1d</span><a class="headerlink" href="#adaptiveavgpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool1d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveAvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size H</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool2d">
<h4><span class="hidden-section">AdaptiveAvgPool2d</span><a class="headerlink" href="#adaptiveavgpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool2d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single H for a square image H x H.
H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 10x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool3d">
<h4><span class="hidden-section">AdaptiveAvgPool3d</span><a class="headerlink" href="#adaptiveavgpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool3d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveAvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size D x H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size of the form D x H x W.
Can be a tuple (D, H, W) or a single number D for a cube D x D x D.
D, H and W can be either a <code class="docutils literal notranslate"><span class="pre">int</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> which means the size will
be the same as that of the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7x9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7x7 (cube)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool3d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x9x8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool3d</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="padding-layers">
<h3>Padding layers<a class="headerlink" href="#padding-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="reflectionpad1d">
<h4><span class="hidden-section">ReflectionPad1d</span><a class="headerlink" href="#reflectionpad1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReflectionPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReflectionPad1d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReflectionPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using the reflection of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 2-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, W_{out})\)</span> where</p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[0., 1., 2., 3.],</span>
<span class="go">         [4., 5., 6., 7.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],</span>
<span class="go">         [6., 5., 4., 5., 6., 7., 6., 5.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],</span>
<span class="go">         [7., 6., 5., 4., 5., 6., 7., 6.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="reflectionpad2d">
<h4><span class="hidden-section">ReflectionPad2d</span><a class="headerlink" href="#reflectionpad2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReflectionPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReflectionPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReflectionPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using the reflection of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math">\(\text{padding\_left}\)</span>,
<span class="math">\(\text{padding\_right}\)</span>, <span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</p>
<p><span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span></p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[0., 1., 2.],</span>
<span class="go">          [3., 4., 5.],</span>
<span class="go">          [6., 7., 8.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[8., 7., 6., 7., 8., 7., 6.],</span>
<span class="go">          [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="go">          [2., 1., 0., 1., 2., 1., 0.],</span>
<span class="go">          [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="go">          [8., 7., 6., 7., 8., 7., 6.],</span>
<span class="go">          [5., 4., 3., 4., 5., 4., 3.],</span>
<span class="go">          [2., 1., 0., 1., 2., 1., 0.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReflectionPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[7., 6., 7., 8., 7.],</span>
<span class="go">          [4., 3., 4., 5., 4.],</span>
<span class="go">          [1., 0., 1., 2., 1.],</span>
<span class="go">          [4., 3., 4., 5., 4.],</span>
<span class="go">          [7., 6., 7., 8., 7.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad1d">
<h4><span class="hidden-section">ReplicationPad1d</span><a class="headerlink" href="#replicationpad1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReplicationPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad1d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReplicationPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 2-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, W_{out})\)</span> where</p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[0., 1., 2., 3.],</span>
<span class="go">         [4., 5., 6., 7.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],</span>
<span class="go">         [4., 4., 4., 5., 6., 7., 7., 7.]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],</span>
<span class="go">         [4., 4., 4., 4., 5., 6., 7., 7.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad2d">
<h4><span class="hidden-section">ReplicationPad2d</span><a class="headerlink" href="#replicationpad2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReplicationPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReplicationPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math">\(\text{padding\_left}\)</span>,
<span class="math">\(\text{padding\_right}\)</span>, <span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</p>
<p><span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span></p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[0., 1., 2.],</span>
<span class="go">          [3., 4., 5.],</span>
<span class="go">          [6., 7., 8.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[0., 0., 0., 1., 2., 2., 2.],</span>
<span class="go">          [0., 0., 0., 1., 2., 2., 2.],</span>
<span class="go">          [0., 0., 0., 1., 2., 2., 2.],</span>
<span class="go">          [3., 3., 3., 4., 5., 5., 5.],</span>
<span class="go">          [6., 6., 6., 7., 8., 8., 8.],</span>
<span class="go">          [6., 6., 6., 7., 8., 8., 8.],</span>
<span class="go">          [6., 6., 6., 7., 8., 8., 8.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[0., 0., 1., 2., 2.],</span>
<span class="go">          [0., 0., 1., 2., 2.],</span>
<span class="go">          [0., 0., 1., 2., 2.],</span>
<span class="go">          [3., 3., 4., 5., 5.],</span>
<span class="go">          [6., 6., 7., 8., 8.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="replicationpad3d">
<h4><span class="hidden-section">ReplicationPad3d</span><a class="headerlink" href="#replicationpad3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReplicationPad3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReplicationPad3d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReplicationPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor using replication of the input boundary.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 6-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>,
<span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>,
<span class="math">\(\text{padding\_front}\)</span>, <span class="math">\(\text{padding\_back}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where</p>
<p><span class="math">\(D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}\)</span></p>
<p><span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span></p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">480</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReplicationPad3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="zeropad2d">
<h4><span class="hidden-section">ZeroPad2d</span><a class="headerlink" href="#zeropad2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ZeroPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ZeroPad2d</code><span class="sig-paren">(</span><em>padding</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ZeroPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with zero.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math">\(\text{padding\_left}\)</span>,
<span class="math">\(\text{padding\_right}\)</span>, <span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</p>
<p><span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span></p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[-0.1678, -0.4418,  1.9466],</span>
<span class="go">          [ 0.9604, -0.4219, -0.5241],</span>
<span class="go">          [-0.9162, -0.5436, -0.6446]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ZeroPad2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],</span>
<span class="go">          [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],</span>
<span class="go">          [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],</span>
<span class="go">          [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad1d">
<h4><span class="hidden-section">ConstantPad1d</span><a class="headerlink" href="#constantpad1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConstantPad1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad1d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConstantPad1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in both boundaries. If a 2-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, W_{out})\)</span> where</p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],</span>
<span class="go">         [-1.3287,  1.8966,  0.1466, -0.2771]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,</span>
<span class="go">           3.5000],</span>
<span class="go">         [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,</span>
<span class="go">           3.5000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[ 1.6616,  1.4523, -1.1255],</span>
<span class="go">         [-3.6372,  0.1182, -1.8652]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad1d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad2d">
<h4><span class="hidden-section">ConstantPad2d</span><a class="headerlink" href="#constantpad2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConstantPad2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad2d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConstantPad2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 4-<cite>tuple</cite>, uses (<span class="math">\(\text{padding\_left}\)</span>,
<span class="math">\(\text{padding\_right}\)</span>, <span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</p>
<p><span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span></p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[ 1.6585,  0.4320],</span>
<span class="go">         [-0.8701, -0.4649]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],</span>
<span class="go">         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="constantpad3d">
<h4><span class="hidden-section">ConstantPad3d</span><a class="headerlink" href="#constantpad3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConstantPad3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConstantPad3d</code><span class="sig-paren">(</span><em>padding</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConstantPad3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads the input tensor boundaries with a constant value.</p>
<p>For <cite>N</cite>-dimensional padding, use <a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.pad()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – the size of the padding. If is <cite>int</cite>, uses the same
padding in all boundaries. If a 6-<cite>tuple</cite>, uses
(<span class="math">\(\text{padding\_left}\)</span>, <span class="math">\(\text{padding\_right}\)</span>,
<span class="math">\(\text{padding\_top}\)</span>, <span class="math">\(\text{padding\_bottom}\)</span>,
<span class="math">\(\text{padding\_front}\)</span>, <span class="math">\(\text{padding\_back}\)</span>)</p>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where</p>
<p><span class="math">\(D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}\)</span></p>
<p><span class="math">\(H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}\)</span></p>
<p><span class="math">\(W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}\)</span></p>
</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># using different paddings for different sides</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConstantPad3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activations-weighted-sum-nonlinearity">
<h3>Non-linear activations (weighted sum, nonlinearity)<a class="headerlink" href="#non-linear-activations-weighted-sum-nonlinearity" title="Permalink to this headline">¶</a></h3>
<div class="section" id="elu">
<h4><span class="hidden-section">ELU</span><a class="headerlink" href="#elu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ELU</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – the <span class="math">\(\alpha\)</span> value for the ELU formulation. Default: 1.0</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/ELU.png" src="scripts/activation_images/ELU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hardshrink">
<h4><span class="hidden-section">Hardshrink</span><a class="headerlink" href="#hardshrink" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Hardshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Hardshrink</code><span class="sig-paren">(</span><em>lambd=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hard shrinkage function element-wise:</p>
<div class="math">
\[\text{HardShrink}(x) =
\begin{cases}
x, & \text{ if } x > \lambda \\
x, & \text{ if } x < -\lambda \\
0, & \text{ otherwise }
\end{cases}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>lambd</strong> – the <span class="math">\(\lambda\)</span> value for the Hardshrink formulation. Default: 0.5</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/Hardshrink.png" src="scripts/activation_images/Hardshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hardtanh">
<h4><span class="hidden-section">Hardtanh</span><a class="headerlink" href="#hardtanh" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Hardtanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Hardtanh</code><span class="sig-paren">(</span><em>min_val=-1.0</em>, <em>max_val=1.0</em>, <em>inplace=False</em>, <em>min_value=None</em>, <em>max_value=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise</p>
<p>HardTanh is defined as:</p>
<div class="math">
\[\text{HardTanh}(x) = \begin{cases}
    1 & \text{ if } x > 1 \\
    -1 & \text{ if } x < -1 \\
    x & \text{ otherwise } \\
\end{cases}

\]</div>
<p>The range of the linear region <span class="math">\([-1, 1]\)</span> can be adjusted using
<code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>min_val</strong> – minimum value of the linear region range. Default: -1</p></li>
<li><p><strong>max_val</strong> – maximum value of the linear region range. Default: 1</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p>Keyword arguments <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_value</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_value</span></code>
have been deprecated in favor of <code class="xref py py-attr docutils literal notranslate"><span class="pre">min_val</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_val</span></code>.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/Hardtanh.png" src="scripts/activation_images/Hardtanh.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardtanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="leakyrelu">
<h4><span class="hidden-section">LeakyReLU</span><a class="headerlink" href="#leakyrelu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LeakyReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LeakyReLU</code><span class="sig-paren">(</span><em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)

\]</div>
<p>or</p>
<div class="math">
\[\text{LeakyRELU}(x) =
\begin{cases}
x, & \text{ if } x \geq 0 \\
\text{negative\_slope} \times x, & \text{ otherwise }
\end{cases}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>negative_slope</strong> – Controls the angle of the negative slope. Default: 1e-2</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/LeakyReLU.png" src="scripts/activation_images/LeakyReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsigmoid">
<h4><span class="hidden-section">LogSigmoid</span><a class="headerlink" href="#logsigmoid" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LogSigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSigmoid</code><a class="headerlink" href="#torch.nn.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)

\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/LogSigmoid.png" src="scripts/activation_images/LogSigmoid.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="prelu">
<h4><span class="hidden-section">PReLU</span><a class="headerlink" href="#prelu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.PReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PReLU</code><span class="sig-paren">(</span><em>num_parameters=1</em>, <em>init=0.25</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{PReLU}(x) = \max(0,x) + a * \min(0,x)

\]</div>
<p>or</p>
<div class="math">
\[\text{PReLU}(x) =
\begin{cases}
x, & \text{ if } x \geq 0 \\
ax, & \text{ otherwise }
\end{cases}

\]</div>
<p>Here <span class="math">\(a\)</span> is a learnable parameter. When called without arguments, <cite>nn.PReLU()</cite> uses a single
parameter <span class="math">\(a\)</span> across all input channels. If called with <cite>nn.PReLU(nChannels)</cite>,
a separate <span class="math">\(a\)</span> is used for each input channel.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>weight decay should not be used when learning <span class="math">\(a\)</span> for good performance.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is
no channel dim and the number of channels = 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_parameters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of <span class="math">\(a\)</span> to learn.
Although it takes an int as input, there is only two values are legitimate:
1, or the number of channels at input. Default: 1</p></li>
<li><p><strong>init</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the initial value of <span class="math">\(a\)</span>. Default: 0.25</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><p><strong>~PReLU.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of shape (<code class="xref py py-attr docutils literal notranslate"><span class="pre">num_parameters</span></code>).</p>
</dd>
</dl>
<img alt="scripts/activation_images/PReLU.png" src="scripts/activation_images/PReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="relu">
<h4><span class="hidden-section">ReLU</span><a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise:</p>
<p><span class="math">\(\text{ReLU}(x)= \max(0, x)\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/ReLU.png" src="scripts/activation_images/ReLU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="o">&gt;&gt;&gt;</span> <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
  <span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
  <span class="o">&gt;&gt;&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="n">An</span> <span class="n">implementation</span> <span class="n">of</span> <span class="n">CReLU</span> <span class="o">-</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">arxiv</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="nb">abs</span><span class="o">/</span><span class="mf">1603.05201</span>

  <span class="o">&gt;&gt;&gt;</span> <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
  <span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="o">&gt;&gt;&gt;</span> <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span><span class="n">m</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="relu6">
<h4><span class="hidden-section">ReLU6</span><a class="headerlink" href="#relu6" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReLU6">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU6</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{ReLU6}(x) = \min(\max(0,x), 6)

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/ReLU6.png" src="scripts/activation_images/ReLU6.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="rrelu">
<h4><span class="hidden-section">RReLU</span><a class="headerlink" href="#rrelu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.RReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RReLU</code><span class="sig-paren">(</span><em>lower=0.125</em>, <em>upper=0.3333333333333333</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.RReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the randomized leaky rectified liner unit function, element-wise,
as described in the paper:</p>
<p><a class="reference external" href="https://arxiv.org/abs/1505.00853">Empirical Evaluation of Rectified Activations in Convolutional Network</a>.</p>
<p>The function is defined as:</p>
<div class="math">
\[\text{RReLU}(x) =
\begin{cases}
    x & \text{if } x \geq 0 \\
    ax & \text{ otherwise }
\end{cases}

\]</div>
<p>where <span class="math">\(a\)</span> is randomly sampled from uniform distribution
<span class="math">\(\mathcal{U}(\text{lower}, \text{upper})\)</span>.</p>
<blockquote>
<div><p>See: <a class="reference external" href="https://arxiv.org/pdf/1505.00853.pdf">https://arxiv.org/pdf/1505.00853.pdf</a></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lower</strong> – lower bound of the uniform distribution. Default: <span class="math">\(\frac{1}{8}\)</span></p></li>
<li><p><strong>upper</strong> – upper bound of the uniform distribution. Default: <span class="math">\(\frac{1}{3}\)</span></p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="selu">
<h4><span class="hidden-section">SELU</span><a class="headerlink" href="#selu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.SELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SELU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.SELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applied element-wise, as:</p>
<div class="math">
\[\text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))

\]</div>
<p>with <span class="math">\(\alpha = 1.6732632423543772848170429916717\)</span> and
<span class="math">\(\text{scale} = 1.0507009873554804934193349852946\)</span>.</p>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/SELU.png" src="scripts/activation_images/SELU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="celu">
<h4><span class="hidden-section">CELU</span><a class="headerlink" href="#celu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.CELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CELU</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.CELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))

\]</div>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a> .</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> – the <span class="math">\(\alpha\)</span> value for the CELU formulation. Default: 1.0</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/CELU.png" src="scripts/activation_images/CELU.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="sigmoid">
<h4><span class="hidden-section">Sigmoid</span><a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Sigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sigmoid</code><a class="headerlink" href="#torch.nn.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}

\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/Sigmoid.png" src="scripts/activation_images/Sigmoid.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softplus">
<h4><span class="hidden-section">Softplus</span><a class="headerlink" href="#softplus" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softplus">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softplus</code><span class="sig-paren">(</span><em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))

\]</div>
<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.</p>
<p>For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>beta</strong> – the <span class="math">\(\beta\)</span> value for the Softplus formulation. Default: 1</p></li>
<li><p><strong>threshold</strong> – values above this revert to a linear function. Default: 20</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/Softplus.png" src="scripts/activation_images/Softplus.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softshrink">
<h4><span class="hidden-section">Softshrink</span><a class="headerlink" href="#softshrink" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softshrink</code><span class="sig-paren">(</span><em>lambd=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise:</p>
<div class="math">
\[\text{SoftShrinkage}(x) =
\begin{cases}
x - \lambda, & \text{ if } x > \lambda \\
x + \lambda, & \text{ if } x < -\lambda \\
0, & \text{ otherwise }
\end{cases}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>lambd</strong> – the <span class="math">\(\lambda\)</span> value for the Softshrink formulation. Default: 0.5</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/Softshrink.png" src="scripts/activation_images/Softshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softsign">
<h4><span class="hidden-section">Softsign</span><a class="headerlink" href="#softsign" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softsign">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softsign</code><a class="headerlink" href="#torch.nn.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{SoftSign}(x) = \frac{x}{ 1 + |x|}

\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/Softsign.png" src="scripts/activation_images/Softsign.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanh">
<h4><span class="hidden-section">Tanh</span><a class="headerlink" href="#tanh" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Tanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanh</code><a class="headerlink" href="#torch.nn.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{Tanh}(x) = \tanh(x) = \frac{e^x - e^{-x}} {e^x + e^{-x}}

\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/Tanh.png" src="scripts/activation_images/Tanh.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanhshrink">
<h4><span class="hidden-section">Tanhshrink</span><a class="headerlink" href="#tanhshrink" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Tanhshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanhshrink</code><a class="headerlink" href="#torch.nn.Tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function:</p>
<div class="math">
\[\text{Tanhshrink}(x) = x - \text{Tanh}(x)

\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<img alt="scripts/activation_images/Tanhshrink.png" src="scripts/activation_images/Tanhshrink.png" />
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="threshold">
<h4><span class="hidden-section">Threshold</span><a class="headerlink" href="#threshold" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Threshold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Threshold</code><span class="sig-paren">(</span><em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor.</p>
<p>Threshold is defined as:</p>
<div class="math">
\[y =
\begin{cases}
x, &\text{ if } x > \text{threshold} \\
\text{value}, &\text{ otherwise }
\end{cases}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>threshold</strong> – The value to threshold at</p></li>
<li><p><strong>value</strong> – The value to replace with</p></li>
<li><p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activations-other">
<h3>Non-linear activations (other)<a class="headerlink" href="#non-linear-activations-other" title="Permalink to this headline">¶</a></h3>
<div class="section" id="softmin">
<h4><span class="hidden-section">Softmin</span><a class="headerlink" href="#softmin" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softmin">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmin</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmin function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range <cite>[0, 1]</cite> and sum to 1.</p>
<p>Softmin is defined as:</p>
<div class="math">
\[\text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}

\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((*)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which Softmin will be computed (so every slice
along dim will sum to 1).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor of the same dimension and shape as the input, with
values in the range [0, 1]</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax">
<h4><span class="hidden-section">Softmax</span><a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmax function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range [0,1] and sum to 1.</p>
<p>Softmax is defined as:</p>
<div class="math">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}

\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((*)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This module doesn’t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use <cite>LogSoftmax</cite> instead (it’s faster and has better numerical properties).</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax2d">
<h4><span class="hidden-section">Softmax2d</span><a class="headerlink" href="#softmax2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softmax2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax2d</code><a class="headerlink" href="#torch.nn.Softmax2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies SoftMax over features to each spatial location.</p>
<p>When given an image of <code class="docutils literal notranslate"><span class="pre">Channels</span> <span class="pre">x</span> <span class="pre">Height</span> <span class="pre">x</span> <span class="pre">Width</span></code>, it will
apply <cite>Softmax</cite> to each location <span class="math">\((Channels, h_i, w_j)\)</span></p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax2d</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># you softmax over the 2nd dimension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsoftmax">
<h4><span class="hidden-section">LogSoftmax</span><a class="headerlink" href="#logsoftmax" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LogSoftmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSoftmax</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LogSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <span class="math">\(\log(\text{Softmax}(x))\)</span> function to an n-dimensional
input Tensor. The LogSoftmax formulation can be simplified as:</p>
<div class="math">
\[\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)

\]</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span> where <cite>*</cite> means, any number of additional
dimensions</p></li>
<li><p>Output: <span class="math">\((*)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which LogSoftmax will be computed.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor of the same dimension and shape as the input with
values in the range [-inf, 0)</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivelogsoftmaxwithloss">
<h4><span class="hidden-section">AdaptiveLogSoftmaxWithLoss</span><a class="headerlink" href="#adaptivelogsoftmaxwithloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveLogSoftmaxWithLoss</code><span class="sig-paren">(</span><em>in_features</em>, <em>n_classes</em>, <em>cutoffs</em>, <em>div_value=4.0</em>, <em>head_bias=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Efficient softmax approximation as described in
<a class="reference external" href="https://arxiv.org/abs/1609.04309">Efficient softmax approximation for GPUs</a> by Edouard Grave, Armand Joulin,
Moustapha Cissé, David Grangier, and Hervé Jégou.</p>
<p>Adaptive softmax is an approximate strategy for training models with large
output spaces. It is most effective when the label distribution is highly
imbalanced, for example in natural language modelling, where the word
frequency distribution approximately follows the <a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law">Zipf’s law</a>.</p>
<p>Adaptive softmax partitions the labels into several clusters, according to
their frequency. These clusters may contain different number of targets
each.
Additionally, clusters containing less frequent labels assign lower
dimensional embeddings to those labels, which speeds up the computation.
For each minibatch, only clusters for which at least one target is
present are evaluated.</p>
<p>The idea is that the clusters which are accessed frequently
(like the first one, containing most frequent labels), should also be cheap
to compute – that is, contain a small number of assigned labels.</p>
<p>We highly recommend taking a look at the original paper for more details.</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">cutoffs</span></code> should be an ordered Sequence of integers sorted
in the increasing order.
It controls number of clusters and the partitioning of targets into
clusters. For example setting <code class="docutils literal notranslate"><span class="pre">cutoffs</span> <span class="pre">=</span> <span class="pre">[10,</span> <span class="pre">100,</span> <span class="pre">1000]</span></code>
means that first <cite>10</cite> targets will be assigned
to the ‘head’ of the adaptive softmax, targets <cite>11, 12, …, 100</cite> will be
assigned to the first cluster, and targets <cite>101, 102, …, 1000</cite> will be
assigned to the second cluster, while targets
<cite>1001, 1002, …, n_classes - 1</cite> will be assigned
to the last, third cluster.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">div_value</span></code> is used to compute the size of each additional cluster,
which is given as
<span class="math">\(\left\lfloor\frac{in\_features}{div\_value^{idx}}\right\rfloor\)</span>,
where <span class="math">\(idx\)</span> is the cluster index (with clusters
for less frequent words having larger indices,
and indices starting from <span class="math">\(1\)</span>).</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">head_bias</span></code> if set to True, adds a bias term to the ‘head’ of the
adaptive softmax. See paper for details. Set to False in the official
implementation.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Labels passed as inputs to this module should be sorted accoridng to
their frequency. This means that the most frequent label should be
represented by the index <cite>0</cite>, and the least frequent
label should be represented by the index <cite>n_classes - 1</cite>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This module returns a <code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">output</span></code>
and <code class="docutils literal notranslate"><span class="pre">loss</span></code> fields. See further documentation for details.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To compute log-probabilities for all classes, the <code class="docutils literal notranslate"><span class="pre">log_prob</span></code>
method can be used.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of features in the input tensor</p></li>
<li><p><strong>n_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of classes in the dataset</p></li>
<li><p><strong>cutoffs</strong> (<em>Sequence</em>) – Cutoffs used to assign targets to their buckets</p></li>
<li><p><strong>div_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – value used as an exponent to compute sizes
of the clusters. Default: 4.0</p></li>
<li><p><strong>head_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a bias term to the ‘head’ of the
adaptive softmax. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>output</strong> is a Tensor of size <code class="docutils literal notranslate"><span class="pre">N</span></code> containing computed target
log probabilities for each example</p></li>
<li><p><strong>loss</strong> is a Scalar representing the computed negative
log likelihood loss</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">output</span></code> and <code class="docutils literal notranslate"><span class="pre">loss</span></code> fields</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>input: <span class="math">\((N, in\_features)\)</span></p></li>
<li><p>target: <span class="math">\((N)\)</span> where each value satisfies <span class="math">\(0 &lt;= target[i] &lt;= n\_classes\)</span></p></li>
<li><p>output1: <span class="math">\((N)\)</span></p></li>
<li><p>output2: <code class="docutils literal notranslate"><span class="pre">Scalar</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes log probabilities for all <span class="math">\(n\_classes\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a minibatch of examples</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log-probabilities of for each class <span class="math">\(c\)</span>
in range <span class="math">\(0 &lt;= c &lt;= n\_classes\)</span>, where <span class="math">\(n\_classes\)</span> is a
parameter passed to <code class="docutils literal notranslate"><span class="pre">AdaptiveLogSoftmaxWithLoss</span></code> constructor.</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, in\_features)\)</span></p></li>
<li><p>Output: <span class="math">\((N, n\_classes)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.nn.AdaptiveLogSoftmaxWithLoss.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveLogSoftmaxWithLoss.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>This is equivalent to <cite>self.log_pob(input).argmax(dim=1)</cite>,
but is more efficient in some cases.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a minibatch of examples</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a class with the highest probability for each example</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, in\_features)\)</span></p></li>
<li><p>Output: <span class="math">\((N)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="normalization-layers">
<h3>Normalization layers<a class="headerlink" href="#normalization-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="batchnorm1d">
<h4><span class="hidden-section">BatchNorm1d</span><a class="headerlink" href="#batchnorm1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BatchNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math">\(\gamma\)</span> are sampled
from <span class="math">\(\mathcal{U}(0, 1)\)</span> and the elements of <span class="math">\(\beta\)</span> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, L)</cite> slices, it’s common terminology to call this Temporal Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, L)\)</span> or <span class="math">\(L\)</span> from input of size <span class="math">\((N, L)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C)\)</span> or <span class="math">\((N, C, L)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C)\)</span> or <span class="math">\((N, C, L)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm2d">
<h4><span class="hidden-section">BatchNorm2d</span><a class="headerlink" href="#batchnorm2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BatchNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math">\(\gamma\)</span> are sampled
from <span class="math">\(\mathcal{U}(0, 1)\)</span> and the elements of <span class="math">\(\beta\)</span> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm3d">
<h4><span class="hidden-section">BatchNorm3d</span><a class="headerlink" href="#batchnorm3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BatchNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em>, <em>track_running_stats=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math">\(\gamma\)</span> are sampled
from <span class="math">\(\mathcal{U}(0, 1)\)</span> and the elements of <span class="math">\(\beta\)</span> are set to 0.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code>
of 0.1.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this layer then does not
keep running estimates, and batch statistics are instead used during
evaluation time as well.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, D, H, W)</cite> slices, it’s common terminology to call this Volumetric Batch Normalization
or Spatio-temporal Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability.
Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var
computation. Can be set to <code class="docutils literal notranslate"><span class="pre">None</span></code> for cumulative moving average
(i.e. simple average). Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="groupnorm">
<h4><span class="hidden-section">GroupNorm</span><a class="headerlink" href="#groupnorm" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.GroupNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GroupNorm</code><span class="sig-paren">(</span><em>num_groups</em>, <em>num_channels</em>, <em>eps=1e-05</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.GroupNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Group Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1803.08494">Group Normalization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

\]</div>
<p>The input channels are separated into <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_groups</span></code> groups, each containing
<code class="docutils literal notranslate"><span class="pre">num_channels</span> <span class="pre">/</span> <span class="pre">num_groups</span></code> channels. The mean and standard-deviation are calculated
separately over the each group. <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable
per-channel affine transform parameter vectorss of size <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_channels</span></code> if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of groups to separate the channels into</p></li>
<li><p><strong>num_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of channels expected in input</p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-channel affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, *)\)</span> where <span class="math">\(C=\text{num\_channels}\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, *)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 3 groups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Separate 6 channels into 6 groups (equivalent with InstanceNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Put all 6 channels into a single group (equivalent with LayerNorm)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activating the module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm1d">
<h4><span class="hidden-section">InstanceNorm1d</span><a class="headerlink" href="#instancenorm1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.InstanceNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.InstanceNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 2D or 3D input (a mini-batch of 1D
inputs with optional additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> and <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> is applied
on each channel of channeled data like multidimensional time series, but
<a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionaly, <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, L)\)</span> or <span class="math">\(L\)</span> from input of size <span class="math">\((N, L)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, L)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, L)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm2d">
<h4><span class="hidden-section">InstanceNorm2d</span><a class="headerlink" href="#instancenorm2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.InstanceNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.InstanceNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> and <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> is applied
on each channel of channeled data like RGB images, but
<a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionaly, <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm3d">
<h4><span class="hidden-section">InstanceNorm3d</span><a class="headerlink" href="#instancenorm3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.InstanceNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em>, <em>track_running_stats=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.InstanceNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs
with additional channel dimension) as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. <span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable parameter vectors
of size C (where C is the input size) if <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>By default, this layer uses instance statistics computed from input data in
both training and evaluation modes.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">track_running_stats</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, during training this
layer keeps running estimates of its computed mean and variance, which are
then used for normalization during evaluation. The running estimates are
kept with a default <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <code class="xref py py-attr docutils literal notranslate"><span class="pre">momentum</span></code> argument is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t\)</span>,
where <span class="math">\(\hat{x}\)</span> is the estimated statistic and <span class="math">\(x_t\)</span> is the
new observed value.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> and <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> are very similar, but
have some subtle differences. <a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> is applied
on each channel of channeled data like 3D models with RGB color, but
<a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> is usually applied on entire sample and often in NLP
tasks. Additionaly, <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> applies elementwise affine
transform, while <a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> usually don’t apply affine
transform.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> – <span class="math">\(C\)</span> from an expected input of size
<span class="math">\((N, C, D, H, W)\)</span></p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>momentum</strong> – the value used for the running_mean and running_var computation. Default: 0.1</p></li>
<li><p><strong>affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module has
learnable affine parameters, initialized the same way as done for batch normalization.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>track_running_stats</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this
module tracks the running mean and variance, and when set to <code class="docutils literal notranslate"><span class="pre">False</span></code>,
this module does not track such statistics and always uses batch
statistics in both training and eval modes. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="layernorm">
<h4><span class="hidden-section">LayerNorm</span><a class="headerlink" href="#layernorm" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LayerNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LayerNorm</code><span class="sig-paren">(</span><em>normalized_shape</em>, <em>eps=1e-05</em>, <em>elementwise_affine=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LayerNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization over a mini-batch of inputs as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a> .</p>
<div class="math">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

\]</div>
<p>The mean and standard-deviation are calculated separately over the last
certain number dimensions which have to be of the shape specified by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code>.
<span class="math">\(\gamma\)</span> and <span class="math">\(\beta\)</span> are learnable affine transform parameters of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike Batch Normalization and Instance Normalization, which applies
scalar scale and bias for each entire channel/plane with the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> option, Layer Normalization applies per-element scale and
bias with <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code>.</p>
</div>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>normalized_shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em> or </em><em>torch.Size</em>) – <p>input shape from an expected input
of size</p>
<div class="math">
\[[* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1]
    \times \ldots \times \text{normalized\_shape}[-1]]

\]</div>
<p>If a single integer is used, it is treated as a singleton list, and this module will
normalize over the last dimension which is expected to be of that specific size.</p>
</p></li>
<li><p><strong>eps</strong> – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>elementwise_affine</strong> – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-element affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span></p></li>
<li><p>Output: <span class="math">\((N, *)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Normalize over last two dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Normalize over last dimension of size 10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activating the module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="localresponsenorm">
<h4><span class="hidden-section">LocalResponseNorm</span><a class="headerlink" href="#localresponsenorm" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LocalResponseNorm">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LocalResponseNorm</code><span class="sig-paren">(</span><em>size</em>, <em>alpha=0.0001</em>, <em>beta=0.75</em>, <em>k=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LocalResponseNorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies local response normalization over an input signal composed
of several input planes, where channels occupy the second dimension.
Applies normalization across channels.</p>
<div class="math">
\[b_{c} = a_{c}\left(k + \frac{\alpha}{n}
\sum_{c'=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c'}^2\right)^{-\beta}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> – amount of neighbouring channels used for normalization</p></li>
<li><p><strong>alpha</strong> – multiplicative factor. Default: 0.0001</p></li>
<li><p><strong>beta</strong> – exponent. Default: 0.75</p></li>
<li><p><strong>k</strong> – additive factor. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, *)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, *)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lrn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LocalResponseNorm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">signal_2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">signal_4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_2d</span> <span class="o">=</span> <span class="n">lrn</span><span class="p">(</span><span class="n">signal_2d</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_4d</span> <span class="o">=</span> <span class="n">lrn</span><span class="p">(</span><span class="n">signal_4d</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="recurrent-layers">
<h3>Recurrent layers<a class="headerlink" href="#recurrent-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="rnn">
<h4><span class="hidden-section">RNN</span><a class="headerlink" href="#rnn" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.RNN">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNN</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.RNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer Elman RNN with <span class="math">\(tanh\)</span> or <span class="math">\(ReLU\)</span> non-linearity to an
input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[h_t = \text{tanh}(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})

\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is
the input at time <cite>t</cite>, and <span class="math">\(h_{(t-1)}\)</span> is the hidden state of the
previous layer at time <cite>t-1</cite> or the initial hidden state at time <cite>0</cite>.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">nonlinearity</span></code> is <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>, then <cite>ReLU</cite> is used instead of <cite>tanh</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></p></li>
<li><p><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two RNNs together to form a <cite>stacked RNN</cite>,
with the second RNN taking in outputs of the first RNN and
computing the final results. Default: 1</p></li>
<li><p><strong>nonlinearity</strong> – The non-linearity to use. Can be either <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> or <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code></p></li>
<li><p><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as <cite>(batch, seq, feature)</cite>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
RNN layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional RNN. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs: input, h_0</dt><dd><ul class="simple">
<li><p><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
or <a class="reference internal" href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_sequence()</span></code></a>
for details.</p></li>
<li><p><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided. If the RNN is bidirectional,
num_directions should be 2, else it should be 1.</p></li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt><dd><ul>
<li><p><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features (<cite>h_t</cite>) from the last layer of the RNN,
for each <cite>t</cite>.  If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has
been given as the input, the output will also be a packed sequence.</p>
<p>For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.
Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p><strong>h_n</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the hidden state for <cite>t = seq_len</cite>.</p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code>.</p>
</li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input1: <span class="math">\((L, N, H_{in})\)</span> tensor containing input features where
<span class="math">\(H_{in}=\text{input\_size}\)</span> and <cite>L</cite> represents a sequence length.</p></li>
<li><p>Input2: <span class="math">\((S, N, H_{out})\)</span> tensor
containing the initial hidden state for each element in the batch.
<span class="math">\(H_{out}=\text{hidden\_size}\)</span>
Defaults to zero if not provided. where <span class="math">\(S=\text{num\_layers} * \text{num\_directions}\)</span>
If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p></li>
<li><p>Output1: <span class="math">\((L, N, H_{all})\)</span> where <span class="math">\(H_all=\text{num\_directions} * \text{hidden\_size}\)</span></p></li>
<li><p>Output2: <span class="math">\((S, N, H_{out})\)</span> tensor containing the next hidden state
for each element in the batch</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~RNN.weight_ih_l[k]</strong> – the learnable input-hidden weights of the k-th layer,
of shape <cite>(hidden_size, input_size)</cite> for <cite>k = 0</cite>. Otherwise, the shape is
<cite>(hidden_size, num_directions * hidden_size)</cite></p></li>
<li><p><strong>~RNN.weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the k-th layer,
of shape <cite>(hidden_size, hidden_size)</cite></p></li>
<li><p><strong>~RNN.bias_ih_l[k]</strong> – the learnable input-hidden bias of the k-th layer,
of shape <cite>(hidden_size)</cite></p></li>
<li><p><strong>~RNN.bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the k-th layer,
of shape <cite>(hidden_size)</cite></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the following conditions are satisfied:
1) cudnn is enabled,
2) input data is on the GPU
3) input data has dtype <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>
4) V100 GPU is used,
5) input data is not in <code class="docutils literal notranslate"><span class="pre">PackedSequence</span></code> format
persistent algorithm can be selected to improve performance.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstm">
<h4><span class="hidden-section">LSTM</span><a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LSTM">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input
sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[\begin{array}{ll} \\
    i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
    f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
    g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\
    o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
    c_t = f_t * c_{(t-1)} + i_t * g_t \\
    h_t = o_t * \tanh(c_t) \\
\end{array}

\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(c_t\)</span> is the cell
state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the input at time <cite>t</cite>, <span class="math">\(h_{(t-1)}\)</span>
is the hidden state of the layer at time <cite>t-1</cite> or the initial hidden
state at time <cite>0</cite>, and <span class="math">\(i_t\)</span>, <span class="math">\(f_t\)</span>, <span class="math">\(g_t\)</span>,
<span class="math">\(o_t\)</span> are the input, forget, cell, and output gates, respectively.
<span class="math">\(\sigma\)</span> is the sigmoid function, and <span class="math">\(*\)</span> is the Hadamard product.</p>
<p>In a multilayer LSTM, the input <span class="math">\(x^{(l)}_t\)</span> of the <span class="math">\(l\)</span> -th layer
(<span class="math">\(l &gt;= 2\)</span>) is the hidden state <span class="math">\(h^{(l-1)}_t\)</span> of the previous layer multiplied by
dropout <span class="math">\(\delta^{(l-1)}_t\)</span> where each <span class="math">\(\delta^{(l-1)}_t\)</span> is a Bernoulli random
variable which is <span class="math">\(0\)</span> with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></p></li>
<li><p><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two LSTMs together to form a <cite>stacked LSTM</cite>,
with the second LSTM taking in outputs of the first LSTM and
computing the final results. Default: 1</p></li>
<li><p><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
LSTM layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional LSTM. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs: input, (h_0, c_0)</dt><dd><ul>
<li><p><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence.
The input can also be a packed variable length sequence.
See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a> or
<a class="reference internal" href="#torch.nn.utils.rnn.pack_sequence" title="torch.nn.utils.rnn.pack_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_sequence()</span></code></a> for details.</p></li>
<li><p><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.
If the LSTM is bidirectional, num_directions should be 2, else it should be 1.</p></li>
<li><p><strong>c_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial cell state for each element in the batch.</p>
<p>If <cite>(h_0, c_0)</cite> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
</dd>
<dt>Outputs: output, (h_n, c_n)</dt><dd><ul>
<li><p><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features <cite>(h_t)</cite> from the last layer of the LSTM,
for each <cite>t</cite>. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been
given as the input, the output will also be a packed sequence.</p>
<p>For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.
Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p><strong>h_n</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the hidden state for <cite>t = seq_len</cite>.</p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code> and similarly for <em>c_n</em>.</p>
</li>
<li><p><strong>c_n</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the cell state for <cite>t = seq_len</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~LSTM.weight_ih_l[k]</strong> – the learnable input-hidden weights of the <span class="math">\(\text{k}^{th}\)</span> layer
<cite>(W_ii|W_if|W_ig|W_io)</cite>, of shape <cite>(4*hidden_size, input_size)</cite> for <cite>k = 0</cite>.
Otherwise, the shape is <cite>(4*hidden_size, num_directions * hidden_size)</cite></p></li>
<li><p><strong>~LSTM.weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the <span class="math">\(\text{k}^{th}\)</span> layer
<cite>(W_hi|W_hf|W_hg|W_ho)</cite>, of shape <cite>(4*hidden_size, hidden_size)</cite></p></li>
<li><p><strong>~LSTM.bias_ih_l[k]</strong> – the learnable input-hidden bias of the <span class="math">\(\text{k}^{th}\)</span> layer
<cite>(b_ii|b_if|b_ig|b_io)</cite>, of shape <cite>(4*hidden_size)</cite></p></li>
<li><p><strong>~LSTM.bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the <span class="math">\(\text{k}^{th}\)</span> layer
<cite>(b_hi|b_hf|b_hg|b_ho)</cite>, of shape <cite>(4*hidden_size)</cite></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the following conditions are satisfied:
1) cudnn is enabled,
2) input data is on the GPU
3) input data has dtype <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>
4) V100 GPU is used,
5) input data is not in <code class="docutils literal notranslate"><span class="pre">PackedSequence</span></code> format
persistent algorithm can be selected to improve performance.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">hn</span><span class="p">,</span> <span class="n">cn</span><span class="p">)</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="gru">
<h4><span class="hidden-section">GRU</span><a class="headerlink" href="#gru" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.GRU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.GRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[\begin{array}{ll}
    r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
    z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\
    n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\
    h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}
\end{array}

\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the input
at time <cite>t</cite>, <span class="math">\(h_{(t-1)}\)</span> is the hidden state of the layer
at time <cite>t-1</cite> or the initial hidden state at time <cite>0</cite>, and <span class="math">\(r_t\)</span>,
<span class="math">\(z_t\)</span>, <span class="math">\(n_t\)</span> are the reset, update, and new gates, respectively.
<span class="math">\(\sigma\)</span> is the sigmoid function, and <span class="math">\(*\)</span> is the Hadamard product.</p>
<p>In a multilayer GRU, the input <span class="math">\(x^{(l)}_t\)</span> of the <span class="math">\(l\)</span> -th layer
(<span class="math">\(l &gt;= 2\)</span>) is the hidden state <span class="math">\(h^{(l-1)}_t\)</span> of the previous layer multiplied by
dropout <span class="math">\(\delta^{(l-1)}_t\)</span> where each <span class="math">\(\delta^{(l-1)}_t\)</span> is a Bernoulli random
variable which is <span class="math">\(0\)</span> with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></p></li>
<li><p><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></p></li>
<li><p><strong>num_layers</strong> – Number of recurrent layers. E.g., setting <code class="docutils literal notranslate"><span class="pre">num_layers=2</span></code>
would mean stacking two GRUs together to form a <cite>stacked GRU</cite>,
with the second GRU taking in outputs of the first GRU and
computing the final results. Default: 1</p></li>
<li><p><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>batch_first</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided
as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>dropout</strong> – If non-zero, introduces a <cite>Dropout</cite> layer on the outputs of each
GRU layer except the last layer, with dropout probability equal to
<code class="xref py py-attr docutils literal notranslate"><span class="pre">dropout</span></code>. Default: 0</p></li>
<li><p><strong>bidirectional</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, becomes a bidirectional GRU. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs: input, h_0</dt><dd><ul class="simple">
<li><p><strong>input</strong> of shape <cite>(seq_len, batch, input_size)</cite>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
for details.</p></li>
<li><p><strong>h_0</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided. If the RNN is bidirectional,
num_directions should be 2, else it should be 1.</p></li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt><dd><ul>
<li><p><strong>output</strong> of shape <cite>(seq_len, batch, num_directions * hidden_size)</cite>: tensor
containing the output features h_t from the last layer of the GRU,
for each <cite>t</cite>. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been
given as the input, the output will also be a packed sequence.
For the unpacked case, the directions can be separated
using <code class="docutils literal notranslate"><span class="pre">output.view(seq_len,</span> <span class="pre">batch,</span> <span class="pre">num_directions,</span> <span class="pre">hidden_size)</span></code>,
with forward and backward being direction <cite>0</cite> and <cite>1</cite> respectively.</p>
<p>Similarly, the directions can be separated in the packed case.</p>
</li>
<li><p><strong>h_n</strong> of shape <cite>(num_layers * num_directions, batch, hidden_size)</cite>: tensor
containing the hidden state for <cite>t = seq_len</cite></p>
<p>Like <em>output</em>, the layers can be separated using
<code class="docutils literal notranslate"><span class="pre">h_n.view(num_layers,</span> <span class="pre">num_directions,</span> <span class="pre">batch,</span> <span class="pre">hidden_size)</span></code>.</p>
</li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input1: <span class="math">\((L, N, H_{in})\)</span> tensor containing input features where
<span class="math">\(H_{in}=\text{input\_size}\)</span> and <cite>L</cite> represents a sequence length.</p></li>
<li><p>Input2: <span class="math">\((S, N, H_{out})\)</span> tensor
containing the initial hidden state for each element in the batch.
<span class="math">\(H_{out}=\text{hidden\_size}\)</span>
Defaults to zero if not provided. where <span class="math">\(S=\text{num\_layers} * \text{num\_directions}\)</span>
If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p></li>
<li><p>Output1: <span class="math">\((L, N, H_{all})\)</span> where <span class="math">\(H_all=\text{num\_directions} * \text{hidden\_size}\)</span></p></li>
<li><p>Output2: <span class="math">\((S, N, H_{out})\)</span> tensor containing the next hidden state
for each element in the batch</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~GRU.weight_ih_l[k]</strong> – the learnable input-hidden weights of the <span class="math">\(\text{k}^{th}\)</span> layer
(W_ir|W_iz|W_in), of shape <cite>(3*hidden_size, input_size)</cite> for <cite>k = 0</cite>.
Otherwise, the shape is <cite>(3*hidden_size, num_directions * hidden_size)</cite></p></li>
<li><p><strong>~GRU.weight_hh_l[k]</strong> – the learnable hidden-hidden weights of the <span class="math">\(\text{k}^{th}\)</span> layer
(W_hr|W_hz|W_hn), of shape <cite>(3*hidden_size, hidden_size)</cite></p></li>
<li><p><strong>~GRU.bias_ih_l[k]</strong> – the learnable input-hidden bias of the <span class="math">\(\text{k}^{th}\)</span> layer
(b_ir|b_iz|b_in), of shape <cite>(3*hidden_size)</cite></p></li>
<li><p><strong>~GRU.bias_hh_l[k]</strong> – the learnable hidden-hidden bias of the <span class="math">\(\text{k}^{th}\)</span> layer
(b_hr|b_hz|b_hn), of shape <cite>(3*hidden_size)</cite></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the following conditions are satisfied:
1) cudnn is enabled,
2) input data is on the GPU
3) input data has dtype <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>
4) V100 GPU is used,
5) input data is not in <code class="docutils literal notranslate"><span class="pre">PackedSequence</span></code> format
persistent algorithm can be selected to improve performance.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="rnncell">
<h4><span class="hidden-section">RNNCell</span><a class="headerlink" href="#rnncell" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.RNNCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNNCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em>, <em>nonlinearity='tanh'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.RNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>An Elman RNN cell with tanh or ReLU non-linearity.</p>
<div class="math">
\[h' = \tanh(W_{ih} x + b_{ih}  +  W_{hh} h + b_{hh})\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">nonlinearity</span></code> is <cite>‘relu’</cite>, then ReLU is used in place of tanh.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></p></li>
<li><p><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></p></li>
<li><p><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>nonlinearity</strong> – The non-linearity to use. Can be either <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code> or <code class="docutils literal notranslate"><span class="pre">'relu'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs: input, hidden</dt><dd><ul class="simple">
<li><p><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</p></li>
<li><p><strong>hidden</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.
Defaults to zero if not provided.</p></li>
</ul>
</dd>
<dt>Outputs: h’</dt><dd><ul class="simple">
<li><p><strong>h’</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input1: <span class="math">\((N, H_{in})\)</span> tensor containing input features where
<span class="math">\(H_{in}\)</span> = <cite>input_size</cite></p></li>
<li><p>Input2: <span class="math">\((N, H_{out})\)</span> tensor containing the initial hidden
state for each element in the batch where <span class="math">\(H_{out}\)</span> = <cite>hidden_size</cite>
Defaults to zero if not provided.</p></li>
<li><p>Output: <span class="math">\((N, H_{out})\)</span> tensor containing the next hidden state
for each element in the batch</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~RNNCell.weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(hidden_size, input_size)</cite></p></li>
<li><p><strong>~RNNCell.weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(hidden_size, hidden_size)</cite></p></li>
<li><p><strong>~RNNCell.bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(hidden_size)</cite></p></li>
<li><p><strong>~RNNCell.bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(hidden_size)</cite></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx = rnn(input[i], hx)</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstmcell">
<h4><span class="hidden-section">LSTMCell</span><a class="headerlink" href="#lstmcell" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LSTMCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTMCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A long short-term memory (LSTM) cell.</p>
<div class="math">
\[\begin{array}{ll}
i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\
f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\
g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\
o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\
c' = f * c + i * g \\
h' = o * \tanh(c') \\
\end{array}\]</div>
<p>where <span class="math">\(\sigma\)</span> is the sigmoid function, and <span class="math">\(*\)</span> is the Hadamard product.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></p></li>
<li><p><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></p></li>
<li><p><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and
<cite>b_hh</cite>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs: input, (h_0, c_0)</dt><dd><ul>
<li><p><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</p></li>
<li><p><strong>h_0</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.</p></li>
<li><p><strong>c_0</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial cell state
for each element in the batch.</p>
<p>If <cite>(h_0, c_0)</cite> is not provided, both <strong>h_0</strong> and <strong>c_0</strong> default to zero.</p>
</li>
</ul>
</dd>
<dt>Outputs: (h_1, c_1)</dt><dd><ul class="simple">
<li><p><strong>h_1</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</p></li>
<li><p><strong>c_1</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next cell state
for each element in the batch</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~LSTMCell.weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(4*hidden_size, input_size)</cite></p></li>
<li><p><strong>~LSTMCell.weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(4*hidden_size, hidden_size)</cite></p></li>
<li><p><strong>~LSTMCell.bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(4*hidden_size)</cite></p></li>
<li><p><strong>~LSTMCell.bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(4*hidden_size)</cite></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx, cx = rnn(input[i], (hx, cx))</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="grucell">
<h4><span class="hidden-section">GRUCell</span><a class="headerlink" href="#grucell" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.GRUCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRUCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.GRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A gated recurrent unit (GRU) cell</p>
<div class="math">
\[\begin{array}{ll}
r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\
z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\
n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\
h' = (1 - z) * n + z * h
\end{array}\]</div>
<p>where <span class="math">\(\sigma\)</span> is the sigmoid function, and <span class="math">\(*\)</span> is the Hadamard product.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> – The number of expected features in the input <cite>x</cite></p></li>
<li><p><strong>hidden_size</strong> – The number of features in the hidden state <cite>h</cite></p></li>
<li><p><strong>bias</strong> – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then the layer does not use bias weights <cite>b_ih</cite> and
<cite>b_hh</cite>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs: input, hidden</dt><dd><ul class="simple">
<li><p><strong>input</strong> of shape <cite>(batch, input_size)</cite>: tensor containing input features</p></li>
<li><p><strong>hidden</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the initial hidden
state for each element in the batch.
Defaults to zero if not provided.</p></li>
</ul>
</dd>
<dt>Outputs: h’</dt><dd><ul class="simple">
<li><p><strong>h’</strong> of shape <cite>(batch, hidden_size)</cite>: tensor containing the next hidden state
for each element in the batch</p></li>
</ul>
</dd>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input1: <span class="math">\((N, H_{in})\)</span> tensor containing input features where
<span class="math">\(H_{in}\)</span> = <cite>input_size</cite></p></li>
<li><p>Input2: <span class="math">\((N, H_{out})\)</span> tensor containing the initial hidden
state for each element in the batch where <span class="math">\(H_{out}\)</span> = <cite>hidden_size</cite>
Defaults to zero if not provided.</p></li>
<li><p>Output: <span class="math">\((N, H_{out})\)</span> tensor containing the next hidden state
for each element in the batch</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~GRUCell.weight_ih</strong> – the learnable input-hidden weights, of shape
<cite>(3*hidden_size, input_size)</cite></p></li>
<li><p><strong>~GRUCell.weight_hh</strong> – the learnable hidden-hidden weights, of shape
<cite>(3*hidden_size, hidden_size)</cite></p></li>
<li><p><strong>~GRUCell.bias_ih</strong> – the learnable input-hidden bias, of shape <cite>(3*hidden_size)</cite></p></li>
<li><p><strong>~GRUCell.bias_hh</strong> – the learnable hidden-hidden bias, of shape <cite>(3*hidden_size)</cite></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All the weights and biases are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>
where <span class="math">\(k = \frac{1}{\text{hidden\_size}}\)</span></p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="go">        hx = rnn(input[i], hx)</span>
<span class="go">        output.append(hx)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="linear-layers">
<h3>Linear layers<a class="headerlink" href="#linear-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="linear">
<h4><span class="hidden-section">Linear</span><a class="headerlink" href="#linear" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Linear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Linear</code><span class="sig-paren">(</span><em>in_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math">\(y = xA^T + b\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> – size of each input sample</p></li>
<li><p><strong>out_features</strong> – size of each output sample</p></li>
<li><p><strong>bias</strong> – If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the layer will not learn an additive bias.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *, H_{in})\)</span> where <span class="math">\(*\)</span> means any number of
additional dimensions and <span class="math">\(H_{in} = \text{in\_features}\)</span></p></li>
<li><p>Output: <span class="math">\((N, *, H_{out})\)</span> where all but the last dimension
are the same shape as the input and <span class="math">\(H_{out} = \text{out\_features}\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~Linear.weight</strong> – the learnable weights of the module of shape
<span class="math">\((\text{out\_features}, \text{in\_features})\)</span>. The values are
initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where
<span class="math">\(k = \frac{1}{\text{in\_features}}\)</span></p></li>
<li><p><strong>~Linear.bias</strong> – the learnable bias of the module of shape <span class="math">\((\text{out\_features})\)</span>.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the values are initialized from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math">\(k = \frac{1}{\text{in\_features}}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([128, 30])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="bilinear">
<h4><span class="hidden-section">Bilinear</span><a class="headerlink" href="#bilinear" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Bilinear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Bilinear</code><span class="sig-paren">(</span><em>in1_features</em>, <em>in2_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a bilinear transformation to the incoming data:
<span class="math">\(y = x_1 A x_2 + b\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in1_features</strong> – size of each first input sample</p></li>
<li><p><strong>in2_features</strong> – size of each second input sample</p></li>
<li><p><strong>out_features</strong> – size of each output sample</p></li>
<li><p><strong>bias</strong> – If set to False, the layer will not learn an additive bias.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input1: <span class="math">\((N, *, H_{in1})\)</span> where <span class="math">\(H_{in1}=\text{in1\_features}\)</span> and
<span class="math">\(*\)</span> means any number of additional dimensions. All but the last dimension
of the inputs should be the same.</p></li>
<li><p>Input2: <span class="math">\((N, *, H_{in2})\)</span> where <span class="math">\(H_{in2}=\text{in2\_features}\)</span>.</p></li>
<li><p>Output: <span class="math">\((N, *, H_{out})\)</span> where <span class="math">\(H_{out}=\text{out\_features}\)</span>
and all but the last dimension are the same shape as the input.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~Bilinear.weight</strong> – the learnable weights of the module of shape
<span class="math">\((\text{out\_features}, \text{in1\_features}, \text{in2\_features})\)</span>.
The values are initialized from <span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where
<span class="math">\(k = \frac{1}{\text{in1\_features}}\)</span></p></li>
<li><p><strong>~Bilinear.bias</strong> – the learnable bias of the module of shape <span class="math">\((\text{out\_features})\)</span>.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the values are initialized from
<span class="math">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where
<span class="math">\(k = \frac{1}{\text{in1\_features}}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Bilinear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([128, 40])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dropout-layers">
<h3>Dropout layers<a class="headerlink" href="#dropout-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="dropout">
<h4><span class="hidden-section">Dropout</span><a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Dropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli
distribution. Each channel will be zeroed out independently on every forward
call.</p>
<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature
detectors</a> .</p>
<p>Furthermore, the outputs are scaled by a factor of <span class="math">\(\frac{1}{1-p}\)</span> during
training. This means that during evaluation the module simply computes an
identity function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of an element to be zeroed. Default: 0.5</p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span>. Input can be of any shape</p></li>
<li><p>Output: <span class="math">\((*)\)</span>. Output is of the same shape as input</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout2d">
<h4><span class="hidden-section">Dropout2d</span><a class="headerlink" href="#dropout2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Dropout2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout2d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 2D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 2D tensor <span class="math">\(\text{input}[i, j]\)</span>).
Each channel will be zeroed out independently on every forward call with
probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>Usually the input comes from <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> modules.</p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.Dropout2d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – probability of an element to be zero-ed.</p></li>
<li><p><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout3d">
<h4><span class="hidden-section">Dropout3d</span><a class="headerlink" href="#dropout3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Dropout3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout3d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 3D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 3D tensor <span class="math">\(\text{input}[i, j]\)</span>).
Each channel will be zeroed out independently on every forward call with
probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>Usually the input comes from <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Conv3d</span></code> modules.</p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then i.i.d. dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.Dropout3d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – probability of an element to be zeroed.</p></li>
<li><p><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, D, H, W)\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout3d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="alphadropout">
<h4><span class="hidden-section">AlphaDropout</span><a class="headerlink" href="#alphadropout" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AlphaDropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AlphaDropout</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AlphaDropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Alpha Dropout over the input.</p>
<p>Alpha Dropout is a type of Dropout that maintains the self-normalizing
property.
For an input with zero mean and unit standard deviation, the output of
Alpha Dropout maintains the original mean and standard deviation of the
input.
Alpha Dropout goes hand-in-hand with SELU activation function, which ensures
that the outputs have zero mean and unit standard deviation.</p>
<p>During training, it randomly masks some of the elements of the input
tensor with probability <em>p</em> using samples from a bernoulli distribution.
The elements to masked are randomized on every forward call, and scaled
and shifted to maintain zero mean and unit standard deviation.</p>
<p>During evaluation the module simply computes an identity function.</p>
<p>More details can be found in the paper <a class="reference external" href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a> .</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – probability of an element to be dropped. Default: 0.5</p></li>
<li><p><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation
in-place</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span>. Input can be of any shape</p></li>
<li><p>Output: <span class="math">\((*)\)</span>. Output is of the same shape as input</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AlphaDropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="sparse-layers">
<h3>Sparse layers<a class="headerlink" href="#sparse-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="embedding">
<h4><span class="hidden-section">Embedding</span><a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Embedding">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Embedding</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em>, <em>_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – size of the dictionary of embeddings</p></li>
<li><p><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of each embedding vector</p></li>
<li><p><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – If given, pads the output with the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code>
(initialized to zeros) whenever it encounters the index.</p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> matrix will be a sparse tensor.
See Notes for more details regarding sparse gradients.</p></li>
</ul>
</dd>
<dt class="field-even">Variables</dt>
<dd class="field-even"><p><strong>~Embedding.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape (num_embeddings, embedding_dim)
initialized from <span class="math">\(\mathcal{N}(0, 1)\)</span></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span>, LongTensor of arbitrary shape containing the indices to extract</p></li>
<li><p>Output: <span class="math">\((*, H)\)</span>, where <cite>*</cite> is the input shape and <span class="math">\(H=\text{embedding\_dim}\)</span></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Keep in mind that only a limited number of optimizers support
sparse gradients: currently it’s <code class="xref py py-class docutils literal notranslate"><span class="pre">optim.SGD</span></code> (<cite>CUDA</cite> and <cite>CPU</cite>),
<code class="xref py py-class docutils literal notranslate"><span class="pre">optim.SparseAdam</span></code> (<cite>CUDA</cite> and <cite>CPU</cite>) and <code class="xref py py-class docutils literal notranslate"><span class="pre">optim.Adagrad</span></code> (<cite>CPU</cite>)</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>With <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> set, the embedding vector at
<code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code> is initialized to all zeros. However, note that this
vector can be modified afterwards, e.g., using a customized
initialization method, and thus changing the vector used to pad the
output. The gradient for this vector from <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a>
is always zero.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[-0.0251, -1.6902,  0.7172],</span>
<span class="go">         [-0.6431,  0.0748,  0.6969],</span>
<span class="go">         [ 1.4970,  1.3448, -0.9685],</span>
<span class="go">         [-0.3677, -2.7265, -0.1685]],</span>

<span class="go">        [[ 1.4970,  1.3448, -0.9685],</span>
<span class="go">         [ 0.4362, -0.4004,  0.9400],</span>
<span class="go">         [-0.6431,  0.0748,  0.6969],</span>
<span class="go">         [ 0.9124, -2.3616,  1.1151]]])</span>


<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.1535, -2.0309,  0.9315],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [-0.1655,  0.9897,  0.0635]]])</span>
</pre></div>
</div>
<dl class="classmethod">
<dt id="torch.nn.Embedding.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>embeddings</em>, <em>freeze=True</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Embedding.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates Embedding instance from given 2-dimensional FloatTensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embeddings</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – FloatTensor containing weights for the Embedding.
First dimension is being passed to Embedding as <code class="docutils literal notranslate"><span class="pre">num_embeddings</span></code>, second as <code class="docutils literal notranslate"><span class="pre">embedding_dim</span></code>.</p></li>
<li><p><strong>freeze</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the tensor does not get updated in the learning process.
Equivalent to <code class="docutils literal notranslate"><span class="pre">embedding.weight.requires_grad</span> <span class="pre">=</span> <span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – See module initialization documentation.</p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – See module initialization documentation.</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – See module initialization documentation. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – See module initialization documentation. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – See module initialization documentation.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># FloatTensor containing pretrained weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">,</span> <span class="mf">6.3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Get embeddings for index 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[ 4.0000,  5.1000,  6.3000]])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="embeddingbag">
<h4><span class="hidden-section">EmbeddingBag</span><a class="headerlink" href="#embeddingbag" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.EmbeddingBag">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">EmbeddingBag</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em>, <em>sparse=False</em>, <em>_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.EmbeddingBag" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sums or means of ‘bags’ of embeddings, without instantiating the
intermediate embeddings.</p>
<p>For bags of constant length, this class</p>
<blockquote>
<div><ul class="simple">
<li><p>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;sum&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.sum(dim=0)</span></code>,</p></li>
<li><p>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;mean&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.mean(dim=0)</span></code>,</p></li>
<li><p>with <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code> is equivalent to <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">Embedding</span></code></a> followed by <code class="docutils literal notranslate"><span class="pre">torch.max(dim=0)</span></code>.</p></li>
</ul>
</div></blockquote>
<p>However, <a class="reference internal" href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingBag</span></code></a> is much more time and memory efficient than using a chain of these
operations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – size of the dictionary of embeddings</p></li>
<li><p><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the size of each embedding vector</p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>. Specifies the way to reduce the bag.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> matrix will be a sparse tensor. See
Notes for more details regarding sparse gradients. Note: this option is not
supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Variables</dt>
<dd class="field-even"><p><strong>~EmbeddingBag.weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the learnable weights of the module of shape <cite>(num_embeddings, embedding_dim)</cite>
initialized from <span class="math">\(\mathcal{N}(0, 1)\)</span>.</p>
</dd>
</dl>
<p>Inputs: <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (LongTensor) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> (LongTensor, optional)</p>
<blockquote>
<div><ul>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 2D of shape <cite>(B, N)</cite>,</p>
<p>it will be treated as <code class="docutils literal notranslate"><span class="pre">B</span></code> bags (sequences) each of fixed length <code class="docutils literal notranslate"><span class="pre">N</span></code>, and
this will return <code class="docutils literal notranslate"><span class="pre">B</span></code> values aggregated in a way depending on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is ignored and required to be <code class="docutils literal notranslate"><span class="pre">None</span></code> in this case.</p>
</li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D of shape <cite>(N)</cite>,</p>
<p>it will be treated as a concatenation of multiple bags (sequences).
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is required to be a 1D tensor containing the
starting index positions of each bag in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. Therefore,
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> of shape <cite>(B)</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be viewed as
having <code class="docutils literal notranslate"><span class="pre">B</span></code> bags. Empty bags (i.e., having 0-length) will have
returned vectors filled by zeros.</p>
</li>
</ul>
</div></blockquote>
<p>Output shape: <cite>(B, embedding_dim)</cite></p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_sum</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="go">tensor([[-0.8861, -5.4350, -0.0523],</span>
<span class="go">        [ 1.1306, -2.5798, -1.0044]])</span>
</pre></div>
</div>
<dl class="classmethod">
<dt id="torch.nn.EmbeddingBag.from_pretrained">
<em class="property">classmethod </em><code class="descname">from_pretrained</code><span class="sig-paren">(</span><em>embeddings</em>, <em>freeze=True</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.EmbeddingBag.from_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates EmbeddingBag instance from given 2-dimensional FloatTensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embeddings</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – FloatTensor containing weights for the EmbeddingBag.
First dimension is being passed to EmbeddingBag as ‘num_embeddings’, second as ‘embedding_dim’.</p></li>
<li><p><strong>freeze</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the tensor does not get updated in the learning process.
Equivalent to <code class="docutils literal notranslate"><span class="pre">embeddingbag.weight.requires_grad</span> <span class="pre">=</span> <span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – See module initialization documentation. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – See module initialization documentation. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – See module initialization documentation. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – See module initialization documentation. Default: <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – See module initialization documentation. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># FloatTensor containing pretrained weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mf">5.1</span><span class="p">,</span> <span class="mf">6.3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddingbag</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Get embeddings for index 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddingbag</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[ 2.5000,  3.7000,  4.6500]])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="distance-functions">
<h3>Distance functions<a class="headerlink" href="#distance-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="cosinesimilarity">
<h4><span class="hidden-section">CosineSimilarity</span><a class="headerlink" href="#cosinesimilarity" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.CosineSimilarity">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineSimilarity</code><span class="sig-paren">(</span><em>dim=1</em>, <em>eps=1e-08</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.CosineSimilarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between <span class="math">\(x_1\)</span> and <span class="math">\(x_2\)</span>, computed along dim.</p>
<div class="math">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension where cosine similarity is computed. Default: 1</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-8</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input1: <span class="math">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite></p></li>
<li><p>Input2: <span class="math">\((\ast_1, D, \ast_2)\)</span>, same shape as the Input1</p></li>
<li><p>Output: <span class="math">\((\ast_1, \ast_2)\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CosineSimilarity</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pairwisedistance">
<h4><span class="hidden-section">PairwiseDistance</span><a class="headerlink" href="#pairwisedistance" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.PairwiseDistance">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PairwiseDistance</code><span class="sig-paren">(</span><em>p=2.0</em>, <em>eps=1e-06</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.PairwiseDistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the batchwise pairwise distance between vectors <span class="math">\(v_1\)</span>, <span class="math">\(v_2\)</span> using the p-norm:</p>
<div class="math">
\[\Vert x \Vert _p = \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<em>real</em>) – the norm degree. Default: 2</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-6</p></li>
<li><p><strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Determines whether or not to keep the batch dimension.
Default: False</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input1: <span class="math">\((N, D)\)</span> where <cite>D = vector dimension</cite></p></li>
<li><p>Input2: <span class="math">\((N, D)\)</span>, same shape as the Input1</p></li>
<li><p>Output: <span class="math">\((N)\)</span>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">keepdim</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, then <span class="math">\((N, 1)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pdist</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PairwiseDistance</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="loss-functions">
<h3>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="l1loss">
<h4><span class="hidden-section">L1Loss</span><a class="headerlink" href="#l1loss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.L1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">L1Loss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean absolute error (MAE) between each element in
the input <span class="math">\(x\)</span> and target <span class="math">\(y\)</span>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left| x_n - y_n \right|,

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then:</p>
<div class="math">
\[\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
    \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
\end{cases}

\]</div>
<p><span class="math">\(x\)</span> and <span class="math">\(y\)</span> are tensors of arbitrary shapes with a total
of <span class="math">\(n\)</span> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <span class="math">\(n\)</span>.</p>
<p>The division by <span class="math">\(n\)</span> can be avoided if one sets <code class="docutils literal notranslate"><span class="pre">reduction</span> <span class="pre">=</span> <span class="pre">'sum'</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <span class="math">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then
<span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="mseloss">
<h4><span class="hidden-section">MSELoss</span><a class="headerlink" href="#mseloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MSELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MSELoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean squared error (squared L2 norm) between
each element in the input <span class="math">\(x\)</span> and target <span class="math">\(y\)</span>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = \left( x_n - y_n \right)^2,

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then:</p>
<div class="math">
\[\ell(x, y) =
\begin{cases}
    \operatorname{mean}(L), &  \text{if reduction} = \text{'mean';}\\
    \operatorname{sum}(L),  &  \text{if reduction} = \text{'sum'.}
\end{cases}

\]</div>
<p><span class="math">\(x\)</span> and <span class="math">\(y\)</span> are tensors of arbitrary shapes with a total
of <span class="math">\(n\)</span> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <span class="math">\(n\)</span>.</p>
<p>The division by <span class="math">\(n\)</span> can be avoided if one sets <code class="docutils literal notranslate"><span class="pre">reduction</span> <span class="pre">=</span> <span class="pre">'sum'</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <span class="math">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="crossentropyloss">
<h4><span class="hidden-section">CrossEntropyLoss</span><a class="headerlink" href="#crossentropyloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.CrossEntropyLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CrossEntropyLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.LogSoftmax()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">nn.NLLLoss()</span></code> in one single class.</p>
<p>It is useful when training a classification problem with <cite>C</cite> classes.
If provided, the optional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> should be a 1D <cite>Tensor</cite>
assigning weight to each of the classes.
This is particularly useful when you have an unbalanced training set.</p>
<p>The <cite>input</cite> is expected to contain raw, unnormalized scores for each class.</p>
<p><cite>input</cite> has to be a Tensor of size either <span class="math">\((minibatch, C)\)</span> or
<span class="math">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math">\(K \geq 1\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>This criterion expects a class index in the range <span class="math">\([0, C-1]\)</span> as the
<cite>target</cite> for each value of a 1D tensor of size <cite>minibatch</cite>.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
               = -x[class] + \log\left(\sum_j \exp(x[j])\right)

\]</div>
<p>or in the case of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> argument being specified:</p>
<div class="math">
\[\text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)

\]</div>
<p>The losses are averaged across observations for each minibatch.</p>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 1\)</span>,
where <span class="math">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each class.
If given, has to be a Tensor of size <cite>C</cite></p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets.</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite>, or
<span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 1\)</span>
in the case of <cite>K</cite>-dimensional loss.</p></li>
<li><p>Target: <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>, or
<span class="math">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 1\)</span> in the case of
K-dimensional loss.</p></li>
<li><p>Output: scalar.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then the same size as the target:
<span class="math">\((N)\)</span>, or
<span class="math">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 1\)</span> in the case
of K-dimensional loss.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="ctcloss">
<h4><span class="hidden-section">CTCLoss</span><a class="headerlink" href="#ctcloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.CTCLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CTCLoss</code><span class="sig-paren">(</span><em>blank=0</em>, <em>reduction='mean'</em>, <em>zero_infinity=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.CTCLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The Connectionist Temporal Classification loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>blank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – blank label. Default <span class="math">\(0\)</span>.</p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
‘mean’: the output losses will be divided by the target lengths and
then the mean over the batch is taken. Default: ‘mean’</p></li>
<li><p><strong>zero_infinity</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to zero infinite losses and the associated gradients.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
Infinite losses mainly occur when the inputs are too short
to be aligned to the targets.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><dl class="simple">
<dt>log_probs: Tensor of size <span class="math">\((T, N, C)\)</span> where <cite>C = number of characters in alphabet including blank</cite>,</dt><dd><p><cite>T = input length</cite>, and <cite>N = batch size</cite>.
The logarithmized probabilities of the outputs
(e.g. obtained with <a class="reference internal" href="#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.log_softmax()</span></code></a>).</p>
</dd>
<dt>targets: Tensor of size <span class="math">\((N, S)\)</span> or <cite>(sum(target_lengths))</cite>.</dt><dd><p>Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.</p>
</dd>
<dt>input_lengths: Tuple or tensor of size <span class="math">\((N)\)</span>.</dt><dd><p>Lengths of the inputs (must each be <span class="math">\(\leq T\)</span>)</p>
</dd>
<dt>target_lengths: Tuple or tensor of size  <span class="math">\((N)\)</span>.</dt><dd><p>Lengths of the targets</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ctc_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">16</span><span class="p">,),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">,(</span><span class="mi">16</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="simple">
<dt>Reference:</dt><dd><p>A. Graves et al.: Connectionist Temporal Classification:
Labelling Unsegmented Sequence Data with Recurrent Neural Networks:
<a class="reference external" href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to use CuDNN, the following must be satisfied: <code class="xref py py-attr docutils literal notranslate"><span class="pre">targets</span></code> must be
in concatenated format, all <code class="xref py py-attr docutils literal notranslate"><span class="pre">input_lengths</span></code> must be <cite>T</cite>.  <span class="math">\(blank=0\)</span>,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">target_lengths</span></code> <span class="math">\(\leq 256\)</span>, the integer arguments must be of
dtype <code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.int32</span></code>.</p>
<p>The regular implementation uses the (more common in PyTorch) <cite>torch.long</cite> dtype.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="nllloss">
<h4><span class="hidden-section">NLLLoss</span><a class="headerlink" href="#nllloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.NLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">NLLLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.NLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss. It is useful to train a classification
problem with <cite>C</cite> classes.</p>
<p>If provided, the optional argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> should be a 1D Tensor assigning
weight to each of the classes. This is particularly useful when you have an
unbalanced training set.</p>
<p>The <cite>input</cite> given through a forward call is expected to contain
log-probabilities of each class. <cite>input</cite> has to be a Tensor of size either
<span class="math">\((minibatch, C)\)</span> or <span class="math">\((minibatch, C, d_1, d_2, ..., d_K)\)</span>
with <span class="math">\(K \geq 1\)</span> for the <cite>K</cite>-dimensional case (described later).</p>
<p>Obtaining log-probabilities in a neural network is easily achieved by
adding a  <cite>LogSoftmax</cite>  layer in the last layer of your network.
You may use <cite>CrossEntropyLoss</cite> instead, if you prefer not to add an extra
layer.</p>
<p>The <cite>target</cite> that this loss expects is a class index in the range <span class="math">\([0, C-1]\)</span>
where <cite>C = number of classes</cite>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_{y_n} x_{n,y_n}, \quad
w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &
    \text{if reduction} = \text{'mean';}\\
    \sum_{n=1}^N l_n,  &
    \text{if reduction} = \text{'sum'.}
\end{cases}

\]</div>
<p>Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <span class="math">\((minibatch, C, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 1\)</span>,
where <span class="math">\(K\)</span> is the number of dimensions, and a target of appropriate shape
(see below). In the case of images, it computes NLL loss per-pixel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over
non-ignored targets.</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite>, or
<span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 1\)</span>
in the case of <cite>K</cite>-dimensional loss.</p></li>
<li><p>Target: <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>, or
<span class="math">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 1\)</span> in the case of
K-dimensional loss.</p></li>
<li><p>Output: scalar.
If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then the same size as the target: <span class="math">\((N)\)</span>, or
<span class="math">\((N, d_1, d_2, ..., d_K)\)</span> with <span class="math">\(K \geq 1\)</span> in the case
of K-dimensional loss.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2D loss example (used, for example, with image inputs)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C x height x width</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">conv</span><span class="p">(</span><span class="n">data</span><span class="p">)),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poissonnllloss">
<h4><span class="hidden-section">PoissonNLLLoss</span><a class="headerlink" href="#poissonnllloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.PoissonNLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PoissonNLLLoss</code><span class="sig-paren">(</span><em>log_input=True</em>, <em>full=False</em>, <em>size_average=None</em>, <em>eps=1e-08</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.PoissonNLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Negative log likelihood loss with Poisson distribution of target.</p>
<p>The loss can be described as:</p>
<div class="math">
\[\text{target} \sim \mathrm{Poisson}(\text{input})

\text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})
                            + \log(\text{target!})\]</div>
<p>The last term can be omitted or approximated with Stirling formula. The
approximation is used for target values more than 1. For targets less or
equal to 1 zeros are added to the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_input</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> the loss is computed as
<span class="math">\(\exp(\text{input}) - \text{target}*\text{input}\)</span>, if <code class="docutils literal notranslate"><span class="pre">False</span></code> the loss is
<span class="math">\(\text{input} - \text{target}*\log(\text{input}+\text{eps})\)</span>.</p></li>
<li><p><strong>full</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – <p>whether to compute full loss, i. e. to add the
Stirling approximation term</p>
<div class="math">
\[\text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).

\]</div>
</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid evaluation of <span class="math">\(\log(0)\)</span> when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">log_input</span> <span class="pre">=</span> <span class="pre">False</span></code>. Default: 1e-8</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PoissonNLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">log_input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <span class="math">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar by default. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math">\((N, *)\)</span>,
the same shape as the input</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="kldivloss">
<h4><span class="hidden-section">KLDivLoss</span><a class="headerlink" href="#kldivloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.KLDivLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">KLDivLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.KLDivLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss</p>
<p>KL divergence is a useful distance measure for continuous distributions
and is often useful when performing direct regression over the space of
(discretely sampled) continuous output distributions.</p>
<p>As with <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a>, the <cite>input</cite> given is expected to contain
<em>log-probabilities</em> and is not restricted to a 2D Tensor.
The targets are given as <em>probabilities</em> (i.e. without taking the logarithm).</p>
<p>This criterion expects a <cite>target</cite> <cite>Tensor</cite> of the same size as the
<cite>input</cite> <cite>Tensor</cite>.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math">
\[l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right)

\]</div>
<p>where the index <span class="math">\(N\)</span> spans all dimensions of <code class="docutils literal notranslate"><span class="pre">input</span></code> and <span class="math">\(L\)</span> has the same
shape as <code class="docutils literal notranslate"><span class="pre">input</span></code>. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code> (default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then:</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), & \text{if reduction} = \text{'mean';} \\
    \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
\end{cases}

\]</div>
<p>In default <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> mode <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>, the losses are averaged for each minibatch over observations
<strong>as well as</strong> over dimensions. <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> mode gives the correct KL divergence where losses
are averaged over batch dimension only. <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> mode’s behavior will be changed to the same as
<code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> in the next major release.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied.
<code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code>: the sum of the output will be divided by batchsize.
<code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed.
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the output will be divided by the number of elements in the output.
Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated,
and in the meantime, specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> doesn’t return the true kl divergence value, please use
:attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> which aligns with KL math definition.
In the next major release, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> will be changed to be the same as <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code>.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <span class="math">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar by default. If :attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math">\((N, *)\)</span>,
the same shape as the input</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="bceloss">
<h4><span class="hidden-section">BCELoss</span><a class="headerlink" href="#bceloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BCELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCELoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BCELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the Binary Cross Entropy
between the target and the output:</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
    \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
\end{cases}

\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <span class="math">\(y\)</span> should be numbers
between 0 and 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size <cite>nbatch</cite>.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <span class="math">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math">\((N, *)\)</span>, same
shape as input.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="bcewithlogitsloss">
<h4><span class="hidden-section">BCEWithLogitsLoss</span><a class="headerlink" href="#bcewithlogitsloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BCEWithLogitsLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCEWithLogitsLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em>, <em>pos_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BCEWithLogitsLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This loss combines a <cite>Sigmoid</cite> layer and the <cite>BCELoss</cite> in one single
class. This version is more numerically stable than using a plain <cite>Sigmoid</cite>
followed by a <cite>BCELoss</cite> as, by combining the operations into one layer,
we take advantage of the log-sum-exp trick for numerical stability.</p>
<p>The unreduced (i.e. with <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> set to <code class="docutils literal notranslate"><span class="pre">'none'</span></code>) loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
+ (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],

\]</div>
<p>where <span class="math">\(N\)</span> is the batch size. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is not <code class="docutils literal notranslate"><span class="pre">'none'</span></code>
(default <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>), then</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
    \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
\end{cases}

\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <cite>t[i]</cite> should be numbers
between 0 and 1.</p>
<p>It’s possible to trade off recall and precision by adding weights to positive examples.
In this case the loss can be described as:</p>
<div class="math">
\[\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ p_n y_n \cdot \log \sigma(x_n)
+ (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],

\]</div>
<p>where <span class="math">\(p_n\)</span> is the weight of the positive class for sample <span class="math">\(n\)</span> in the batch.
<span class="math">\(p_n &gt; 1\)</span> increases the recall, <span class="math">\(p_n &lt; 1\)</span> increases the precision.</p>
<p>For example, if a dataset contains 100 positive and 300 negative examples of a single class,
then <cite>pos_weight</cite> for the class should be equal to <span class="math">\(\frac{300}{100}=3\)</span>.
The loss would act as if the dataset contains <span class="math">\(3\times 100=300\)</span> positive examples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size <cite>nbatch</cite>.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>pos_weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a weight of positive examples.
Must be a vector with length equal to the number of classes.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Shape:</dt><dd><blockquote>
<div><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <span class="math">\(*\)</span> means, any number of additional dimensions</p></li>
<li><p>Target: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math">\((N, *)\)</span>, same
shape as input.</p></li>
</ul>
</div></blockquote>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="marginrankingloss">
<h4><span class="hidden-section">MarginRankingLoss</span><a class="headerlink" href="#marginrankingloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MarginRankingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MarginRankingLoss</code><span class="sig-paren">(</span><em>margin=0.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MarginRankingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given
inputs <span class="math">\(x1\)</span>, <span class="math">\(x2\)</span>, two 1D mini-batch <cite>Tensor`s,
and a label 1D mini-batch tensor :math:`y</cite> (containing 1 or -1).</p>
<p>If <span class="math">\(y = 1\)</span> then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for <span class="math">\(y = -1\)</span>.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math">
\[\text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <span class="math">\(0\)</span>.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, D)\)</span> where <cite>N</cite> is the batch size and <cite>D</cite> is the size of a sample.</p></li>
<li><p>Target: <span class="math">\((N)\)</span></p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math">\((N)\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="hingeembeddingloss">
<h4><span class="hidden-section">HingeEmbeddingLoss</span><a class="headerlink" href="#hingeembeddingloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.HingeEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">HingeEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.HingeEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the loss given an input tensor <span class="math">\(x\)</span> and a labels tensor <span class="math">\(y\)</span>
(containing 1 or -1).
This is usually used for measuring whether two inputs are similar or
dissimilar, e.g. using the L1 pairwise distance as <span class="math">\(x\)</span>, and is typically
used for learning nonlinear embeddings or semi-supervised learning.</p>
<p>The loss function for <span class="math">\(n\)</span>-th sample in the mini-batch is</p>
<div class="math">
\[l_n = \begin{cases}
    x_n, & \text{if}\; y_n = 1,\\
    \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1,
\end{cases}

\]</div>
<p>and the total loss functions is</p>
<div class="math">
\[\ell(x, y) = \begin{cases}
    \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
    \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
\end{cases}

\]</div>
<p>where <span class="math">\(L = \{l_1,\dots,l_N\}^\top\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <cite>1</cite>.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span> where <span class="math">\(*\)</span> means, any number of dimensions. The sum operation
operates over all the elements.</p></li>
<li><p>Target: <span class="math">\((*)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar. If :attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then same shape as the input</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="multilabelmarginloss">
<h4><span class="hidden-section">MultiLabelMarginLoss</span><a class="headerlink" href="#multilabelmarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MultiLabelMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelMarginLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MultiLabelMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class multi-classification
hinge loss (margin-based loss) between input <span class="math">\(x\)</span> (a 2D mini-batch <cite>Tensor</cite>)
and output <span class="math">\(y\)</span> (which is a 2D <cite>Tensor</cite> of target class indices).
For each sample in the mini-batch:</p>
<div class="math">
\[\text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}

\]</div>
<p>where <span class="math">\(x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}\)</span>, <span class="math">\(y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}\)</span>, <span class="math">\(0 \leq y[j] \leq \text{x.size}(0)-1\)</span>, and <span class="math">\(i \neq y[j]\)</span> for all <span class="math">\(i\)</span> and <span class="math">\(j\)</span>.</p>
<p><span class="math">\(y\)</span> and <span class="math">\(x\)</span> must have the same size.</p>
<p>The criterion only considers a contiguous block of non-negative targets that
starts at the front.</p>
<p>This allows for different samples to have variable amounts of target classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((C)\)</span> or <span class="math">\((N, C)\)</span> where <cite>N</cite> is the batch size and <cite>C</cite>
is the number of classes.</p></li>
<li><p>Target: <span class="math">\((C)\)</span> or <span class="math">\((N, C)\)</span>, label targets padded by -1 ensuring same shape as the input.</p></li>
<li><p>Output: scalar. If :attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math">\((N)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiLabelMarginLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># for target y, only consider labels 3 and 0, not after label -1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))</span>
<span class="go">tensor(0.8500)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="smoothl1loss">
<h4><span class="hidden-section">SmoothL1Loss</span><a class="headerlink" href="#smoothl1loss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.SmoothL1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SmoothL1Loss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.
It is less sensitive to outliers than the <cite>MSELoss</cite> and in some cases
prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick).
Also known as the Huber loss:</p>
<div class="math">
\[\text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}

\]</div>
<p>where <span class="math">\(z_{i}\)</span> is given by:</p>
<div class="math">
\[z_{i} =
\begin{cases}
0.5 (x_i - y_i)^2, & \text{if } |x_i - y_i| < 1 \\
|x_i - y_i| - 0.5, & \text{otherwise }
\end{cases}

\]</div>
<p><span class="math">\(x\)</span> and <span class="math">\(y\)</span> arbitrary shapes with a total of <span class="math">\(n\)</span> elements each
the sum operation still operates over all the elements, and divides by <span class="math">\(n\)</span>.</p>
<p>The division by <span class="math">\(n\)</span> can be avoided if sets <code class="docutils literal notranslate"><span class="pre">reduction</span> <span class="pre">=</span> <span class="pre">'sum'</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, *)\)</span> where <span class="math">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math">\((N, *)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then
<span class="math">\((N, *)\)</span>, same shape as the input</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="softmarginloss">
<h4><span class="hidden-section">SoftMarginLoss</span><a class="headerlink" href="#softmarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.SoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SoftMarginLoss</code><span class="sig-paren">(</span><em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.SoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a two-class classification
logistic loss between input tensor <span class="math">\(x\)</span> and target tensor <span class="math">\(y\)</span>
(containing 1 or -1).</p>
<div class="math">
\[\text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((*)\)</span> where <span class="math">\(*\)</span> means, any number of additional
dimensions</p></li>
<li><p>Target: <span class="math">\((*)\)</span>, same shape as the input</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then same shape as the input</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="multilabelsoftmarginloss">
<h4><span class="hidden-section">MultiLabelSoftMarginLoss</span><a class="headerlink" href="#multilabelsoftmarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MultiLabelSoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelSoftMarginLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MultiLabelSoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-label one-versus-all
loss based on max-entropy, between input <span class="math">\(x\)</span> and target <span class="math">\(y\)</span> of size
<span class="math">\((N, C)\)</span>.
For each sample in the minibatch:</p>
<div class="math">
\[loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})
                 + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)

\]</div>
<p>where <span class="math">\(i \in \left\{0, \; \cdots , \; \text{x.nElement}() - 1\right\}\)</span>,
<span class="math">\(y[i] \in \left\{0, \; 1\right\}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C)\)</span> where <cite>N</cite> is the batch size and <cite>C</cite> is the number of classes.</p></li>
<li><p>Target: <span class="math">\((N, C)\)</span>, label targets padded by -1 ensuring same shape as the input.</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math">\((N)\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="cosineembeddingloss">
<h4><span class="hidden-section">CosineEmbeddingLoss</span><a class="headerlink" href="#cosineembeddingloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.CosineEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=0.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.CosineEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given input tensors
<span class="math">\(x_1\)</span>, <span class="math">\(x_2\)</span> and a <cite>Tensor</cite> label <span class="math">\(y\)</span> with values 1 or -1.
This is used for measuring whether two inputs are similar or dissimilar,
using the cosine distance, and is typically used for learning nonlinear
embeddings or semi-supervised learning.</p>
<p>The loss function for each sample is:</p>
<div class="math">
\[\text{loss}(x, y) =
\begin{cases}
1 - \cos(x_1, x_2), & \text{if } y = 1 \\
\max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y = -1
\end{cases}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Should be a number from <span class="math">\(-1\)</span> to <span class="math">\(1\)</span>,</p>
</dd>
</dl>
<p>:param <span class="math">\(0\)</span> to <span class="math">\(0.5\)</span> is suggested. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">margin</span></code> is missing, the:
:param default value is <span class="math">\(0\)</span>.:
:param size_average: Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,</p>
<blockquote>
<div><p>the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="multimarginloss">
<h4><span class="hidden-section">MultiMarginLoss</span><a class="headerlink" href="#multimarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MultiMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiMarginLoss</code><span class="sig-paren">(</span><em>p=1</em>, <em>margin=1.0</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MultiMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class classification hinge
loss (margin-based loss) between input <span class="math">\(x\)</span> (a 2D mini-batch <cite>Tensor</cite>) and
output <span class="math">\(y\)</span> (which is a 1D tensor of target class indices,
<span class="math">\(0 \leq y \leq \text{x.size}(1)-1\)</span>):</p>
<p>For each mini-batch sample, the loss in terms of the 1D input <span class="math">\(x\)</span> and scalar
output <span class="math">\(y\)</span> is:</p>
<div class="math">
\[\text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}

\]</div>
<p>where <span class="math">\(x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}\)</span>
and <span class="math">\(i \neq y\)</span>.</p>
<p>Optionally, you can give non-equal weighting on the classes by passing
a 1D <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> tensor into the constructor.</p>
<p>The loss function then becomes:</p>
<div class="math">
\[\text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Has a default value of <span class="math">\(1\)</span>. <span class="math">\(1\)</span> and <span class="math">\(2\)</span>
are the only supported values.</p></li>
<li><p><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Has a default value of <span class="math">\(1\)</span>.</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, it has to be a Tensor of size <cite>C</cite>. Otherwise, it is
treated as if having all ones.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="tripletmarginloss">
<h4><span class="hidden-section">TripletMarginLoss</span><a class="headerlink" href="#tripletmarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.TripletMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">TripletMarginLoss</code><span class="sig-paren">(</span><em>margin=1.0</em>, <em>p=2.0</em>, <em>eps=1e-06</em>, <em>swap=False</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.TripletMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the triplet loss given an input
tensors <span class="math">\(x1\)</span>, <span class="math">\(x2\)</span>, <span class="math">\(x3\)</span> and a margin with a value greater than <span class="math">\(0\)</span>.
This is used for measuring a relative similarity between samples. A triplet
is composed by <cite>a</cite>, <cite>p</cite> and <cite>n</cite>: <cite>anchor</cite>, <cite>positive examples</cite> and <cite>negative
examples</cite> respectively. The shapes of all input tensors should be
<span class="math">\((N, D)\)</span>.</p>
<p>The distance swap is described in detail in the paper <a class="reference external" href="http://www.iis.ee.ic.ac.uk/%7Evbalnt/shallow_descr/TFeat_paper.pdf">Learning shallow
convolutional feature descriptors with triplet losses</a> by
V. Balntas, E. Riba et al.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="math">
\[L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}

\]</div>
<p>where</p>
<div class="math">
\[d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Default: <span class="math">\(1\)</span>.</p></li>
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – The norm degree for pairwise distance. Default: <span class="math">\(2\)</span>.</p></li>
<li><p><strong>swap</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The distance swap is described in detail in the paper
<cite>Learning shallow convolutional feature descriptors with triplet losses</cite> by
V. Balntas, E. Riba et al. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there are multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, D)\)</span> where <span class="math">\(D\)</span> is the vector dimension.</p></li>
<li><p>Output: scalar. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code> is <code class="docutils literal notranslate"><span class="pre">'none'</span></code>, then <span class="math">\((N)\)</span>.</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">triplet_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">input3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="vision-layers">
<h3>Vision layers<a class="headerlink" href="#vision-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pixelshuffle">
<h4><span class="hidden-section">PixelShuffle</span><a class="headerlink" href="#pixelshuffle" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.PixelShuffle">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PixelShuffle</code><span class="sig-paren">(</span><em>upscale_factor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.PixelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a tensor of shape <span class="math">\((*, C \times r^2, H, W)\)</span>
to a tensor of shape <span class="math">\((*, C, H \times r, W \times r)\)</span>.</p>
<p>This is useful for implementing efficient sub-pixel convolution
with a stride of <span class="math">\(1/r\)</span>.</p>
<p>Look at the paper:
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a>
by Shi et. al (2016) for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – factor to increase spatial resolution by</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, L, H_{in}, W_{in})\)</span> where <span class="math">\(L=C \times \text{upscale\_factor}^2\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = H_{in} \times \text{upscale\_factor}\)</span>
and <span class="math">\(W_{out} = W_{in} \times \text{upscale\_factor}\)</span></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pixel_shuffle</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsample">
<h4><span class="hidden-section">Upsample</span><a class="headerlink" href="#upsample" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Upsample">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Upsample</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p>
<p>The input data is assumed to be of the form
<cite>minibatch x channels x [optional depth] x [optional height] x width</cite>.
Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</p>
<p>The algorithms available for upsampling are nearest neighbor and linear,
bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor,
respectively.</p>
<p>One can either give a <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code> or the target output <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> to
calculate the output size. (You cannot give both, as it is ambiguous)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>(int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int],</strong> (<em>size</em>) – optional): output spatial sizes</p></li>
<li><p><strong>(float or Tuple[float] or Tuple[float, float] or</strong> (<em>scale_factor</em>) – Tuple[float, float, float], optional): multiplier for spatial size.
Has to match input size if it is a tuple.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – the upsampling algorithm: one of <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'linear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> and <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code></p></li>
<li><p><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the corner pixels of the input
and output tensors are aligned, and thus preserving the values at
those pixels. This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is
<code class="docutils literal notranslate"><span class="pre">'linear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, W_{in})\)</span>, <span class="math">\((N, C, H_{in}, W_{in})\)</span> or <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, W_{out})\)</span>, <span class="math">\((N, C, H_{out}, W_{out})\)</span>
or <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span>, where</p></li>
</ul>
</dd>
</dl>
<div class="math">
\[D_{out} = \left\lfloor D_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<div class="math">
\[H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, <cite>bicubic</cite>, and <cite>trilinear</cite>) don’t proportionally
align the output and input pixels, and thus the output values can depend
on the input size. This was the default behavior for these modes up to
version 0.3.1. Since then, the default behavior is
<code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>. See below for concrete examples on how this
affects the outputs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want downsampling/general resizing, you should use <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span>  <span class="c1"># align_corners=False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],</span>
<span class="go">          [ 1.5000,  1.7500,  2.2500,  2.5000],</span>
<span class="go">          [ 2.5000,  2.7500,  3.2500,  3.5000],</span>
<span class="go">          [ 3.0000,  3.2500,  3.7500,  4.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],</span>
<span class="go">          [ 1.6667,  2.0000,  2.3333,  2.6667],</span>
<span class="go">          [ 2.3333,  2.6667,  3.0000,  3.3333],</span>
<span class="go">          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Try scaling the same data in a larger tensor</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_3x3</span>
<span class="go">tensor([[[[ 1.,  2.,  0.],</span>
<span class="go">          [ 3.,  4.,  0.],</span>
<span class="go">          [ 0.,  0.,  0.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">)</span>  <span class="c1"># align_corners=False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Notice that values in top left corner are the same with the small input (except at boundary)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">input_3x3</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],</span>
<span class="go">          [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],</span>
<span class="go">          [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],</span>
<span class="go">          [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],</span>
<span class="go">          [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Notice that values in top left corner are now changed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">input_3x3</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],</span>
<span class="go">          [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],</span>
<span class="go">          [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],</span>
<span class="go">          [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],</span>
<span class="go">          [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],</span>
<span class="go">          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingnearest2d">
<h4><span class="hidden-section">UpsamplingNearest2d</span><a class="headerlink" href="#upsamplingnearest2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.UpsamplingNearest2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingNearest2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.UpsamplingNearest2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – output spatial sizes</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em><em>, </em><em>optional</em>) – multiplier for
spatial size.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</p></li>
</ul>
</dd>
</dl>
<div class="math">
\[H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 1.,  1.,  2.,  2.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.],</span>
<span class="go">          [ 3.,  3.,  4.,  4.]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingbilinear2d">
<h4><span class="hidden-section">UpsamplingBilinear2d</span><a class="headerlink" href="#upsamplingbilinear2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.UpsamplingBilinear2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingBilinear2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.UpsamplingBilinear2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D bilinear upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code>
as it’s constructor argument.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> is given, it is the output size of the image <cite>(h, w)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – output spatial sizes</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em><em>, </em><em>optional</em>) – multiplier for
spatial size.</p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class is deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code>. It is
equivalent to <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></p></li>
<li><p>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where</p></li>
</ul>
</dd>
</dl>
<div class="math">
\[H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<div class="math">
\[W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor

\]</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">tensor([[[[ 1.,  2.],</span>
<span class="go">          [ 3.,  4.]]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],</span>
<span class="go">          [ 1.6667,  2.0000,  2.3333,  2.6667],</span>
<span class="go">          [ 2.3333,  2.6667,  3.0000,  3.3333],</span>
<span class="go">          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dataparallel-layers-multi-gpu-distributed">
<h3>DataParallel layers (multi-GPU, distributed)<a class="headerlink" href="#dataparallel-layers-multi-gpu-distributed" title="Permalink to this headline">¶</a></h3>
<div class="section" id="dataparallel">
<h4><span class="hidden-section">DataParallel</span><a class="headerlink" href="#dataparallel" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.DataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">DataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.DataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements data parallelism at the module level.</p>
<p>This container parallelizes the application of the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> by
splitting the input across the specified devices by chunking in the batch
dimension (other objects will be copied once per device). In the forward
pass, the module is replicated on each device, and each replica handles a
portion of the input. During the backwards pass, gradients from each replica
are summed into the original module.</p>
<p>The batch size should be larger than the number of GPUs used.</p>
<p>See also: <span class="xref std std-ref">cuda-nn-dataparallel-instead</span></p>
<p>Arbitrary positional and keyword inputs are allowed to be passed into
DataParallel but some types are specially handled. tensors will be
<strong>scattered</strong> on dim specified (default 0). tuple, list and dict types will
be shallow copied. The other types will be shared among different threads
and can be corrupted if written to in the model’s forward pass.</p>
<p>The parallelized <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> must have its parameters and buffers on
<code class="docutils literal notranslate"><span class="pre">device_ids[0]</span></code> before running this <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a>
module.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In each forward, <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> is <strong>replicated</strong> on each device, so any
updates to the running module in <code class="docutils literal notranslate"><span class="pre">forward</span></code> will be lost. For example,
if <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> has a counter attribute that is incremented in each
<code class="docutils literal notranslate"><span class="pre">forward</span></code>, it will always stay at the initial value becasue the update
is done on the replicas which are destroyed after <code class="docutils literal notranslate"><span class="pre">forward</span></code>. However,
<a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a> guarantees that the replica on
<code class="docutils literal notranslate"><span class="pre">device[0]</span></code> will have its parameters and buffers sharing storage with
the base parallelized <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code>. So <strong>in-place</strong> updates to the
parameters or buffers on <code class="docutils literal notranslate"><span class="pre">device[0]</span></code> will be recorded. E.g.,
<a class="reference internal" href="#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a> and <a class="reference internal" href="#torch.nn.utils.spectral_norm" title="torch.nn.utils.spectral_norm"><code class="xref py py-func docutils literal notranslate"><span class="pre">spectral_norm()</span></code></a>
rely on this behavior to update the buffers.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Forward and backward hooks defined on <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and its submodules
will be invoked <code class="docutils literal notranslate"><span class="pre">len(device_ids)</span></code> times, each with inputs located on
a particular device. Particularly, the hooks are only guaranteed to be
executed in correct order with respect to operations on corresponding
devices. For example, it is not guaranteed that hooks set via
<a class="reference internal" href="#torch.nn.Module.register_forward_pre_hook" title="torch.nn.Module.register_forward_pre_hook"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_forward_pre_hook()</span></code></a> be executed before
<cite>all</cite> <code class="docutils literal notranslate"><span class="pre">len(device_ids)</span></code> <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> calls, but
that each such hook be executed before the corresponding
<a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> call of that device.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> returns a scalar (i.e., 0-dimensional tensor) in
<code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code>, this wrapper will return a vector of length equal to
number of devices used in data parallelism, containing the result from
each device.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is a subtlety in using the
<code class="docutils literal notranslate"><span class="pre">pack</span> <span class="pre">sequence</span> <span class="pre">-&gt;</span> <span class="pre">recurrent</span> <span class="pre">network</span> <span class="pre">-&gt;</span> <span class="pre">unpack</span> <span class="pre">sequence</span></code> pattern in a
<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> wrapped in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a>.
See <span class="xref std std-ref">pack-rnn-unpack-with-data-parallelism</span> section in FAQ for
details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – module to be parallelized</p></li>
<li><p><strong>device_ids</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – CUDA devices (default: all devices)</p></li>
<li><p><strong>output_device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – device location of output (default: device_ids[0])</p></li>
</ul>
</dd>
<dt class="field-even">Variables</dt>
<dd class="field-even"><p><strong>~DataParallel.module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to be parallelized</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>  <span class="c1"># input_var can be on any device, including CPU</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="distributeddataparallel">
<h4><span class="hidden-section">DistributedDataParallel</span><a class="headerlink" href="#distributeddataparallel" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.parallel.DistributedDataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.parallel.</code><code class="descname">DistributedDataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em>, <em>broadcast_buffers=True</em>, <em>process_group=None</em>, <em>bucket_cap_mb=25</em>, <em>check_reduction=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.parallel.DistributedDataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements distributed data parallelism that is based on
<code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> package at the module level.</p>
<p>This container parallelizes the application of the given module by
splitting the input across the specified devices by chunking in the batch
dimension. The module is replicated on each machine and each device, and
each such replica handles a portion of the input. During the backwards
pass, gradients from each node are averaged.</p>
<p>The batch size should be larger than the number of GPUs used locally.</p>
<p>See also: <a class="reference internal" href="index.html#distributed-basics"><span class="std std-ref">Basics</span></a> and <span class="xref std std-ref">cuda-nn-dataparallel-instead</span>.
The same constraints on input as in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code></a> apply.</p>
<p>Creation of this class requires that <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> to be already
initialized, by calling <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.init_process_group()</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> can be used in the following two ways:</p>
<ol class="arabic simple">
<li><p>Single-Process Multi-GPU</p></li>
</ol>
<p>In this case, a single process will be
spawned on each host/node and each process will operate on all the GPUs
of the node where it’s running. To use <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> in
this way, you can simply construct the model as the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># device_ids will include all GPU devices by default</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Multi-Process Single-GPU</p></li>
</ol>
<p>This is the highly recommended way to use <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>, with
multiple processes, each of which operates on a single GPU. This is
currently the fastest approach to do data parallel training using PyTorch
and applies to both single-node(multi-GPU) and multi-node data
parallel training. It is proven to be significantly faster than
<a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code></a> for single-node multi-GPU data
parallel training.</p>
<p>Here is how to use it: on each host with N GPUs, you should spawn up N
processes, while ensuring that each process individually works on a single GPU
from 0 to N-1. Therefore, it is your job to ensure that your training script
operates on a single given GPU by calling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
<p>where i is from 0 to N-1. In each process, you should refer the following
to construct this module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to spawn up multiple processes per node, you can use either
<code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.multiprocessing.spawn</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">nccl</span></code> backend is currently the fastest and
highly recommended backend to be used with Multi-Process Single-GPU
distributed training and this applies to both single-node and multi-node
distributed training</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This module also supports mixed-precision distributed training.
This means that your model can have different types of parameters such
as mixed types of fp16 and fp32, the gradient reduction on these
mixed types of parameters will just work fine.
Also note that <code class="docutils literal notranslate"><span class="pre">nccl</span></code> backend is currently the fastest and highly
recommended backend for fp16/fp32 mixed-precision training.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module works only with the <code class="docutils literal notranslate"><span class="pre">gloo</span></code> and <code class="docutils literal notranslate"><span class="pre">nccl</span></code> backends.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Constructor, forward method, and differentiation of the output (or a
function of the output of this module) is a distributed synchronization
point. Take that into account in case different processes might be
executing different code.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module assumes all parameters are registered in the model by the
time it is created. No parameters should be added nor removed later.
Same applies to buffers.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module assumes all parameters are registered in the model of each
distributed processes are in the same order. The module itself will
conduct gradient all-reduction following the reverse order of the
registered parameters of the model. In other words, it is users’
responsibility to ensure that each distributed process has the exact
same model and thus the exact same parameter registration order.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module assumes all buffers and gradients are dense.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module doesn’t work with <a class="reference internal" href="index.html#torch.autograd.grad" title="torch.autograd.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a> (i.e. it will
only work if gradients are to be accumulated in <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes of
parameters).</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you plan on using this module with a <code class="docutils literal notranslate"><span class="pre">nccl</span></code> backend or a <code class="docutils literal notranslate"><span class="pre">gloo</span></code>
backend (that uses Infiniband), together with a DataLoader that uses
multiple workers, please change the multiprocessing start method to
<code class="docutils literal notranslate"><span class="pre">forkserver</span></code> (Python 3 only) or <code class="docutils literal notranslate"><span class="pre">spawn</span></code>. Unfortunately
Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will
likely experience deadlocks if you don’t change this setting.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Forward and backward hooks defined on <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and its submodules
won’t be invoked anymore, unless the hooks are initialized in the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You should never try to change your model’s parameters after wrapping
up your model with DistributedDataParallel. In other words, when
wrapping up your model with DistributedDataParallel, the constructor of
DistributedDataParallel will register the additional gradient
reduction functions on all the parameters of the model itself at the
time of construction. If you change the model’s parameters after
the DistributedDataParallel construction, this is not supported and
unexpected behaviors can happen, since some parameters’ gradient
reduction functions might not get called.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parameters are never broadcast between processes. The module performs
an all-reduce step on gradients and assumes that they will be modified
by the optimizer in all processes in the same way. Buffers
(e.g. BatchNorm stats) are broadcast from the module in process of rank
0, to all other replicas in the system in every iteration.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – module to be parallelized</p></li>
<li><p><strong>device_ids</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – CUDA devices (default: all devices)</p></li>
<li><p><strong>output_device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – device location of output (default: device_ids[0])</p></li>
<li><p><strong>broadcast_buffers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – flag that enables syncing (broadcasting) buffers of
the module at beginning of the forward function.
(default: <code class="docutils literal notranslate"><span class="pre">True</span></code>)</p></li>
<li><p><strong>process_group</strong> – the process group to be used for distributed data
all-reduction. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the default process group, which
is created by <code class="docutils literal notranslate"><span class="pre">`torch.distributed.init_process_group`</span></code>,
will be used. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>bucket_cap_mb</strong> – DistributedDataParallel will bucket parameters into
multiple buckets so that gradient reduction of each
bucket can potentially overlap with backward computation.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">bucket_cap_mb</span></code> controls the bucket size in MegaBytes (MB)
(default: 25)</p></li>
<li><p><strong>check_reduction</strong> – when setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>, it enables DistributedDataParallel
to automatically check if the previous iteration’s
backward reductions were successfully issued at the
beginning of every iteration’s forward function.
You normally don’t need this option enabled unless you
are observing weird behaviors such as different ranks
are getting different gradients, which should not
happen if DistributedDataParallel is correctly used.
(default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
</ul>
</dd>
<dt class="field-even">Variables</dt>
<dd class="field-even"><p><strong>~DistributedDataParallel.module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to be parallelized</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pg</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="distributeddataparallelcpu">
<h4><span class="hidden-section">DistributedDataParallelCPU</span><a class="headerlink" href="#distributeddataparallelcpu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.parallel.DistributedDataParallelCPU">
<em class="property">class </em><code class="descclassname">torch.nn.parallel.</code><code class="descname">DistributedDataParallelCPU</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.parallel.DistributedDataParallelCPU" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements distributed data parallelism for CPU at the module level.</p>
<p>This module supports the <code class="docutils literal notranslate"><span class="pre">mpi</span></code> and <code class="docutils literal notranslate"><span class="pre">gloo</span></code> backends.</p>
<p>This container parallelizes the application of the given module by splitting
the input across the specified devices by chunking in the batch
dimension. The module is replicated on each machine, and each such replica
handles a portion of the input. During the backwards pass, gradients from
each node are averaged.</p>
<p>This module could be used in conjunction with the DistributedSampler,
(see <a class="reference internal" href="index.html#torch.utils.data.distributed.DistributedSampler" title="torch.utils.data.distributed.DistributedSampler"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedSampler</span></code></a>)
which will load a subset of the original dataset for each node with the same
batch size. So strong scaling should be configured like this:</p>
<p>n = 1, batch size = 12</p>
<p>n = 2, batch size = 64</p>
<p>n = 4, batch size = 32</p>
<p>n = 8, batch size = 16</p>
<p>Creation of this class requires the distributed package to be already
initialized in the process group mode
(see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.init_process_group()</span></code>).</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Constructor, forward method, and differentiation of the output (or a
function of the output of this module) is a distributed synchronization
point. Take that into account in case different node might be
executing different code.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module assumes all parameters are registered in the model by the
time it is created. No parameters should be added nor removed later.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module assumes all gradients are dense.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module doesn’t work with <a class="reference internal" href="index.html#torch.autograd.grad" title="torch.autograd.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a> (i.e. it will
only work if gradients are to be accumulated in <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes of
parameters).</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Forward and backward hooks defined on <code class="xref py py-attr docutils literal notranslate"><span class="pre">module</span></code> and its submodules
won’t be invoked anymore, unless the hooks are initialized in the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parameters are broadcast between nodes in the __init__() function. The
module performs an all-reduce step on gradients and assumes that they
will be modified by the optimizer in all nodes in the same way.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – module to be parallelized</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DistributedDataParallelCPU</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="utilities">
<h3>Utilities<a class="headerlink" href="#utilities" title="Permalink to this headline">¶</a></h3>
<div class="section" id="clip-grad-norm">
<h4><span class="hidden-section">clip_grad_norm_</span><a class="headerlink" href="#clip-grad-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.clip_grad_norm_">
<code class="descclassname">torch.nn.utils.</code><code class="descname">clip_grad_norm_</code><span class="sig-paren">(</span><em>parameters</em>, <em>max_norm</em>, <em>norm_type=2</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.clip_grad_norm_" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient norm of an iterable of parameters.</p>
<p>The norm is computed over all gradients together, as if they were
concatenated into a single vector. Gradients are modified in-place.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – max norm of the gradients</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – type of the used p-norm. Can be <code class="docutils literal notranslate"><span class="pre">'inf'</span></code> for
infinity norm.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Total norm of the parameters (viewed as a single vector).</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="clip-grad-value">
<h4><span class="hidden-section">clip_grad_value_</span><a class="headerlink" href="#clip-grad-value" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.clip_grad_value_">
<code class="descclassname">torch.nn.utils.</code><code class="descname">clip_grad_value_</code><span class="sig-paren">(</span><em>parameters</em>, <em>clip_value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.clip_grad_value_" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient of an iterable of parameters at specified value.</p>
<p>Gradients are modified in-place.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>] or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – an iterable of Tensors or a
single Tensor that will have gradients normalized</p></li>
<li><p><strong>clip_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – maximum allowed value of the gradients.
The gradients are clipped in the range
<span class="math">\(\left[\text{-clip\_value}, \text{clip\_value}\right]\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="parameters-to-vector">
<h4><span class="hidden-section">parameters_to_vector</span><a class="headerlink" href="#parameters-to-vector" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.parameters_to_vector">
<code class="descclassname">torch.nn.utils.</code><code class="descname">parameters_to_vector</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.parameters_to_vector" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert parameters to one vector</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterator of Tensors that are the
parameters of a model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The parameters represented by a single vector</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="vector-to-parameters">
<h4><span class="hidden-section">vector_to_parameters</span><a class="headerlink" href="#vector-to-parameters" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.vector_to_parameters">
<code class="descclassname">torch.nn.utils.</code><code class="descname">vector_to_parameters</code><span class="sig-paren">(</span><em>vec</em>, <em>parameters</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.vector_to_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert one vector to the parameters</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vec</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – a single vector represents the parameters of a model.</p></li>
<li><p><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterator of Tensors that are the
parameters of a model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="weight-norm">
<h4><span class="hidden-section">weight_norm</span><a class="headerlink" href="#weight-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.weight_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">weight_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies weight normalization to a parameter in the given module.</p>
<div class="math">
\[\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}

\]</div>
<p>Weight normalization is a reparameterization that decouples the magnitude
of a weight tensor from its direction. This replaces the parameter specified
by <code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">'weight'</span></code>) with two parameters: one specifying the magnitude
(e.g. <code class="docutils literal notranslate"><span class="pre">'weight_g'</span></code>) and one specifying the direction (e.g. <code class="docutils literal notranslate"><span class="pre">'weight_v'</span></code>).
Weight normalization is implemented via a hook that recomputes the weight
tensor from the magnitude and direction before every <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>
call.</p>
<p>By default, with <code class="docutils literal notranslate"><span class="pre">dim=0</span></code>, the norm is computed independently per output
channel/plane. To compute a norm over the entire weight tensor, use
<code class="docutils literal notranslate"><span class="pre">dim=None</span></code>.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – containing module</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – dimension over which to compute the norm</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The original module with the weight norm hook</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span>
<span class="go">Linear(in_features=20, out_features=40, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_g</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_v</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40, 20])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="remove-weight-norm">
<h4><span class="hidden-section">remove_weight_norm</span><a class="headerlink" href="#remove-weight-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.remove_weight_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">remove_weight_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.remove_weight_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the weight normalization reparameterization from a module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – containing module</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weight_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">remove_weight_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="spectral-norm">
<h4><span class="hidden-section">spectral_norm</span><a class="headerlink" href="#spectral-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.spectral_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">spectral_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em>, <em>n_power_iterations=1</em>, <em>eps=1e-12</em>, <em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.spectral_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies spectral normalization to a parameter in the given module.</p>
<div class="math">
\[\mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})},
\sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}

\]</div>
<p>Spectral normalization stabilizes the training of discriminators (critics)
in Generative Adversarial Networks (GANs) by rescaling the weight tensor
with spectral norm <span class="math">\(\sigma\)</span> of the weight matrix calculated using
power iteration method. If the dimension of the weight tensor is greater
than 2, it is reshaped to 2D in power iteration method to get spectral
norm. This is implemented via a hook that calculates spectral norm and
rescales weight before every <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> call.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1802.05957">Spectral Normalization for Generative Adversarial Networks</a> .</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) – containing module</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</p></li>
<li><p><strong>n_power_iterations</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – number of power iterations to
calculate spectral norm</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – epsilon for numerical stability in
calculating norms</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – dimension corresponding to number of outputs,
the default is <code class="docutils literal notranslate"><span class="pre">0</span></code>, except for modules that are instances of
ConvTranspose{1,2,3}d, when it is <code class="docutils literal notranslate"><span class="pre">1</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The original module with the spectral norm hook</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span>
<span class="go">Linear(in_features=20, out_features=40, bias=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">weight_u</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([40])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="remove-spectral-norm">
<h4><span class="hidden-section">remove_spectral_norm</span><a class="headerlink" href="#remove-spectral-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.remove_spectral_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">remove_spectral_norm</code><span class="sig-paren">(</span><em>module</em>, <em>name='weight'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.remove_spectral_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the spectral normalization reparameterization from a module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – containing module</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – name of weight parameter</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">remove_spectral_norm</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="packedsequence">
<h4><span class="hidden-section">PackedSequence</span><a class="headerlink" href="#packedsequence" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.rnn.PackedSequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">PackedSequence</code><span class="sig-paren">(</span><em>data</em>, <em>batch_sizes=None</em>, <em>sorted_indices=None</em>, <em>unsorted_indices=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.rnn.PackedSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds the data and list of <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_sizes</span></code> of a packed sequence.</p>
<p>All RNN modules accept packed sequences as inputs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Instances of this class should never be created manually. They are meant
to be instantiated by functions like <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
<p>Batch sizes represent the number elements at each sequence step in
the batch, not the varying sequence lengths passed to
<a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.  For instance, given data <code class="docutils literal notranslate"><span class="pre">abc</span></code> and <code class="docutils literal notranslate"><span class="pre">x</span></code>
the <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> would contain data <code class="docutils literal notranslate"><span class="pre">axbc</span></code> with
<code class="docutils literal notranslate"><span class="pre">batch_sizes=[2,1,1]</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>~PackedSequence.data</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor containing packed sequence</p></li>
<li><p><strong>~PackedSequence.batch_sizes</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor of integers holding
information about the batch size at each sequence step</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="pack-padded-sequence">
<h4><span class="hidden-section">pack_padded_sequence</span><a class="headerlink" href="#pack-padded-sequence" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.rnn.pack_padded_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pack_padded_sequence</code><span class="sig-paren">(</span><em>input</em>, <em>lengths</em>, <em>batch_first=False</em>, <em>enforce_sorted=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.rnn.pack_padded_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs a Tensor containing padded sequences of variable length.</p>
<p>Input can be of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> where <cite>T</cite> is the length of the longest sequence
(equal to <code class="docutils literal notranslate"><span class="pre">lengths[0]</span></code>), <cite>B</cite> is the batch size, and <cite>*</cite> is any number of
dimensions (including 0). If <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> is True <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> inputs are
expected.</p>
<p>For unsorted sequences, use <cite>enforce_sorted = False</cite>. If <code class="docutils literal notranslate"><span class="pre">enforce_sorted</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the sequences should be sorted by length in a decreasing order, i.e.
<code class="docutils literal notranslate"><span class="pre">input[:,0]</span></code> should be the longest sequence, and <code class="docutils literal notranslate"><span class="pre">input[:,B-1]</span></code> the shortest
one. <cite>enforce_sorted = True</cite> is only necessary for ONNX export.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function accepts any input that has at least two dimensions. You
can apply it to pack the labels, and use the output of the RNN with
them to compute the loss directly. A Tensor can be retrieved from
a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object by accessing its <code class="docutils literal notranslate"><span class="pre">.data</span></code> attribute.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – padded batch of variable length sequences.</p></li>
<li><p><strong>lengths</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – list of sequences lengths of each batch element.</p></li>
<li><p><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input is expected in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code>
format.</p></li>
<li><p><strong>enforce_sorted</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input is expected to
contain sequences sorted by length in a decreasing order. If
<code class="docutils literal notranslate"><span class="pre">False</span></code>, this condition is not checked. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="pad-packed-sequence">
<h4><span class="hidden-section">pad_packed_sequence</span><a class="headerlink" href="#pad-packed-sequence" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.rnn.pad_packed_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pad_packed_sequence</code><span class="sig-paren">(</span><em>sequence</em>, <em>batch_first=False</em>, <em>padding_value=0.0</em>, <em>total_length=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.rnn.pad_packed_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads a packed batch of variable length sequences.</p>
<p>It is an inverse operation to <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal notranslate"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
<p>The returned Tensor’s data will be of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code>, where <cite>T</cite> is the length
of the longest sequence and <cite>B</cite> is the batch size. If <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> is True,
the data will be transposed into <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> format.</p>
<p>Batch elements will be ordered decreasingly by their length.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code> is useful to implement the
<code class="docutils literal notranslate"><span class="pre">pack</span> <span class="pre">sequence</span> <span class="pre">-&gt;</span> <span class="pre">recurrent</span> <span class="pre">network</span> <span class="pre">-&gt;</span> <span class="pre">unpack</span> <span class="pre">sequence</span></code> pattern in a
<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> wrapped in <a class="reference internal" href="#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataParallel</span></code></a>.
See <span class="xref std std-ref">this FAQ section</span> for
details.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequence</strong> (<em>PackedSequence</em>) – batch to pad</p></li>
<li><p><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the output will be in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code>
format.</p></li>
<li><p><strong>padding_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – values for padded elements.</p></li>
<li><p><strong>total_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – if not <code class="docutils literal notranslate"><span class="pre">None</span></code>, the output will be padded to
have length <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code>. This method will throw <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ValueError</span></code></a>
if <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_length</span></code> is less than the max sequence length in
<code class="xref py py-attr docutils literal notranslate"><span class="pre">sequence</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple of Tensor containing the padded sequence, and a Tensor
containing the list of lengths of each sequence in the batch.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="pad-sequence">
<h4><span class="hidden-section">pad_sequence</span><a class="headerlink" href="#pad-sequence" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.rnn.pad_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pad_sequence</code><span class="sig-paren">(</span><em>sequences</em>, <em>batch_first=False</em>, <em>padding_value=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.rnn.pad_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Pad a list of variable length Tensors with <code class="docutils literal notranslate"><span class="pre">padding_value</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">pad_sequence</span></code> stacks a list of Tensors along a new dimension,
and pads them to equal length. For example, if the input is list of
sequences with size <code class="docutils literal notranslate"><span class="pre">L</span> <span class="pre">x</span> <span class="pre">*</span></code> and if batch_first is False, and <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code>
otherwise.</p>
<p><cite>B</cite> is batch size. It is equal to the number of elements in <code class="docutils literal notranslate"><span class="pre">sequences</span></code>.
<cite>T</cite> is length of the longest sequence.
<cite>L</cite> is length of the sequence.
<cite>*</cite> is any number of trailing dimensions, including none.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">pad_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad_sequence</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([25, 3, 300])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function returns a Tensor of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> or <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code>
where <cite>T</cite> is the length of the longest sequence. This function assumes
trailing dimensions and type of all the Tensors in sequences are same.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequences</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – list of variable length sequences.</p></li>
<li><p><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – output will be in <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> if True, or in
<code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> otherwise</p></li>
<li><p><strong>padding_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – value for padded elements. Default: 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor of size <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">x</span> <span class="pre">B</span> <span class="pre">x</span> <span class="pre">*</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_first</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Tensor of size <code class="docutils literal notranslate"><span class="pre">B</span> <span class="pre">x</span> <span class="pre">T</span> <span class="pre">x</span> <span class="pre">*</span></code> otherwise</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="pack-sequence">
<h4><span class="hidden-section">pack_sequence</span><a class="headerlink" href="#pack-sequence" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.rnn.pack_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pack_sequence</code><span class="sig-paren">(</span><em>sequences</em>, <em>enforce_sorted=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.rnn.pack_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs a list of variable length Tensors</p>
<p><code class="docutils literal notranslate"><span class="pre">sequences</span></code> should be a list of Tensors of size <code class="docutils literal notranslate"><span class="pre">L</span> <span class="pre">x</span> <span class="pre">*</span></code>, where <cite>L</cite> is
the length of a sequence and <cite>*</cite> is any number of trailing dimensions,
including zero.</p>
<p>For unsorted sequences, use <cite>enforce_sorted = False</cite>. If <code class="docutils literal notranslate"><span class="pre">enforce_sorted</span></code>
is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the sequences should be sorted in the order of decreasing length.
<code class="docutils literal notranslate"><span class="pre">enforce_sorted</span> <span class="pre">=</span> <span class="pre">True</span></code> is only necessary for ONNX export.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="k">import</span> <span class="n">pack_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pack_sequence</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
<span class="go">PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequences</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – A list of sequences of decreasing length.</p></li>
<li><p><strong>enforce_sorted</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, checks that the input
contains sequences sorted by length in a decreasing order. If
<code class="docutils literal notranslate"><span class="pre">False</span></code>, this condition is not checked. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a> object</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-functional">
<h2>torch.nn.functional<a class="headerlink" href="#torch-nn-functional" title="Permalink to this headline">¶</a></h2>
<div class="section" id="convolution-functions">
<h3>Convolution functions<a class="headerlink" href="#convolution-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id30">
<h4><span class="hidden-section">conv1d</span><a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or
a one-element tuple <cite>(sW,)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – implicit paddings on both sides of the input. Can be a
single number or a one-element tuple <cite>(padW,)</cite>. Default: 0</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a one-element tuple <cite>(dW,)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by
the number of groups. Default: 1</p></li>
<li><p><strong>padding_mode</strong> – the type of paddings applied to both sided can be: <cite>zeros</cite> or <cite>circular</cite>. Default: <cite>zeros</cite></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id31">
<h4><span class="hidden-section">conv2d</span><a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias tensor of shape <span class="math">\((\text{out\_channels})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – implicit paddings on both sides of the input. Can be a
single number or a tuple <cite>(padH, padW)</cite>. Default: 0</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dH, dW)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>padding_mode</strong> – the type of paddings applied to both sided can be: <cite>zeros</cite> or <cite>circular</cite>. Default: <cite>zeros</cite></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id32">
<h4><span class="hidden-section">conv3d</span><a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iT , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kT , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias tensor of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – implicit paddings on both sides of the input. Can be a
single number or a tuple <cite>(padT, padH, padW)</cite>. Default: 0</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by
the number of groups. Default: 1</p></li>
<li><p><strong>padding_mode</strong> – the type of paddings applied to both sided can be: <cite>zeros</cite> or <cite>circular</cite>. Default: <cite>zeros</cite></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose1d">
<h4><span class="hidden-section">conv_transpose1d</span><a class="headerlink" href="#conv-transpose1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input signal
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sW,)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padW,)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padW)</span></code>. Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dW,)</span></code>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose2d">
<h4><span class="hidden-section">conv_transpose2d</span><a class="headerlink" href="#conv-transpose2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sH,</span> <span class="pre">sW)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padH,</span> <span class="pre">padW)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padH,</span> <span class="pre">out_padW)</span></code>.
Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dH,</span> <span class="pre">dW)</span></code>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose3d">
<h4><span class="hidden-section">conv_transpose3d</span><a class="headerlink" href="#conv-transpose3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iT , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kT , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sT,</span> <span class="pre">sH,</span> <span class="pre">sW)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padT,</span> <span class="pre">padH,</span> <span class="pre">padW)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(out_padT,</span> <span class="pre">out_padH,</span> <span class="pre">out_padW)</span></code>. Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id33">
<h4><span class="hidden-section">unfold</span><a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.unfold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">unfold</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts sliding local blocks from an batched input tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently, only 4-D input tensors (batched image-like tensors) are
supported.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>More than one element of the unfolded tensor may refer to a single
memory location. As a result, in-place operations (especially ones that
are vectorized) may result in incorrect behavior. If you need to write
to the tensor, please clone it first.</p>
</div>
<p>See <a class="reference internal" href="#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Unfold</span></code></a> for details</p>
</dd></dl>

</div>
<div class="section" id="id34">
<h4><span class="hidden-section">fold</span><a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.fold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">fold</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines an array of sliding local blocks into a large containing
tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently, only 4-D output tensors (batched image-like tensors) are
supported.</p>
</div>
<p>See <a class="reference internal" href="#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Fold</span></code></a> for details</p>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-functions">
<h3>Pooling functions<a class="headerlink" href="#pooling-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="avg-pool1d">
<h4><span class="hidden-section">avg_pool1d</span><a class="headerlink" href="#avg-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool1d" title="torch.nn.AvgPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool1d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>kernel_size</strong> – the size of the window. Can be a single number or a
tuple <cite>(kW,)</cite></p></li>
<li><p><strong>stride</strong> – the stride of the window. Can be a single number or a tuple
<cite>(sW,)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padW,)</cite>. Default: 0</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the
output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[ 2.,  4.,  6.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avg-pool2d">
<h4><span class="hidden-section">avg_pool2d</span><a class="headerlink" href="#avg-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 2D average-pooling operation in <span class="math">\(kH \times kW\)</span> regions by step size
<span class="math">\(sH \times sW\)</span> steps. The number of output features is equal to the number of
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool2d" title="torch.nn.AvgPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool2d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor <span class="math">\((\text{minibatch} , \text{in\_channels} , iH , iW)\)</span></p></li>
<li><p><strong>kernel_size</strong> – size of the pooling region. Can be a single number or a
tuple <cite>(kH, kW)</cite></p></li>
<li><p><strong>stride</strong> – stride of the pooling operation. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padH, padW)</cite>. Default: 0</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="avg-pool3d">
<h4><span class="hidden-section">avg_pool3d</span><a class="headerlink" href="#avg-pool3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 3D average-pooling operation in <span class="math">\(kT \times kH \times kW\)</span> regions by step
size <span class="math">\(sT \times sH \times sW\)</span> steps. The number of output features is equal to
<span class="math">\(\lfloor\frac{\text{input planes}}{sT}\rfloor\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool3d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor <span class="math">\((\text{minibatch} , \text{in\_channels} , iT \times iH , iW)\)</span></p></li>
<li><p><strong>kernel_size</strong> – size of the pooling region. Can be a single number or a
tuple <cite>(kT, kH, kW)</cite></p></li>
<li><p><strong>stride</strong> – stride of the pooling operation. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padT, padH, padW)</cite>, Default: 0</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape</p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="max-pool1d">
<h4><span class="hidden-section">max_pool1d</span><a class="headerlink" href="#max-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool1d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool2d">
<h4><span class="hidden-section">max_pool2d</span><a class="headerlink" href="#max-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool2d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool3d">
<h4><span class="hidden-section">max_pool3d</span><a class="headerlink" href="#max-pool3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool3d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool1d">
<h4><span class="hidden-section">max_unpool1d</span><a class="headerlink" href="#max-unpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_unpool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool1d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_unpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool2d">
<h4><span class="hidden-section">max_unpool2d</span><a class="headerlink" href="#max-unpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_unpool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool2d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_unpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool3d">
<h4><span class="hidden-section">max_unpool3d</span><a class="headerlink" href="#max-unpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_unpool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool3d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_unpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool1d">
<h4><span class="hidden-section">lp_pool1d</span><a class="headerlink" href="#lp-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.lp_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.lp_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <cite>p</cite> is
zero, the gradient is set to zero as well.</p>
<p>See <a class="reference internal" href="#torch.nn.LPPool1d" title="torch.nn.LPPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">LPPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool2d">
<h4><span class="hidden-section">lp_pool2d</span><a class="headerlink" href="#lp-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.lp_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.lp_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <cite>p</cite> is
zero, the gradient is set to zero as well.</p>
<p>See <a class="reference internal" href="#torch.nn.LPPool2d" title="torch.nn.LPPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">LPPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool1d">
<h4><span class="hidden-section">adaptive_max_pool1d</span><a class="headerlink" href="#adaptive-max-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool1d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool1d" title="torch.nn.AdaptiveMaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool1d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size (single integer)</p></li>
<li><p><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool2d">
<h4><span class="hidden-section">adaptive_max_pool2d</span><a class="headerlink" href="#adaptive-max-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool2d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool2d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size (single integer or
double-integer tuple)</p></li>
<li><p><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool3d">
<h4><span class="hidden-section">adaptive_max_pool3d</span><a class="headerlink" href="#adaptive-max-pool3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool3d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool3d" title="torch.nn.AdaptiveMaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool3d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size (single integer or
triple-integer tuple)</p></li>
<li><p><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool1d">
<h4><span class="hidden-section">adaptive_avg_pool1d</span><a class="headerlink" href="#adaptive-avg-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool1d" title="torch.nn.AdaptiveAvgPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool1d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size (single integer)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool2d">
<h4><span class="hidden-section">adaptive_avg_pool2d</span><a class="headerlink" href="#adaptive-avg-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool2d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size (single integer or
double-integer tuple)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool3d">
<h4><span class="hidden-section">adaptive_avg_pool3d</span><a class="headerlink" href="#adaptive-avg-pool3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool3d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size (single integer or
triple-integer tuple)</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activation-functions">
<h3>Non-linear activation functions<a class="headerlink" href="#non-linear-activation-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id35">
<h4><span class="hidden-section">threshold</span><a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.threshold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor.</p>
<p>See <a class="reference internal" href="#torch.nn.Threshold" title="torch.nn.Threshold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Threshold</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.threshold_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold_</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.threshold_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.threshold" title="torch.nn.functional.threshold"><code class="xref py py-func docutils literal notranslate"><span class="pre">threshold()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id36">
<h4><span class="hidden-section">relu</span><a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise. See
<a class="reference internal" href="#torch.nn.ReLU" title="torch.nn.ReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.relu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu_</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.relu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.relu" title="torch.nn.functional.relu"><code class="xref py py-func docutils literal notranslate"><span class="pre">relu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id37">
<h4><span class="hidden-section">hardtanh</span><a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.hardtanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise. See <a class="reference internal" href="#torch.nn.Hardtanh" title="torch.nn.Hardtanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardtanh</span></code></a> for more
details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.hardtanh_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh_</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.hardtanh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardtanh()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id38">
<h4><span class="hidden-section">relu6</span><a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.relu6">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu6</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.relu6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(\text{ReLU6}(x) = \min(\max(0,x), 6)\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.ReLU6" title="torch.nn.ReLU6"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU6</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id39">
<h4><span class="hidden-section">elu</span><a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.elu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.elu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ELU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.elu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu_</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.elu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.elu" title="torch.nn.functional.elu"><code class="xref py py-func docutils literal notranslate"><span class="pre">elu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id40">
<h4><span class="hidden-section">selu</span><a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.selu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">selu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\)</span>,
with <span class="math">\(\alpha=1.6732632423543772848170429916717\)</span> and
<span class="math">\(scale=1.0507009873554804934193349852946\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.SELU" title="torch.nn.SELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">SELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id41">
<h4><span class="hidden-section">celu</span><a class="headerlink" href="#id41" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.celu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">celu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.celu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.CELU" title="torch.nn.CELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">CELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="leaky-relu">
<h4><span class="hidden-section">leaky_relu</span><a class="headerlink" href="#leaky-relu" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.leaky_relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.leaky_relu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu_</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.leaky_relu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.leaky_relu" title="torch.nn.functional.leaky_relu"><code class="xref py py-func docutils literal notranslate"><span class="pre">leaky_relu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="id42">
<h4><span class="hidden-section">prelu</span><a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.prelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">prelu</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.prelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise the function
<span class="math">\(\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)\)</span> where weight is a
learnable parameter.</p>
<p>See <a class="reference internal" href="#torch.nn.PReLU" title="torch.nn.PReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">PReLU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id43">
<h4><span class="hidden-section">rrelu</span><a class="headerlink" href="#id43" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.rrelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.rrelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomized leaky ReLU.</p>
<p>See <a class="reference internal" href="#torch.nn.RReLU" title="torch.nn.RReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">RReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.rrelu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu_</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.rrelu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.rrelu" title="torch.nn.functional.rrelu"><code class="xref py py-func docutils literal notranslate"><span class="pre">rrelu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="glu">
<h4><span class="hidden-section">glu</span><a class="headerlink" href="#glu" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.glu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">glu</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.glu" title="Permalink to this definition">¶</a></dt>
<dd><p>The gated linear unit. Computes:</p>
<div class="math">
\[\text{GLU}(a, b) = a \otimes \sigma(b)

\]</div>
<p>where <cite>input</cite> is split in half along <cite>dim</cite> to form <cite>a</cite> and <cite>b</cite>, <span class="math">\(\sigma\)</span>
is the sigmoid function and <span class="math">\(\otimes\)</span> is the element-wise product between matrices.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1612.08083">Language Modeling with Gated Convolutional Networks</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension on which to split the input. Default: -1</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="id44">
<h4><span class="hidden-section">logsigmoid</span><a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.logsigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">logsigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.logsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math">\(\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.LogSigmoid" title="torch.nn.LogSigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogSigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id45">
<h4><span class="hidden-section">hardshrink</span><a class="headerlink" href="#id45" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.hardshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hard shrinkage function element-wise</p>
<p>See <a class="reference internal" href="#torch.nn.Hardshrink" title="torch.nn.Hardshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id46">
<h4><span class="hidden-section">tanhshrink</span><a class="headerlink" href="#id46" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.tanhshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanhshrink</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(\text{Tanhshrink}(x) = x - \text{Tanh}(x)\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Tanhshrink" title="torch.nn.Tanhshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanhshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id47">
<h4><span class="hidden-section">softsign</span><a class="headerlink" href="#id47" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softsign">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softsign</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, the function <span class="math">\(\text{SoftSign}(x) = \frac{x}{1 + |x|}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Softsign" title="torch.nn.Softsign"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softsign</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id48">
<h4><span class="hidden-section">softplus</span><a class="headerlink" href="#id48" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softplus">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softplus</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softplus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id49">
<h4><span class="hidden-section">softmin</span><a class="headerlink" href="#id49" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softmin">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmin</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmin function.</p>
<p>Note that <span class="math">\(\text{Softmin}(x) = \text{Softmax}(-x)\)</span>. See softmax definition for mathematical formula.</p>
<p>See <a class="reference internal" href="#torch.nn.Softmin" title="torch.nn.Softmin"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmin</span></code></a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmin will be computed (so every slice
along dim will sum to 1).</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="id50">
<h4><span class="hidden-section">softmax</span><a class="headerlink" href="#id50" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax function.</p>
<p>Softmax is defined as:</p>
<p><span class="math">\(\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}\)</span></p>
<p>It is applied to all slices along dim, and will re-scale them so that the elements
lie in the range <cite>[0, 1]</cite> and sum to 1.</p>
<p>See <a class="reference internal" href="#torch.nn.Softmax" title="torch.nn.Softmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmax</span></code></a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmax will be computed.</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function doesn’t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use log_softmax instead (it’s faster and has better numerical properties).</p>
</div>
</dd></dl>

</div>
<div class="section" id="id51">
<h4><span class="hidden-section">softshrink</span><a class="headerlink" href="#id51" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise</p>
<p>See <a class="reference internal" href="#torch.nn.Softshrink" title="torch.nn.Softshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="gumbel-softmax">
<h4><span class="hidden-section">gumbel_softmax</span><a class="headerlink" href="#gumbel-softmax" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.gumbel_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">gumbel_softmax</code><span class="sig-paren">(</span><em>logits</em>, <em>tau=1</em>, <em>hard=False</em>, <em>eps=1e-10</em>, <em>dim=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.gumbel_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples from the <a class="reference external" href="https://arxiv.org/abs/1611.00712https://arxiv.org/abs/1611.01144">Gumbel-Softmax distribution</a> and optionally discretizes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> – <cite>[…, num_features]</cite> unnormalized log probabilities</p></li>
<li><p><strong>tau</strong> – non-negative scalar temperature</p></li>
<li><p><strong>hard</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the returned samples will be discretized as one-hot vectors,
but will be differentiated as if it is the soft sample in autograd</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmax will be computed. Default: -1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Sampled tensor of same shape as <cite>logits</cite> from the Gumbel-Softmax distribution.
If <code class="docutils literal notranslate"><span class="pre">hard=True</span></code>, the returned samples will be one-hot, otherwise they will
be probability distributions that sum to 1 across <cite>dim</cite>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is here for legacy reasons, may be removed from nn.Functional in the future.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The main trick for <cite>hard</cite> is to do  <cite>y_hard - y_soft.detach() + y_soft</cite></p>
<p>It achieves two things:
- makes the output value exactly one-hot
(since we add then subtract y_soft value)
- makes the gradient equal to y_soft gradient
(since we strip all other gradients)</p>
</div>
<dl>
<dt>Examples::</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Sample soft categorical using reparametrization trick:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Sample hard categorical using &quot;Straight-through&quot; trick:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="log-softmax">
<h4><span class="hidden-section">log_softmax</span><a class="headerlink" href="#log-softmax" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.log_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">log_softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax followed by a logarithm.</p>
<p>While mathematically equivalent to log(softmax(x)), doing these two
operations separately is slower, and numerically unstable. This function
uses an alternative formulation to compute the output and gradient correctly.</p>
<p>See <a class="reference internal" href="#torch.nn.LogSoftmax" title="torch.nn.LogSoftmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogSoftmax</span></code></a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which log_softmax will be computed.</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="id52">
<h4><span class="hidden-section">tanh</span><a class="headerlink" href="#id52" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.tanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Tanh" title="torch.nn.Tanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanh</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="id53">
<h4><span class="hidden-section">sigmoid</span><a class="headerlink" href="#id53" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.sigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}\)</span></p>
<p>See <a class="reference internal" href="#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
</div>
<div class="section" id="normalization-functions">
<h3>Normalization functions<a class="headerlink" href="#normalization-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="batch-norm">
<h4><span class="hidden-section">batch_norm</span><a class="headerlink" href="#batch-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.batch_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">batch_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean</em>, <em>running_var</em>, <em>weight=None</em>, <em>bias=None</em>, <em>training=False</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.batch_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization for each channel across a batch of data.</p>
<p>See <a class="reference internal" href="#torch.nn.BatchNorm1d" title="torch.nn.BatchNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm1d</span></code></a>, <a class="reference internal" href="#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a>,
<a class="reference internal" href="#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="instance-norm">
<h4><span class="hidden-section">instance_norm</span><a class="headerlink" href="#instance-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.instance_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">instance_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean=None</em>, <em>running_var=None</em>, <em>weight=None</em>, <em>bias=None</em>, <em>use_input_stats=True</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.instance_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization for each channel in each data sample in a
batch.</p>
<p>See <a class="reference internal" href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a>, <a class="reference internal" href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a>,
<a class="reference internal" href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="layer-norm">
<h4><span class="hidden-section">layer_norm</span><a class="headerlink" href="#layer-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.layer_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">layer_norm</code><span class="sig-paren">(</span><em>input</em>, <em>normalized_shape</em>, <em>weight=None</em>, <em>bias=None</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.layer_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization for last certain number of dimensions.</p>
<p>See <a class="reference internal" href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="local-response-norm">
<h4><span class="hidden-section">local_response_norm</span><a class="headerlink" href="#local-response-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.local_response_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">local_response_norm</code><span class="sig-paren">(</span><em>input</em>, <em>size</em>, <em>alpha=0.0001</em>, <em>beta=0.75</em>, <em>k=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.local_response_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies local response normalization over an input signal composed of
several input planes, where channels occupy the second dimension.
Applies normalization across channels.</p>
<p>See <a class="reference internal" href="#torch.nn.LocalResponseNorm" title="torch.nn.LocalResponseNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocalResponseNorm</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="normalize">
<h4><span class="hidden-section">normalize</span><a class="headerlink" href="#normalize" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.normalize">
<code class="descclassname">torch.nn.functional.</code><code class="descname">normalize</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em>, <em>dim=1</em>, <em>eps=1e-12</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs <span class="math">\(L_p\)</span> normalization of inputs over specified dimension.</p>
<p>For a tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of sizes <span class="math">\((n_0, ..., n_{dim}, ..., n_k)\)</span>, each
<span class="math">\(n_{dim}\)</span> -element vector <span class="math">\(v\)</span> along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is transformed as</p>
<div class="math">
\[v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.

\]</div>
<p>With the default arguments it uses the Euclidean norm over vectors along dimension <span class="math">\(1\)</span> for normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of any shape</p></li>
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the exponent value in the norm formulation. Default: 2</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce. Default: 1</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – small value to avoid division by zero. Default: 1e-12</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is used, this
operation won’t be differentiable.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="linear-functions">
<h3>Linear functions<a class="headerlink" href="#linear-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id54">
<h4><span class="hidden-section">linear</span><a class="headerlink" href="#id54" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.linear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">linear</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math">\(y = xA^T + b\)</span>.</p>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li><p>Input: <span class="math">\((N, *, in\_features)\)</span> where <cite>*</cite> means any number of
additional dimensions</p></li>
<li><p>Weight: <span class="math">\((out\_features, in\_features)\)</span></p></li>
<li><p>Bias: <span class="math">\((out\_features)\)</span></p></li>
<li><p>Output: <span class="math">\((N, *, out\_features)\)</span></p></li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="id55">
<h4><span class="hidden-section">bilinear</span><a class="headerlink" href="#id55" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">bilinear</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.bilinear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="dropout-functions">
<h3>Dropout functions<a class="headerlink" href="#dropout-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id56">
<h4><span class="hidden-section">dropout</span><a class="headerlink" href="#id56" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli
distribution.</p>
<p>See <a class="reference internal" href="#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of an element to be zeroed. Default: 0.5</p></li>
<li><p><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="alpha-dropout">
<h4><span class="hidden-section">alpha_dropout</span><a class="headerlink" href="#alpha-dropout" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.alpha_dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">alpha_dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.alpha_dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies alpha dropout to the input.</p>
<p>See <a class="reference internal" href="#torch.nn.AlphaDropout" title="torch.nn.AlphaDropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlphaDropout</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="id57">
<h4><span class="hidden-section">dropout2d</span><a class="headerlink" href="#id57" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.dropout2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout2d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 2D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 2D tensor <span class="math">\(\text{input}[i, j]\)</span>) of the input tensor).
Each channel will be zeroed out independently on every forward call with
probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>See <a class="reference internal" href="#torch.nn.Dropout2d" title="torch.nn.Dropout2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout2d</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of a channel to be zeroed. Default: 0.5</p></li>
<li><p><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="id58">
<h4><span class="hidden-section">dropout3d</span><a class="headerlink" href="#id58" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.dropout3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout3d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 3D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 3D tensor <span class="math">\(\text{input}[i, j]\)</span>) of the input tensor).
Each channel will be zeroed out independently on every forward call with
probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>See <a class="reference internal" href="#torch.nn.Dropout3d" title="torch.nn.Dropout3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout3d</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of a channel to be zeroed. Default: 0.5</p></li>
<li><p><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="sparse-functions">
<h3>Sparse functions<a class="headerlink" href="#sparse-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id59">
<h4><span class="hidden-section">embedding</span><a class="headerlink" href="#id59" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.embedding">
<code class="descclassname">torch.nn.functional.</code><code class="descname">embedding</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that looks up embeddings in a fixed dictionary and size.</p>
<p>This module is often used to retrieve word embeddings using indices.
The input to the module is a list of indices, and the embedding matrix,
and the output is the corresponding word embeddings.</p>
<p>See <a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LongTensor</em>) – Tensor containing indices into the embedding matrix</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The embedding matrix with number of rows equal to the maximum possible index + 1,
and number of columns equal to the embedding size</p></li>
<li><p><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – If given, pads the output with the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code>
(initialized to zeros) whenever it encounters the index.</p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.
Note: this will modify <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> in-place.</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> will be a sparse tensor. See Notes under
<a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details regarding sparse gradients.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: LongTensor of arbitrary shape containing the indices to extract</p></li>
<li><dl class="simple">
<dt>Weight: Embedding matrix of floating point type with shape <cite>(V, embedding_dim)</cite>,</dt><dd><p>where V = maximum index + 1 and embedding_dim = the embedding size</p>
</dd>
</dl>
</li>
<li><p>Output: <cite>(*, embedding_dim)</cite>, where <cite>*</cite> is the input shape</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># an embedding matrix containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">)</span>
<span class="go">tensor([[[ 0.8490,  0.9625,  0.6753],</span>
<span class="go">         [ 0.9666,  0.7761,  0.6108],</span>
<span class="go">         [ 0.6246,  0.9751,  0.3618],</span>
<span class="go">         [ 0.4161,  0.2419,  0.7383]],</span>

<span class="go">        [[ 0.6246,  0.9751,  0.3618],</span>
<span class="go">         [ 0.0237,  0.7794,  0.0528],</span>
<span class="go">         [ 0.9666,  0.7761,  0.6108],</span>
<span class="go">         [ 0.3385,  0.8612,  0.1867]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.5609,  0.5384,  0.8720],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.6262,  0.2438,  0.7471]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="embedding-bag">
<h4><span class="hidden-section">embedding_bag</span><a class="headerlink" href="#embedding-bag" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.embedding_bag">
<code class="descclassname">torch.nn.functional.</code><code class="descname">embedding_bag</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>offsets=None</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.embedding_bag" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sums, means or maxes of <cite>bags</cite> of embeddings, without instantiating the
intermediate embeddings.</p>
<p>See <a class="reference internal" href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.EmbeddingBag</span></code></a> for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LongTensor</em>) – Tensor containing bags of indices into the embedding matrix</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The embedding matrix with number of rows equal to the maximum possible index + 1,
and number of columns equal to the embedding size</p></li>
<li><p><strong>offsets</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) – Only used when <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D. <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> determines
the starting index position of each bag (sequence) in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.
Note: this will modify <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> in-place.</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The <code class="docutils literal notranslate"><span class="pre">p</span></code> in the <code class="docutils literal notranslate"><span class="pre">p</span></code>-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option.
Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>. Specifies the way to reduce the bag.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> will be a sparse tensor. See Notes under
<a class="reference internal" href="#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details regarding sparse gradients.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Shape:</p>
<blockquote>
<div><ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (LongTensor) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> (LongTensor, optional)</p>
<ul>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 2D of shape <cite>(B, N)</cite>,</p>
<p>it will be treated as <code class="docutils literal notranslate"><span class="pre">B</span></code> bags (sequences) each of fixed length <code class="docutils literal notranslate"><span class="pre">N</span></code>, and
this will return <code class="docutils literal notranslate"><span class="pre">B</span></code> values aggregated in a way depending on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is ignored and required to be <code class="docutils literal notranslate"><span class="pre">None</span></code> in this case.</p>
</li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D of shape <cite>(N)</cite>,</p>
<p>it will be treated as a concatenation of multiple bags (sequences).
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is required to be a 1D tensor containing the
starting index positions of each bag in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. Therefore,
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> of shape <cite>(B)</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be viewed as
having <code class="docutils literal notranslate"><span class="pre">B</span></code> bags. Empty bags (i.e., having 0-length) will have
returned vectors filled by zeros.</p>
</li>
</ul>
</li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> (Tensor): the learnable weights of the module of
shape <cite>(num_embeddings, embedding_dim)</cite></p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code>: aggregated embedding values of shape <cite>(B, embedding_dim)</cite></p></li>
</ul>
</div></blockquote>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="go">tensor([[ 0.3397,  0.3552,  0.5545],</span>
<span class="go">        [ 0.5893,  0.4386,  0.5882]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="one-hot">
<h4><span class="hidden-section">one_hot</span><a class="headerlink" href="#one-hot" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.one_hot">
<code class="descclassname">torch.nn.functional.</code><code class="descname">one_hot</code><span class="sig-paren">(</span><em>tensor</em>, <em>num_classes=0</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.nn.functional.one_hot" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes LongTensor with index values of shape <code class="docutils literal notranslate"><span class="pre">(*)</span></code> and returns a tensor
of shape <code class="docutils literal notranslate"><span class="pre">(*,</span> <span class="pre">num_classes)</span></code> that have zeros everywhere except where the
index of last dimension matches the corresponding value of the input tensor,
in which case it will be 1.</p>
<p>See also <a class="reference external" href="https://en.wikipedia.org/wiki/One-hot">One-hot on Wikipedia</a> .</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<em>LongTensor</em>) – class values of any shape.</p></li>
<li><p><strong>num_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Total number of classes. If set to -1, the number
of classes will be inferred as one greater than the largest class
value in the input tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>LongTensor that has one more dimension with 1 values at the
index of last dimension indicated by the input, and 0 everywhere
else.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[1, 0, 0],</span>
<span class="go">        [0, 1, 0],</span>
<span class="go">        [0, 0, 1],</span>
<span class="go">        [1, 0, 0],</span>
<span class="go">        [0, 1, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[1, 0, 0, 0, 0],</span>
<span class="go">        [0, 1, 0, 0, 0],</span>
<span class="go">        [0, 0, 1, 0, 0],</span>
<span class="go">        [1, 0, 0, 0, 0],</span>
<span class="go">        [0, 1, 0, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[[1, 0, 0],</span>
<span class="go">         [0, 1, 0]],</span>
<span class="go">        [[0, 0, 1],</span>
<span class="go">         [1, 0, 0]],</span>
<span class="go">        [[0, 1, 0],</span>
<span class="go">         [0, 0, 1]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="id60">
<h3>Distance functions<a class="headerlink" href="#id60" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pairwise-distance">
<h4><span class="hidden-section">pairwise_distance</span><a class="headerlink" href="#pairwise-distance" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.pairwise_distance">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pairwise_distance</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>p=2.0</em>, <em>eps=1e-06</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.pairwise_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.PairwiseDistance" title="torch.nn.PairwiseDistance"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.PairwiseDistance</span></code></a> for details</p>
</dd></dl>

</div>
<div class="section" id="cosine-similarity">
<h4><span class="hidden-section">cosine_similarity</span><a class="headerlink" href="#cosine-similarity" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.cosine_similarity">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_similarity</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>dim=1</em>, <em>eps=1e-8</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between x1 and x2, computed along dim.</p>
<div class="math">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – First input.</p></li>
<li><p><strong>x2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Second input (of size matching x1).</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension of vectors. Default: 1</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-8</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite>.</p></li>
<li><p>Output: <span class="math">\((\ast_1, \ast_2)\)</span> where 1 is at position <cite>dim</cite>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pdist">
<h4><span class="hidden-section">pdist</span><a class="headerlink" href="#pdist" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.pdist">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pdist</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.pdist" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the p-norm distance between every pair of row vectors in the input.
This is identical to the upper triangular portion, excluding the diagonal, of
<cite>torch.norm(input[:, None] - input, dim=2, p=p)</cite>. This function will be faster
if the rows are contiguous.</p>
<p>If input has shape <span class="math">\(N \times M\)</span> then the output will have shape
<span class="math">\(\frac{1}{2} N (N - 1)\)</span>.</p>
<p>This function is equivalent to <cite>scipy.spatial.distance.pdist(input,
‘minkowski’, p=p)</cite> if <span class="math">\(p \in (0, \infty)\)</span>. When <span class="math">\(p = 0\)</span> it is
equivalent to <cite>scipy.spatial.distance.pdist(input, ‘hamming’) * M</cite>.
When <span class="math">\(p = \infty\)</span>, the closest scipy function is
<cite>scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\(N \times M\)</span>.</p></li>
<li><p><strong>p</strong> – p value for the p-norm distance to calculate between each vector pair
<span class="math">\(\in [0, \infty]\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="id61">
<h3>Loss functions<a class="headerlink" href="#id61" title="Permalink to this headline">¶</a></h3>
<div class="section" id="binary-cross-entropy">
<h4><span class="hidden-section">binary_cross_entropy</span><a class="headerlink" href="#binary-cross-entropy" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures the Binary Cross Entropy
between the target and the output.</p>
<p>See <a class="reference internal" href="#torch.nn.BCELoss" title="torch.nn.BCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BCELoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Tensor of arbitrary shape</p></li>
<li><p><strong>target</strong> – Tensor of the same shape as input</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight
if provided it’s repeated to match input tensor shape</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="binary-cross-entropy-with-logits">
<h4><span class="hidden-section">binary_cross_entropy_with_logits</span><a class="headerlink" href="#binary-cross-entropy-with-logits" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy_with_logits">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy_with_logits</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em>, <em>pos_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy_with_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures Binary Cross Entropy between target and output
logits.</p>
<p>See <a class="reference internal" href="#torch.nn.BCEWithLogitsLoss" title="torch.nn.BCEWithLogitsLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BCEWithLogitsLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Tensor of arbitrary shape</p></li>
<li><p><strong>target</strong> – Tensor of the same shape as input</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight
if provided it’s repeated to match input tensor shape</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>pos_weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a weight of positive examples.
Must be a vector with length equal to the number of classes.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poisson-nll-loss">
<h4><span class="hidden-section">poisson_nll_loss</span><a class="headerlink" href="#poisson-nll-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.poisson_nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">poisson_nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>log_input=True</em>, <em>full=False</em>, <em>size_average=None</em>, <em>eps=1e-08</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.poisson_nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Poisson negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.PoissonNLLLoss" title="torch.nn.PoissonNLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">PoissonNLLLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – expectation of underlying Poisson distribution.</p></li>
<li><p><strong>target</strong> – random sample <span class="math">\(target \sim \text{Poisson}(input)\)</span>.</p></li>
<li><p><strong>log_input</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code> the loss is computed as
<span class="math">\(\exp(\text{input}) - \text{target} * \text{input}\)</span>, if <code class="docutils literal notranslate"><span class="pre">False</span></code> then loss is
<span class="math">\(\text{input} - \text{target} * \log(\text{input}+\text{eps})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>full</strong> – whether to compute full loss, i. e. to add the Stirling
approximation term. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
<span class="math">\(\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})\)</span>.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid evaluation of <span class="math">\(\log(0)\)</span> when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">log_input`=``False`</span></code>. Default: 1e-8</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="cosine-embedding-loss">
<h4><span class="hidden-section">cosine_embedding_loss</span><a class="headerlink" href="#cosine-embedding-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.cosine_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_embedding_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.cosine_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.CosineEmbeddingLoss" title="torch.nn.CosineEmbeddingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CosineEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="cross-entropy">
<h4><span class="hidden-section">cross_entropy</span><a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <cite>log_softmax</cite> and <cite>nll_loss</cite> in a single
function.</p>
<p>See <a class="reference internal" href="#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span>
in the case of K-dimensional loss.</p></li>
<li><p><strong>target</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>,
or <span class="math">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span> for
K-dimensional loss.</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="ctc-loss">
<h4><span class="hidden-section">ctc_loss</span><a class="headerlink" href="#ctc-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.ctc_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">ctc_loss</code><span class="sig-paren">(</span><em>log_probs</em>, <em>targets</em>, <em>input_lengths</em>, <em>target_lengths</em>, <em>blank=0</em>, <em>reduction='mean'</em>, <em>zero_infinity=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.ctc_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The Connectionist Temporal Classification loss.</p>
<p>See <a class="reference internal" href="#torch.nn.CTCLoss" title="torch.nn.CTCLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTCLoss</span></code></a> for details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_probs</strong> – <span class="math">\((T, N, C)\)</span> where <cite>C = number of characters in alphabet including blank</cite>,
<cite>T = input length</cite>, and <cite>N = batch size</cite>.
The logarithmized probabilities of the outputs
(e.g. obtained with <a class="reference internal" href="#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.log_softmax()</span></code></a>).</p></li>
<li><p><strong>targets</strong> – <span class="math">\((N, S)\)</span> or <cite>(sum(target_lengths))</cite>.
Targets cannot be blank. In the second form, the targets are assumed to be concatenated.</p></li>
<li><p><strong>input_lengths</strong> – <span class="math">\((N)\)</span>.
Lengths of the inputs (must each be <span class="math">\(\leq T\)</span>)</p></li>
<li><p><strong>target_lengths</strong> – <span class="math">\((N)\)</span>.
Lengths of the targets</p></li>
<li><p><strong>blank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Blank label. Default <span class="math">\(0\)</span>.</p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the output losses will be divided by the target lengths and
then the mean over the batch is taken, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be
summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>zero_infinity</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to zero infinite losses and the associated gradients.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
Infinite losses mainly occur when the inputs are too short
to be aligned to the targets.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">16</span><span class="p">,),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">,(</span><span class="mi">16</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hinge-embedding-loss">
<h4><span class="hidden-section">hinge_embedding_loss</span><a class="headerlink" href="#hinge-embedding-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.hinge_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hinge_embedding_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>margin=1.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.hinge_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.HingeEmbeddingLoss" title="torch.nn.HingeEmbeddingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">HingeEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="kl-div">
<h4><span class="hidden-section">kl_div</span><a class="headerlink" href="#kl-div" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.kl_div">
<code class="descclassname">torch.nn.functional.</code><code class="descname">kl_div</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.kl_div" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss.</p>
<p>See <a class="reference internal" href="#torch.nn.KLDivLoss" title="torch.nn.KLDivLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">KLDivLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Tensor of arbitrary shape</p></li>
<li><p><strong>target</strong> – Tensor of the same shape as input</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied
<code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code>: the sum of the output will be divided by the batchsize
<code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the output will be divided by the number of elements in the output
Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated,
and in the meantime, specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> doesn’t return the true kl divergence value, please use
:attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> which aligns with KL math definition.
In the next major release, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> will be changed to be the same as ‘batchmean’.</p>
</div>
</dd></dl>

</div>
<div class="section" id="l1-loss">
<h4><span class="hidden-section">l1_loss</span><a class="headerlink" href="#l1-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that takes the mean element-wise absolute value difference.</p>
<p>See <a class="reference internal" href="#torch.nn.L1Loss" title="torch.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="mse-loss">
<h4><span class="hidden-section">mse_loss</span><a class="headerlink" href="#mse-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.mse_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">mse_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.mse_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the element-wise mean squared error.</p>
<p>See <a class="reference internal" href="#torch.nn.MSELoss" title="torch.nn.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="margin-ranking-loss">
<h4><span class="hidden-section">margin_ranking_loss</span><a class="headerlink" href="#margin-ranking-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.margin_ranking_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">margin_ranking_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.margin_ranking_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MarginRankingLoss" title="torch.nn.MarginRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarginRankingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-margin-loss">
<h4><span class="hidden-section">multilabel_margin_loss</span><a class="headerlink" href="#multilabel-margin-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.multilabel_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.multilabel_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiLabelMarginLoss" title="torch.nn.MultiLabelMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiLabelMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-soft-margin-loss">
<h4><span class="hidden-section">multilabel_soft_margin_loss</span><a class="headerlink" href="#multilabel-soft-margin-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.multilabel_soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.multilabel_soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.MultiLabelSoftMarginLoss" title="torch.nn.MultiLabelSoftMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiLabelSoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multi-margin-loss">
<h4><span class="hidden-section">multi_margin_loss</span><a class="headerlink" href="#multi-margin-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.multi_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multi_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>p=1</em>, <em>margin=1.0</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.multi_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,</dt><dd><p>reduce=None, reduction=’mean’) -&gt; Tensor</p>
</dd>
</dl>
<p>See <a class="reference internal" href="#torch.nn.MultiMarginLoss" title="torch.nn.MultiMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="nll-loss">
<h4><span class="hidden-section">nll_loss</span><a class="headerlink" href="#nll-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span>
in the case of K-dimensional loss.</p></li>
<li><p><strong>target</strong> – <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>,
or <span class="math">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span> for
K-dimensional loss.</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="smooth-l1-loss">
<h4><span class="hidden-section">smooth_l1_loss</span><a class="headerlink" href="#smooth-l1-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.smooth_l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">smooth_l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.smooth_l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.</p>
<p>See <a class="reference internal" href="#torch.nn.SmoothL1Loss" title="torch.nn.SmoothL1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="soft-margin-loss">
<h4><span class="hidden-section">soft_margin_loss</span><a class="headerlink" href="#soft-margin-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.SoftMarginLoss" title="torch.nn.SoftMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="triplet-margin-loss">
<h4><span class="hidden-section">triplet_margin_loss</span><a class="headerlink" href="#triplet-margin-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.triplet_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">triplet_margin_loss</code><span class="sig-paren">(</span><em>anchor</em>, <em>positive</em>, <em>negative</em>, <em>margin=1.0</em>, <em>p=2</em>, <em>eps=1e-06</em>, <em>swap=False</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.triplet_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="#torch.nn.TripletMarginLoss" title="torch.nn.TripletMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">TripletMarginLoss</span></code></a> for details</p>
</dd></dl>

</div>
</div>
<div class="section" id="vision-functions">
<h3>Vision functions<a class="headerlink" href="#vision-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pixel-shuffle">
<h4><span class="hidden-section">pixel_shuffle</span><a class="headerlink" href="#pixel-shuffle" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.pixel_shuffle">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pixel_shuffle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.pixel_shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a tensor of shape <span class="math">\((*, C \times r^2, H, W)\)</span> to a
tensor of shape <span class="math">\((*, C, H \times r, W \times r)\)</span>.</p>
<p>See <a class="reference internal" href="#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code class="xref py py-class docutils literal notranslate"><span class="pre">PixelShuffle</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – factor to increase spatial resolution by</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pad">
<h4><span class="hidden-section">pad</span><a class="headerlink" href="#pad" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.pad">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pad</code><span class="sig-paren">(</span><em>input</em>, <em>pad</em>, <em>mode='constant'</em>, <em>value=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads tensor.</p>
<dl class="simple">
<dt>Padding size:</dt><dd><p>The padding size by which to pad some dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
are described starting from the last dimension and moving forward.
<span class="math">\(\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor\)</span> dimensions
of <code class="docutils literal notranslate"><span class="pre">input</span></code> will be padded.
For example, to pad only the last dimension of the input tensor, then
<a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">pad</span></code></a> has the form
<span class="math">\((\text{padding\_left}, \text{padding\_right})\)</span>;
to pad the last 2 dimensions of the input tensor, then use
<span class="math">\((\text{padding\_left}, \text{padding\_right},\)</span>
<span class="math">\(\text{padding\_top}, \text{padding\_bottom})\)</span>;
to pad the last 3 dimensions, use
<span class="math">\((\text{padding\_left}, \text{padding\_right},\)</span>
<span class="math">\(\text{padding\_top}, \text{padding\_bottom}\)</span>
<span class="math">\(\text{padding\_front}, \text{padding\_back})\)</span>.</p>
</dd>
<dt>Padding mode:</dt><dd><p>See <a class="reference internal" href="#torch.nn.ConstantPad2d" title="torch.nn.ConstantPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConstantPad2d</span></code></a>, <a class="reference internal" href="#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReflectionPad2d</span></code></a>, and
<a class="reference internal" href="#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReplicationPad2d</span></code></a> for concrete examples on how each of the
padding modes works. Constant padding is implemented for arbitrary dimensions.
Replicate padding is implemented for padding the last 3 dimensions of 5D input
tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of
3D input tensor. Reflect padding is only implemented for padding the last 2
dimensions of 4D input tensor, or the last dimension of 3D input tensor.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – N-dimensional tensor</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – m-elements tuple, where
<span class="math">\(\frac{m}{2} \leq\)</span> input dimensions and <span class="math">\(m\)</span> is even.</p></li>
<li><p><strong>mode</strong> – <code class="docutils literal notranslate"><span class="pre">'constant'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">'constant'</span></code></p></li>
<li><p><strong>value</strong> – fill value for <code class="docutils literal notranslate"><span class="pre">'constant'</span></code> padding. Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p1d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># pad last dim by 1 on each side</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p1d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># effectively zero padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p2d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p2d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 8, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p3d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p3d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 9, 7, 3])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="interpolate">
<h4><span class="hidden-section">interpolate</span><a class="headerlink" href="#interpolate" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.interpolate">
<code class="descclassname">torch.nn.functional.</code><code class="descname">interpolate</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.interpolate" title="Permalink to this definition">¶</a></dt>
<dd><p>Down/up samples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p>
<p>The algorithm used for interpolation is determined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric sampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [optional depth] x [optional height] x width</cite>.</p>
<p>The modes available for resizing are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite>, <cite>bicubic</cite> (4D-only), <cite>trilinear</cite> (5D-only), <cite>area</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – algorithm used for upsampling:
<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code> | <code class="docutils literal notranslate"><span class="pre">'linear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> |
<code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'area'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code></p></li>
<li><p><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the
input and output as squares rather than points.
If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input and output tensors are aligned by the
center points of their corner pixels. If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the input and
output tensors are aligned by the corner points of their corner
pixels, and the interpolation uses edge value padding for out-of-boundary values.
This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <code class="docutils literal notranslate"><span class="pre">'linear'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference internal" href="#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code></a> for concrete examples on how this
affects the outputs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="id62">
<h4><span class="hidden-section">upsample</span><a class="headerlink" href="#id62" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.upsample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...)</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<p>The algorithm used for upsampling is determined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric upsampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [optional depth] x [optional height] x width</cite>.</p>
<p>The modes available for upsampling are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite>, <cite>bicubic</cite> (4D-only), <cite>trilinear</cite> (5D-only)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em>) – multiplier for spatial size. Has to be an integer.</p></li>
<li><p><strong>mode</strong> (<em>string</em>) – algorithm used for upsampling:
<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code> | <code class="docutils literal notranslate"><span class="pre">'linear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> |
<code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code></p></li>
<li><p><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the
input and output as squares rather than points.
If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input and output tensors are aligned by the
center points of their corner pixels. If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the input and
output tensors are aligned by the corner points of their corner
pixels, and the interpolation uses edge value padding for out-of-boundary values.
This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <code class="docutils literal notranslate"><span class="pre">'linear'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> or <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference internal" href="#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code></a> for concrete examples on how this
affects the outputs.</p>
</div>
</dd></dl>

</div>
<div class="section" id="upsample-nearest">
<h4><span class="hidden-section">upsample_nearest</span><a class="headerlink" href="#upsample-nearest" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.upsample_nearest">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_nearest</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.upsample_nearest" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using nearest neighbours’ pixel values.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='nearest')</span></code>.</p>
</div>
<p>Currently spatial and volumetric upsampling are supported (i.e. expected
inputs are 4 or 5 dimensional).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatia
size.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – multiplier for spatial size. Has to be an integer.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="upsample-bilinear">
<h4><span class="hidden-section">upsample_bilinear</span><a class="headerlink" href="#upsample-bilinear" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.upsample_bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_bilinear</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.upsample_bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using bilinear upsampling.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with
<code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<p>Expected inputs are spatial (4 dimensional). Use <cite>upsample_trilinear</cite> fo
volumetric (5 dimensional) inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – multiplier for spatial size</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="grid-sample">
<h4><span class="hidden-section">grid_sample</span><a class="headerlink" href="#grid-sample" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.grid_sample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">grid_sample</code><span class="sig-paren">(</span><em>input</em>, <em>grid</em>, <em>mode='bilinear'</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.grid_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and a flow-field <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code>, computes the
<code class="docutils literal notranslate"><span class="pre">output</span></code> using <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> values and pixel locations from <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code>.</p>
<p>Currently, only spatial (4-D) and volumetric (5-D) <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> are
supported.</p>
<p>In the spatial (4-D) case, for <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with shape
<span class="math">\((N, C, H_\text{in}, W_\text{in})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> with shape
<span class="math">\((N, H_\text{out}, W_\text{out}, 2)\)</span>, the output will have shape
<span class="math">\((N, C, H_\text{out}, W_\text{out})\)</span>.</p>
<p>For each output location <code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>, the size-2 vector
<code class="docutils literal notranslate"><span class="pre">grid[n,</span> <span class="pre">h,</span> <span class="pre">w]</span></code> specifies <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> pixel locations <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>,
which are used to interpolate the output value <code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>.
In the case of 5D inputs, <code class="docutils literal notranslate"><span class="pre">grid[n,</span> <span class="pre">d,</span> <span class="pre">h,</span> <span class="pre">w]</span></code> specifies the
<code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">z</span></code> pixel locations for interpolating
<code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">d,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> argument specifies <code class="docutils literal notranslate"><span class="pre">nearest</span></code> or
<code class="docutils literal notranslate"><span class="pre">bilinear</span></code> interpolation method to sample the input pixels.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> should have most values in the range of <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>. This is
because the pixel locations are normalized by the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> spatial
dimensions. For example, values <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">-1,</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">-1</span></code> is the left-top pixel of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, and values  <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">1</span></code> is the right-bottom pixel of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> has values outside the range of <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>, those locations
are handled as defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_mode</span></code>. Options are</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;zeros&quot;</span></code>: use <code class="docutils literal notranslate"><span class="pre">0</span></code> for out-of-bound values,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;border&quot;</span></code>: use border values for out-of-bound values,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;reflection&quot;</span></code>: use values at locations reflected by
the border for out-of-bound values. For location far away from the
border, it will keep being reflected until becoming in bound, e.g.,
(normalized) pixel location <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">-3.5</span></code> reflects by <code class="docutils literal notranslate"><span class="pre">-1</span></code> and
becomes <code class="docutils literal notranslate"><span class="pre">x'</span> <span class="pre">=</span> <span class="pre">1.5</span></code>, then reflects by border <code class="docutils literal notranslate"><span class="pre">1</span></code> and becomes
<code class="docutils literal notranslate"><span class="pre">x''</span> <span class="pre">=</span> <span class="pre">-0.5</span></code>.</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is often used in building Spatial Transformer Networks.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in be backward that is not easily switched off.
Please see the notes on <span class="xref std std-doc">/notes/randomness</span> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input of shape <span class="math">\((N, C, H_\text{in}, W_\text{in})\)</span> (4-D case)
or <span class="math">\((N, C, D_\text{in}, H_\text{in}, W_\text{in})\)</span> (5-D case)</p></li>
<li><p><strong>grid</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – flow-field of shape <span class="math">\((N, H_\text{out}, W_\text{out}, 2)\)</span> (4-D case)
or <span class="math">\((N, D_\text{out}, H_\text{out}, W_\text{out}, 3)\)</span> (5-D case)</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – interpolation mode to calculate output values
<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code></p></li>
<li><p><strong>padding_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – padding mode for outside grid values
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code> | <code class="docutils literal notranslate"><span class="pre">'border'</span></code> | <code class="docutils literal notranslate"><span class="pre">'reflection'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="affine-grid">
<h4><span class="hidden-section">affine_grid</span><a class="headerlink" href="#affine-grid" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.affine_grid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">affine_grid</code><span class="sig-paren">(</span><em>theta</em>, <em>size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.affine_grid" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a 2d flow field, given a batch of affine matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">theta</span></code>.
Generally used in conjunction with <a class="reference internal" href="#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code class="xref py py-func docutils literal notranslate"><span class="pre">grid_sample()</span></code></a> to
implement Spatial Transformer Networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input batch of affine matrices (<span class="math">\(N \times 2 \times 3\)</span>)</p></li>
<li><p><strong>size</strong> (<em>torch.Size</em>) – the target output image size (<span class="math">\(N \times C \times H \times W\)</span>).
Example: torch.Size((32, 3, 24, 24))</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor of size (<span class="math">\(N \times H \times W \times 2\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="dataparallel-functions-multi-gpu-distributed">
<h3>DataParallel functions (multi-GPU, distributed)<a class="headerlink" href="#dataparallel-functions-multi-gpu-distributed" title="Permalink to this headline">¶</a></h3>
<div class="section" id="data-parallel">
<h4><span class="hidden-section">data_parallel</span><a class="headerlink" href="#data-parallel" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.parallel.data_parallel">
<code class="descclassname">torch.nn.parallel.</code><code class="descname">data_parallel</code><span class="sig-paren">(</span><em>module</em>, <em>inputs</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em>, <em>module_kwargs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.parallel.data_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates module(input) in parallel across the GPUs given in device_ids.</p>
<p>This is the functional version of the DataParallel module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to evaluate in parallel</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – inputs to the module</p></li>
<li><p><strong>device_ids</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – GPU ids on which to replicate module</p></li>
<li><p><strong>output_device</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="index.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – GPU location of the output  Use -1 to indicate the CPU.
(default: device_ids[0])</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing the result of module(input) located on
output_device</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-init">
<h2>torch.nn.init<a class="headerlink" href="#torch-nn-init" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.nn.init.calculate_gain">
<code class="descclassname">torch.nn.init.</code><code class="descname">calculate_gain</code><span class="sig-paren">(</span><em>nonlinearity</em>, <em>param=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.calculate_gain" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the recommended gain value for the given nonlinearity function.
The values are as follows:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 24%" />
<col style="width: 76%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>nonlinearity</p></th>
<th class="head"><p>gain</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Linear / Identity</p></td>
<td><p><span class="math">\(1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Conv{1,2,3}D</p></td>
<td><p><span class="math">\(1\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Sigmoid</p></td>
<td><p><span class="math">\(1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Tanh</p></td>
<td><p><span class="math">\(\frac{5}{3}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>ReLU</p></td>
<td><p><span class="math">\(\sqrt{2}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Leaky Relu</p></td>
<td><p><span class="math">\(\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}\)</span></p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name)</p></li>
<li><p><strong>param</strong> – optional parameter for the non-linear function</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gain</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>b=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with values drawn from the uniform
distribution <span class="math">\(\mathcal{U}(a, b)\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></p></li>
<li><p><strong>a</strong> – the lower bound of the uniform distribution</p></li>
<li><p><strong>b</strong> – the upper bound of the uniform distribution</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>mean=0</em>, <em>std=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with values drawn from the normal
distribution <span class="math">\(\mathcal{N}(\text{mean}, \text{std})\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></p></li>
<li><p><strong>mean</strong> – the mean of the normal distribution</p></li>
<li><p><strong>std</strong> – the standard deviation of the normal distribution</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.constant_">
<code class="descclassname">torch.nn.init.</code><code class="descname">constant_</code><span class="sig-paren">(</span><em>tensor</em>, <em>val</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.constant_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor with the value <span class="math">\(\text{val}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></p></li>
<li><p><strong>val</strong> – the value to fill the tensor with</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.eye_">
<code class="descclassname">torch.nn.init.</code><code class="descname">eye_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.eye_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2-dimensional input <cite>Tensor</cite> with the identity
matrix. Preserves the identity of the inputs in <cite>Linear</cite> layers, where as
many inputs are preserved as possible.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – a 2-dimensional <cite>torch.Tensor</cite></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">eye_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.dirac_">
<code class="descclassname">torch.nn.init.</code><code class="descname">dirac_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.dirac_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the {3, 4, 5}-dimensional input <cite>Tensor</cite> with the Dirac
delta function. Preserves the identity of the inputs in <cite>Convolutional</cite>
layers, where as many input channels are preserved as possible.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – a {3, 4, 5}-dimensional <cite>torch.Tensor</cite></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">dirac_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.xavier_uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in <cite>Understanding the difficulty of training deep feedforward
neural networks</cite> - Glorot, X. &amp; Bengio, Y. (2010), using a uniform
distribution. The resulting tensor will have values sampled from
<span class="math">\(\mathcal{U}(-a, a)\)</span> where</p>
<div class="math">
\[a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}

\]</div>
<p>Also known as Glorot initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></p></li>
<li><p><strong>gain</strong> – an optional scaling factor</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.xavier_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in <cite>Understanding the difficulty of training deep feedforward
neural networks</cite> - Glorot, X. &amp; Bengio, Y. (2010), using a normal
distribution. The resulting tensor will have values sampled from
<span class="math">\(\mathcal{N}(0, \text{std})\)</span> where</p>
<div class="math">
\[\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}}

\]</div>
<p>Also known as Glorot initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></p></li>
<li><p><strong>gain</strong> – an optional scaling factor</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_uniform_">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_uniform_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em>, <em>nonlinearity='leaky_relu'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.kaiming_uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in <cite>Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification</cite> - He, K. et al. (2015), using a
uniform distribution. The resulting tensor will have values sampled from
<span class="math">\(\mathcal{U}(-\text{bound}, \text{bound})\)</span> where</p>
<div class="math">
\[\text{bound} = \sqrt{\frac{6}{(1 + a^2) \times \text{fan\_in}}}

\]</div>
<p>Also known as He initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></p></li>
<li><p><strong>a</strong> – the negative slope of the rectifier used after this layer (0 for ReLU
by default)</p></li>
<li><p><strong>mode</strong> – either <code class="docutils literal notranslate"><span class="pre">'fan_in'</span></code> (default) or <code class="docutils literal notranslate"><span class="pre">'fan_out'</span></code>. Choosing <code class="docutils literal notranslate"><span class="pre">'fan_in'</span></code>
preserves the magnitude of the variance of the weights in the
forward pass. Choosing <code class="docutils literal notranslate"><span class="pre">'fan_out'</span></code> preserves the magnitudes in the
backwards pass.</p></li>
<li><p><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name),
recommended to use only with <code class="docutils literal notranslate"><span class="pre">'relu'</span></code> or <code class="docutils literal notranslate"><span class="pre">'leaky_relu'</span></code> (default).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_normal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_normal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em>, <em>nonlinearity='leaky_relu'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.kaiming_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with values according to the method
described in <cite>Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification</cite> - He, K. et al. (2015), using a
normal distribution. The resulting tensor will have values sampled from
<span class="math">\(\mathcal{N}(0, \text{std})\)</span> where</p>
<div class="math">
\[\text{std} = \sqrt{\frac{2}{(1 + a^2) \times \text{fan\_in}}}

\]</div>
<p>Also known as He initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></p></li>
<li><p><strong>a</strong> – the negative slope of the rectifier used after this layer (0 for ReLU
by default)</p></li>
<li><p><strong>mode</strong> – either <code class="docutils literal notranslate"><span class="pre">'fan_in'</span></code> (default) or <code class="docutils literal notranslate"><span class="pre">'fan_out'</span></code>. Choosing <code class="docutils literal notranslate"><span class="pre">'fan_in'</span></code>
preserves the magnitude of the variance of the weights in the
forward pass. Choosing <code class="docutils literal notranslate"><span class="pre">'fan_out'</span></code> preserves the magnitudes in the
backwards pass.</p></li>
<li><p><strong>nonlinearity</strong> – the non-linear function (<cite>nn.functional</cite> name),
recommended to use only with <code class="docutils literal notranslate"><span class="pre">'relu'</span></code> or <code class="docutils literal notranslate"><span class="pre">'leaky_relu'</span></code> (default).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.orthogonal_">
<code class="descclassname">torch.nn.init.</code><code class="descname">orthogonal_</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.orthogonal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input <cite>Tensor</cite> with a (semi) orthogonal matrix, as
described in <cite>Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks</cite> - Saxe, A. et al. (2013). The input tensor must have
at least 2 dimensions, and for tensors with more than 2 dimensions the
trailing dimensions are flattened.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite>, where <span class="math">\(n \geq 2\)</span></p></li>
<li><p><strong>gain</strong> – optional scaling factor</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal_</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.sparse_">
<code class="descclassname">torch.nn.init.</code><code class="descname">sparse_</code><span class="sig-paren">(</span><em>tensor</em>, <em>sparsity</em>, <em>std=0.01</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.sparse_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2D input <cite>Tensor</cite> as a sparse matrix, where the
non-zero elements will be drawn from the normal distribution
<span class="math">\(\mathcal{N}(0, 0.01)\)</span>, as described in <cite>Deep learning via
Hessian-free optimization</cite> - Martens, J. (2010).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – an n-dimensional <cite>torch.Tensor</cite></p></li>
<li><p><strong>sparsity</strong> – The fraction of elements in each column to be set to zero</p></li>
<li><p><strong>std</strong> – the standard deviation of the normal distribution used to generate
the non-zero values</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">sparse_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">sparsity</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<span id="document-optim"></span><div class="section" id="module-torch.optim">
<span id="torch-optim"></span><h2>torch.optim<a class="headerlink" href="#module-torch.optim" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="#module-torch.optim" title="torch.optim"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.optim</span></code></a> is a package implementing various optimization algorithms.
Most commonly used methods are already supported, and the interface is general
enough, so that more sophisticated ones can be also easily integrated in the
future.</p>
<div class="section" id="how-to-use-an-optimizer">
<h3>How to use an optimizer<a class="headerlink" href="#how-to-use-an-optimizer" title="Permalink to this headline">¶</a></h3>
<p>To use <a class="reference internal" href="#module-torch.optim" title="torch.optim"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.optim</span></code></a> you have to construct an optimizer object, that will hold
the current state and will update the parameters based on the computed gradients.</p>
<div class="section" id="constructing-it">
<h4>Constructing it<a class="headerlink" href="#constructing-it" title="Permalink to this headline">¶</a></h4>
<p>To construct an <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a> you have to give it an iterable containing the
parameters (all should be <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> s) to optimize. Then,
you can specify optimizer-specific options such as the learning rate, weight decay, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you need to move a model to GPU via <cite>.cuda()</cite>, please do so before
constructing optimizers for it. Parameters of a model after <cite>.cuda()</cite> will
be different objects with those before the call.</p>
<p>In general, you should make sure that optimized parameters live in
consistent locations when optimizers are constructed and used.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">var1</span><span class="p">,</span> <span class="n">var2</span><span class="p">],</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="per-parameter-options">
<h4>Per-parameter options<a class="headerlink" href="#per-parameter-options" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a> s also support specifying per-parameter options. To do this, instead
of passing an iterable of <code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code> s, pass in an iterable of
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> s. Each of them will define a separate parameter group, and should contain
a <code class="docutils literal notranslate"><span class="pre">params</span></code> key, containing a list of parameters belonging to it. Other keys
should match the keyword arguments accepted by the optimizers, and will be used
as optimization options for this group.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can still pass options as keyword arguments. They will be used as
defaults, in the groups that didn’t override them. This is useful when you
only want to vary a single option, while keeping all others consistent
between parameter groups.</p>
</div>
<p>For example, this is very useful when one wants to specify per-layer learning rates:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>
            <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>This means that <code class="docutils literal notranslate"><span class="pre">model.base</span></code>’s parameters will use the default learning rate of <code class="docutils literal notranslate"><span class="pre">1e-2</span></code>,
<code class="docutils literal notranslate"><span class="pre">model.classifier</span></code>’s parameters will use a learning rate of <code class="docutils literal notranslate"><span class="pre">1e-3</span></code>, and a momentum of
<code class="docutils literal notranslate"><span class="pre">0.9</span></code> will be used for all parameters.</p>
</div>
<div class="section" id="taking-an-optimization-step">
<h4>Taking an optimization step<a class="headerlink" href="#taking-an-optimization-step" title="Permalink to this headline">¶</a></h4>
<p>All optimizers implement a <a class="reference internal" href="#torch.optim.Optimizer.step" title="torch.optim.Optimizer.step"><code class="xref py py-func docutils literal notranslate"><span class="pre">step()</span></code></a> method, that updates the
parameters. It can be used in two ways:</p>
<div class="section" id="optimizer-step">
<h5><code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code><a class="headerlink" href="#optimizer-step" title="Permalink to this headline">¶</a></h5>
<p>This is a simplified version supported by most optimizers. The function can be
called once the gradients are computed using e.g.
<code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="optimizer-step-closure">
<h5><code class="docutils literal notranslate"><span class="pre">optimizer.step(closure)</span></code><a class="headerlink" href="#optimizer-step-closure" title="Permalink to this headline">¶</a></h5>
<p>Some optimization algorithms such as Conjugate Gradient and LBFGS need to
reevaluate the function multiple times, so you have to pass in a closure that
allows them to recompute your model. The closure should clear the gradients,
compute the loss, and return it.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="algorithms">
<h3>Algorithms<a class="headerlink" href="#algorithms" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.optim.Optimizer">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Optimizer</code><span class="sig-paren">(</span><em>params</em>, <em>defaults</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all optimizers.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Parameters need to be specified as collections that have a deterministic
ordering that is consistent between runs. Examples of objects that don’t
satisfy those properties are sets and iterators over values of dictionaries.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – an iterable of <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> s or
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> s. Specifies what Tensors should be optimized.</p></li>
<li><p><strong>defaults</strong> – (dict): a dict containing default values of optimization
options (used when a parameter group doesn’t specify them).</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.Optimizer.add_param_group">
<code class="descname">add_param_group</code><span class="sig-paren">(</span><em>param_group</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.add_param_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a param group to the <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a> s <cite>param_groups</cite>.</p>
<p>This can be useful when fine tuning a pre-trained network as frozen layers can be made
trainable and added to the <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a> as training progresses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_group</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – Specifies what Tensors should be optimized along with group</p></li>
<li><p><strong>optimization options.</strong> (<em>specific</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.optim.Optimizer.load_state_dict">
<code class="descname">load_state_dict</code><span class="sig-paren">(</span><em>state_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the optimizer state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – optimizer state. Should be an object returned
from a call to <a class="reference internal" href="#torch.optim.Optimizer.state_dict" title="torch.optim.Optimizer.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.optim.Optimizer.state_dict">
<code class="descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state of the optimizer as a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p>
<p>It contains two entries:</p>
<ul class="simple">
<li><dl class="simple">
<dt>state - a dict holding current optimization state. Its content</dt><dd><p>differs between optimizer classes.</p>
</dd>
</dl>
</li>
<li><p>param_groups - a dict containing all parameter groups</p></li>
</ul>
</dd></dl>

<dl class="method">
<dt id="torch.optim.Optimizer.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step (parameter update).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em>) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.optim.Optimizer.zero_grad">
<code class="descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Clears the gradients of all optimized <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> s.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Adadelta">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Adadelta</code><span class="sig-paren">(</span><em>params</em>, <em>lr=1.0</em>, <em>rho=0.9</em>, <em>eps=1e-06</em>, <em>weight_decay=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adadelta" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Adadelta algorithm.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>rho</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – coefficient used for computing a running average
of squared gradients (default: 0.9)</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-6)</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – coefficient that scale delta before it is applied
to the parameters (default: 1.0)</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.Adadelta.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adadelta.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Adagrad">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Adagrad</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.01</em>, <em>lr_decay=0</em>, <em>weight_decay=0</em>, <em>initial_accumulator_value=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adagrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Adagrad algorithm.</p>
<p>It has been proposed in <a class="reference external" href="http://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – learning rate (default: 1e-2)</p></li>
<li><p><strong>lr_decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – learning rate decay (default: 0)</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.Adagrad.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adagrad.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Adam">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Adam</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.001</em>, <em>betas=(0.9</em>, <em>0.999)</em>, <em>eps=1e-08</em>, <em>weight_decay=0</em>, <em>amsgrad=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Adam algorithm.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – learning rate (default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em><em>, </em><em>optional</em>) – coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
<li><p><strong>amsgrad</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether to use the AMSGrad variant of this
algorithm from the paper <a class="reference external" href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a>
(default: False)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.Adam.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adam.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.SparseAdam">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">SparseAdam</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.001</em>, <em>betas=(0.9</em>, <em>0.999)</em>, <em>eps=1e-08</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.SparseAdam" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements lazy version of Adam algorithm suitable for sparse tensors.</p>
<p>In this variant, only moments that show up in the gradient get updated, and
only those portions of the gradient get applied to the parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – learning rate (default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em><em>, </em><em>optional</em>) – coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.SparseAdam.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.SparseAdam.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Adamax">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Adamax</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.002</em>, <em>betas=(0.9</em>, <em>0.999)</em>, <em>eps=1e-08</em>, <em>weight_decay=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adamax" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Adamax algorithm (a variant of Adam based on infinity norm).</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – learning rate (default: 2e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em><em>, </em><em>optional</em>) – coefficients used for computing
running averages of gradient and its square</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.Adamax.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adamax.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.ASGD">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">ASGD</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.01</em>, <em>lambd=0.0001</em>, <em>alpha=0.75</em>, <em>t0=1000000.0</em>, <em>weight_decay=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.ASGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Averaged Stochastic Gradient Descent.</p>
<p>It has been proposed in <a class="reference external" href="http://dl.acm.org/citation.cfm?id=131098">Acceleration of stochastic approximation by
averaging</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – learning rate (default: 1e-2)</p></li>
<li><p><strong>lambd</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – decay term (default: 1e-4)</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – power for eta update (default: 0.75)</p></li>
<li><p><strong>t0</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – point at which to start averaging (default: 1e6)</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.ASGD.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.ASGD.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.LBFGS">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">LBFGS</code><span class="sig-paren">(</span><em>params</em>, <em>lr=1</em>, <em>max_iter=20</em>, <em>max_eval=None</em>, <em>tolerance_grad=1e-05</em>, <em>tolerance_change=1e-09</em>, <em>history_size=100</em>, <em>line_search_fn=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.LBFGS" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements L-BFGS algorithm.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This optimizer doesn’t support per-parameter options and parameter
groups (there can be only one).</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Right now all parameters have to be on a single device. This will be
improved in the future.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a very memory intensive optimizer (it requires additional
<code class="docutils literal notranslate"><span class="pre">param_bytes</span> <span class="pre">*</span> <span class="pre">(history_size</span> <span class="pre">+</span> <span class="pre">1)</span></code> bytes). If it doesn’t fit in memory
try reducing the history size, or use a different algorithm.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – learning rate (default: 1)</p></li>
<li><p><strong>max_iter</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – maximal number of iterations per optimization step
(default: 20)</p></li>
<li><p><strong>max_eval</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – maximal number of function evaluations per optimization
step (default: max_iter * 1.25).</p></li>
<li><p><strong>tolerance_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – termination tolerance on first order optimality
(default: 1e-5).</p></li>
<li><p><strong>tolerance_change</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – termination tolerance on function
value/parameter changes (default: 1e-9).</p></li>
<li><p><strong>history_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – update history size (default: 100).</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.LBFGS.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.LBFGS.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.RMSprop">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">RMSprop</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.01</em>, <em>alpha=0.99</em>, <em>eps=1e-08</em>, <em>weight_decay=0</em>, <em>momentum=0</em>, <em>centered=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.RMSprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements RMSprop algorithm.</p>
<p>Proposed by G. Hinton in his
<a class="reference external" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">course</a>.</p>
<p>The centered version first appears in <a class="reference external" href="https://arxiv.org/pdf/1308.0850v5.pdf">Generating Sequences
With Recurrent Neural Networks</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – learning rate (default: 1e-2)</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – momentum factor (default: 0)</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – smoothing constant (default: 0.99)</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability (default: 1e-8)</p></li>
<li><p><strong>centered</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, compute the centered RMSProp,
the gradient is normalized by an estimation of its variance</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.RMSprop.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.RMSprop.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Rprop">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Rprop</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.01</em>, <em>etas=(0.5</em>, <em>1.2)</em>, <em>step_sizes=(1e-06</em>, <em>50)</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Rprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the resilient backpropagation algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – learning rate (default: 1e-2)</p></li>
<li><p><strong>etas</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em><em>, </em><em>optional</em>) – pair of (etaminus, etaplis), that
are multiplicative increase and decrease factors
(default: (0.5, 1.2))</p></li>
<li><p><strong>step_sizes</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em><em>, </em><em>optional</em>) – a pair of minimal and
maximal allowed step sizes (default: (1e-6, 50))</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.optim.Rprop.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Rprop.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.SGD">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">SGD</code><span class="sig-paren">(</span><em>params</em>, <em>lr=&lt;required parameter&gt;</em>, <em>momentum=0</em>, <em>dampening=0</em>, <em>weight_decay=0</em>, <em>nesterov=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements stochastic gradient descent (optionally with momentum).</p>
<p>Nesterov momentum is based on the formula from
<a class="reference external" href="http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf">On the importance of initialization and momentum in deep learning</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups</p></li>
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – learning rate</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – momentum factor (default: 0)</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
<li><p><strong>dampening</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – dampening for momentum (default: 0)</p></li>
<li><p><strong>nesterov</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – enables Nesterov momentum (default: False)</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The implementation of SGD with Momentum/Nesterov subtly differs from
Sutskever et. al. and implementations in some other frameworks.</p>
<p>Considering the specific case of Momentum, the update can be written as</p>
<div class="math">
\[v = \rho * v + g \\
p = p - lr * v

\]</div>
<p>where p, g, v and <span class="math">\(\rho\)</span> denote the parameters, gradient,
velocity, and momentum respectively.</p>
<p>This is in contrast to Sutskever et. al. and
other frameworks which employ an update of the form</p>
<div class="math">
\[v = \rho * v + lr * g \\
p = p - v

\]</div>
<p>The Nesterov version is analogously modified.</p>
</div>
<dl class="method">
<dt id="torch.optim.SGD.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.SGD.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="how-to-adjust-learning-rate">
<h3>How to adjust Learning Rate<a class="headerlink" href="#how-to-adjust-learning-rate" title="Permalink to this headline">¶</a></h3>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler</span></code> provides several methods to adjust the learning
rate based on the number of epochs. <a class="reference internal" href="#torch.optim.lr_scheduler.ReduceLROnPlateau" title="torch.optim.lr_scheduler.ReduceLROnPlateau"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.ReduceLROnPlateau</span></code></a>
allows dynamic learning rate reducing based on some validation measurements.</p>
<dl class="class">
<dt id="torch.optim.lr_scheduler.LambdaLR">
<em class="property">class </em><code class="descclassname">torch.optim.lr_scheduler.</code><code class="descname">LambdaLR</code><span class="sig-paren">(</span><em>optimizer</em>, <em>lr_lambda</em>, <em>last_epoch=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.lr_scheduler.LambdaLR" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the learning rate of each parameter group to the initial lr
times a given function. When last_epoch=-1, sets initial lr as lr.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<a class="reference internal" href="index.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a>) – Wrapped optimizer.</p></li>
<li><p><strong>lr_lambda</strong> (<em>function</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – A function which computes a multiplicative
factor given an integer parameter epoch, or a list of such
functions, one for each group in optimizer.param_groups.</p></li>
<li><p><strong>last_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The index of last epoch. Default: -1.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming optimizer has two groups.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lambda1</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">//</span> <span class="mi">30</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lambda2</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mf">0.95</span> <span class="o">**</span> <span class="n">epoch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="p">[</span><span class="n">lambda1</span><span class="p">,</span> <span class="n">lambda2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.optim.lr_scheduler.LambdaLR.load_state_dict">
<code class="descname">load_state_dict</code><span class="sig-paren">(</span><em>state_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.lr_scheduler.LambdaLR.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the schedulers state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – scheduler state. Should be an object returned
from a call to <a class="reference internal" href="#torch.optim.lr_scheduler.LambdaLR.state_dict" title="torch.optim.lr_scheduler.LambdaLR.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.optim.lr_scheduler.LambdaLR.state_dict">
<code class="descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.lr_scheduler.LambdaLR.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state of the scheduler as a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p>
<p>It contains an entry for every variable in self.__dict__ which
is not the optimizer.
The learning rate lambda functions will only be saved if they are callable objects
and not if they are functions or lambdas.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.lr_scheduler.StepLR">
<em class="property">class </em><code class="descclassname">torch.optim.lr_scheduler.</code><code class="descname">StepLR</code><span class="sig-paren">(</span><em>optimizer</em>, <em>step_size</em>, <em>gamma=0.1</em>, <em>last_epoch=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.lr_scheduler.StepLR" title="Permalink to this definition">¶</a></dt>
<dd><p>Decays the learning rate of each parameter group by gamma every
step_size epochs. Notice that such decay can happen simultaneously with
other changes to the learning rate from outside this scheduler. When
last_epoch=-1, sets initial lr as lr.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<a class="reference internal" href="index.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a>) – Wrapped optimizer.</p></li>
<li><p><strong>step_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Period of learning rate decay.</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Multiplicative factor of learning rate decay.
Default: 0.1.</p></li>
<li><p><strong>last_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The index of last epoch. Default: -1.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming optimizer uses lr = 0.05 for all groups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lr = 0.05     if epoch &lt; 30</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lr = 0.005    if 30 &lt;= epoch &lt; 60</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lr = 0.0005   if 60 &lt;= epoch &lt; 90</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="torch.optim.lr_scheduler.MultiStepLR">
<em class="property">class </em><code class="descclassname">torch.optim.lr_scheduler.</code><code class="descname">MultiStepLR</code><span class="sig-paren">(</span><em>optimizer</em>, <em>milestones</em>, <em>gamma=0.1</em>, <em>last_epoch=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.lr_scheduler.MultiStepLR" title="Permalink to this definition">¶</a></dt>
<dd><p>Decays the learning rate of each parameter group by gamma once the
number of epoch reaches one of the milestones. Notice that such decay can
happen simultaneously with other changes to the learning rate from outside
this scheduler. When last_epoch=-1, sets initial lr as lr.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<a class="reference internal" href="index.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a>) – Wrapped optimizer.</p></li>
<li><p><strong>milestones</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – List of epoch indices. Must be increasing.</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Multiplicative factor of learning rate decay.
Default: 0.1.</p></li>
<li><p><strong>last_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The index of last epoch. Default: -1.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Assuming optimizer uses lr = 0.05 for all groups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lr = 0.05     if epoch &lt; 30</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lr = 0.005    if 30 &lt;= epoch &lt; 80</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lr = 0.0005   if epoch &gt;= 80</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">30</span><span class="p">,</span><span class="mi">80</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="torch.optim.lr_scheduler.ExponentialLR">
<em class="property">class </em><code class="descclassname">torch.optim.lr_scheduler.</code><code class="descname">ExponentialLR</code><span class="sig-paren">(</span><em>optimizer</em>, <em>gamma</em>, <em>last_epoch=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.lr_scheduler.ExponentialLR" title="Permalink to this definition">¶</a></dt>
<dd><p>Decays the learning rate of each parameter group by gamma every epoch.
When last_epoch=-1, sets initial lr as lr.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<a class="reference internal" href="index.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a>) – Wrapped optimizer.</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Multiplicative factor of learning rate decay.</p></li>
<li><p><strong>last_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The index of last epoch. Default: -1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.optim.lr_scheduler.CosineAnnealingLR">
<em class="property">class </em><code class="descclassname">torch.optim.lr_scheduler.</code><code class="descname">CosineAnnealingLR</code><span class="sig-paren">(</span><em>optimizer</em>, <em>T_max</em>, <em>eta_min=0</em>, <em>last_epoch=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.lr_scheduler.CosineAnnealingLR" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the learning rate of each parameter group using a cosine annealing
schedule, where <span class="math">\(\eta_{max}\)</span> is set to the initial lr and
<span class="math">\(T_{cur}\)</span> is the number of epochs since the last restart in SGDR:</p>
<div class="math">
\[\eta_{t+1} = \eta_{min} + (\eta_t - \eta_{min})\frac{1 +
\cos(\frac{T_{cur+1}}{T_{max}}\pi)}{1 + \cos(\frac{T_{cur}}{T_{max}}\pi)}

\]</div>
<p>When last_epoch=-1, sets initial lr as lr. Notice that because the schedule
is defined recursively, the learning rate can be simultaneously modified
outside this scheduler by other operators. If the learning rate is set
solely by this scheduler, the learning rate at each step becomes:</p>
<div class="math">
\[\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 +
\cos(\frac{T_{cur}}{T_{max}}\pi))

\]</div>
<p>It has been proposed in
<a class="reference external" href="https://arxiv.org/abs/1608.03983">SGDR: Stochastic Gradient Descent with Warm Restarts</a>. Note that this only
implements the cosine annealing part of SGDR, and not the restarts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<a class="reference internal" href="index.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a>) – Wrapped optimizer.</p></li>
<li><p><strong>T_max</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Maximum number of iterations.</p></li>
<li><p><strong>eta_min</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Minimum learning rate. Default: 0.</p></li>
<li><p><strong>last_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The index of last epoch. Default: -1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.optim.lr_scheduler.ReduceLROnPlateau">
<em class="property">class </em><code class="descclassname">torch.optim.lr_scheduler.</code><code class="descname">ReduceLROnPlateau</code><span class="sig-paren">(</span><em>optimizer</em>, <em>mode='min'</em>, <em>factor=0.1</em>, <em>patience=10</em>, <em>verbose=False</em>, <em>threshold=0.0001</em>, <em>threshold_mode='rel'</em>, <em>cooldown=0</em>, <em>min_lr=0</em>, <em>eps=1e-08</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.lr_scheduler.ReduceLROnPlateau" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduce learning rate when a metric has stopped improving.
Models often benefit from reducing the learning rate by a factor
of 2-10 once learning stagnates. This scheduler reads a metrics
quantity and if no improvement is seen for a ‘patience’ number
of epochs, the learning rate is reduced.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<a class="reference internal" href="index.html#torch.optim.Optimizer" title="torch.optim.Optimizer"><em>Optimizer</em></a>) – Wrapped optimizer.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – One of <cite>min</cite>, <cite>max</cite>. In <cite>min</cite> mode, lr will
be reduced when the quantity monitored has stopped
decreasing; in <cite>max</cite> mode it will be reduced when the
quantity monitored has stopped increasing. Default: ‘min’.</p></li>
<li><p><strong>factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Factor by which the learning rate will be
reduced. new_lr = lr * factor. Default: 0.1.</p></li>
<li><p><strong>patience</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of epochs with no improvement after
which learning rate will be reduced. For example, if
<cite>patience = 2</cite>, then we will ignore the first 2 epochs
with no improvement, and will only decrease the LR after the
3rd epoch if the loss still hasn’t improved then.
Default: 10.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, prints a message to stdout for
each update. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>threshold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Threshold for measuring the new optimum,
to only focus on significant changes. Default: 1e-4.</p></li>
<li><p><strong>threshold_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – One of <cite>rel</cite>, <cite>abs</cite>. In <cite>rel</cite> mode,
dynamic_threshold = best * ( 1 + threshold ) in ‘max’
mode or best * ( 1 - threshold ) in <cite>min</cite> mode.
In <cite>abs</cite> mode, dynamic_threshold = best + threshold in
<cite>max</cite> mode or best - threshold in <cite>min</cite> mode. Default: ‘rel’.</p></li>
<li><p><strong>cooldown</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of epochs to wait before resuming
normal operation after lr has been reduced. Default: 0.</p></li>
<li><p><strong>min_lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – A scalar or a list of scalars. A
lower bound on the learning rate of all param groups
or each group respectively. Default: 0.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Minimal decay applied to lr. If the difference
between new and old lr is smaller than eps, the update is
ignored. Default: 1e-8.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="s1">&#39;min&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Note that step should be called after validate()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<span id="document-autograd"></span><div class="section" id="module-torch.autograd">
<span id="automatic-differentiation-package-torch-autograd"></span><h2>Automatic differentiation package - torch.autograd<a class="headerlink" href="#module-torch.autograd" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code> provides classes and functions implementing automatic
differentiation of arbitrary scalar valued functions. It requires minimal
changes to the existing code - you only need to declare <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> s
for which gradients should be computed with the <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> keyword.</p>
<dl class="function">
<dt id="torch.autograd.backward">
<code class="descclassname">torch.autograd.</code><code class="descname">backward</code><span class="sig-paren">(</span><em>tensors</em>, <em>grad_tensors=None</em>, <em>retain_graph=None</em>, <em>create_graph=False</em>, <em>grad_variables=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sum of gradients of given tensors w.r.t. graph leaves.</p>
<p>The graph is differentiated using the chain rule. If any of <code class="docutils literal notranslate"><span class="pre">tensors</span></code>
are non-scalar (i.e. their data has more than one element) and require
gradient, then the Jacobian-vector product would be computed, in this
case the function additionally requires specifying <code class="docutils literal notranslate"><span class="pre">grad_tensors</span></code>.
It should be a sequence of matching length, that contains the “vector”
in the Jacobian-vector product, usually the gradient of the differentiated
function w.r.t. corresponding tensors (<code class="docutils literal notranslate"><span class="pre">None</span></code> is an acceptable value for
all tensors that don’t need gradient tensors).</p>
<p>This function accumulates gradients in the leaves - you might need to zero
them before calling it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> (<em>sequence of Tensor</em>) – Tensors of which the derivative will be
computed.</p></li>
<li><p><strong>grad_tensors</strong> (<em>sequence of</em><em> (</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.7)"><em>None</em></a><em>)</em>) – The “vector” in the Jacobian-vector
product, usually gradients w.r.t. each element of corresponding tensors.
None values can be specified for scalar Tensors or ones that don’t require
grad. If a None value would be acceptable for all grad_tensors, then this
argument is optional.</p></li>
<li><p><strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the graph used to compute the grad
will be freed. Note that in nearly all cases setting this option to <code class="docutils literal notranslate"><span class="pre">True</span></code>
is not needed and often can be worked around in a much more efficient
way. Defaults to the value of <code class="docutils literal notranslate"><span class="pre">create_graph</span></code>.</p></li>
<li><p><strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, graph of the derivative will
be constructed, allowing to compute higher order derivative products.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.autograd.grad">
<code class="descclassname">torch.autograd.</code><code class="descname">grad</code><span class="sig-paren">(</span><em>outputs</em>, <em>inputs</em>, <em>grad_outputs=None</em>, <em>retain_graph=None</em>, <em>create_graph=False</em>, <em>only_inputs=True</em>, <em>allow_unused=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes and returns the sum of gradients of outputs w.r.t. the inputs.</p>
<p><code class="docutils literal notranslate"><span class="pre">grad_outputs</span></code> should be a sequence of length matching <code class="docutils literal notranslate"><span class="pre">output</span></code>
containing the “vector” in Jacobian-vector product, usually the pre-computed
gradients w.r.t. each of the outputs. If an output doesn’t require_grad,
then the gradient can be <code class="docutils literal notranslate"><span class="pre">None</span></code>).</p>
<p>If <code class="docutils literal notranslate"><span class="pre">only_inputs</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the function will only return a list of gradients
w.r.t the specified inputs. If it’s <code class="docutils literal notranslate"><span class="pre">False</span></code>, then gradient w.r.t. all remaining
leaves will still be computed, and will be accumulated into their <code class="docutils literal notranslate"><span class="pre">.grad</span></code>
attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<em>sequence of Tensor</em>) – outputs of the differentiated function.</p></li>
<li><p><strong>inputs</strong> (<em>sequence of Tensor</em>) – Inputs w.r.t. which the gradient will be
returned (and not accumulated into <code class="docutils literal notranslate"><span class="pre">.grad</span></code>).</p></li>
<li><p><strong>grad_outputs</strong> (<em>sequence of Tensor</em>) – The “vector” in the Jacobian-vector product.
Usually gradients w.r.t. each output. None values can be specified for scalar
Tensors or ones that don’t require grad. If a None value would be acceptable
for all grad_tensors, then this argument is optional. Default: None.</p></li>
<li><p><strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the graph used to compute the grad
will be freed. Note that in nearly all cases setting this option to <code class="docutils literal notranslate"><span class="pre">True</span></code>
is not needed and often can be worked around in a much more efficient
way. Defaults to the value of <code class="docutils literal notranslate"><span class="pre">create_graph</span></code>.</p></li>
<li><p><strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, graph of the derivative will
be constructed, allowing to compute higher order derivative products.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>allow_unused</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, specifying inputs that were not
used when computing outputs (and therefore their grad is always zero)
is an error. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<div class="section" id="locally-disabling-gradient-computation">
<span id="locally-disable-grad"></span><h3>Locally disabling gradient computation<a class="headerlink" href="#locally-disabling-gradient-computation" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.autograd.no_grad">
<em class="property">class </em><code class="descclassname">torch.autograd.</code><code class="descname">no_grad</code><a class="headerlink" href="#torch.autograd.no_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that disabled gradient calculation.</p>
<p>Disabling gradient calculation is useful for inference, when you are sure
that you will not call <code class="xref py py-meth docutils literal notranslate"><span class="pre">Tensor.backward()</span></code>. It will reduce memory
consumption for computations that would otherwise have <cite>requires_grad=True</cite>.
In this mode, the result of every computation will have
<cite>requires_grad=False</cite>, even when the inputs have <cite>requires_grad=True</cite>.</p>
<p>Also functions as a decorator.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">... </span>  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">doubler</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">doubler</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="torch.autograd.enable_grad">
<em class="property">class </em><code class="descclassname">torch.autograd.</code><code class="descname">enable_grad</code><a class="headerlink" href="#torch.autograd.enable_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that enables gradient calculation.</p>
<p>Enables gradient calculation inside a <a class="reference internal" href="#torch.autograd.no_grad" title="torch.autograd.no_grad"><code class="xref py py-class docutils literal notranslate"><span class="pre">no_grad</span></code></a> context. This has
no effect outside of <a class="reference internal" href="#torch.autograd.no_grad" title="torch.autograd.no_grad"><code class="xref py py-class docutils literal notranslate"><span class="pre">no_grad</span></code></a>.</p>
<p>Also functions as a decorator.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">... </span>  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">()</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">doubler</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">z</span> <span class="o">=</span> <span class="n">doubler</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="torch.autograd.set_grad_enabled">
<em class="property">class </em><code class="descclassname">torch.autograd.</code><code class="descname">set_grad_enabled</code><span class="sig-paren">(</span><em>mode</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.set_grad_enabled" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that sets gradient calculation to on or off.</p>
<p><code class="docutils literal notranslate"><span class="pre">set_grad_enabled</span></code> will enable or disable grads based on its argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
It can be used as a context-manager or as a function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – Flag whether to enable grad (<code class="docutils literal notranslate"><span class="pre">True</span></code>), or disable
(<code class="docutils literal notranslate"><span class="pre">False</span></code>). This can be used to conditionally enable
gradients.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_train</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">is_train</span><span class="p">):</span>
<span class="gp">... </span>  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="in-place-operations-on-tensors">
<h3>In-place operations on Tensors<a class="headerlink" href="#in-place-operations-on-tensors" title="Permalink to this headline">¶</a></h3>
<p>Supporting in-place operations in autograd is a hard matter, and we discourage
their use in most cases. Autograd’s aggressive buffer freeing and reuse makes
it very efficient and there are very few occasions when in-place operations
actually lower memory usage by any significant amount. Unless you’re operating
under heavy memory pressure, you might never need to use them.</p>
<div class="section" id="in-place-correctness-checks">
<h4>In-place correctness checks<a class="headerlink" href="#in-place-correctness-checks" title="Permalink to this headline">¶</a></h4>
<p>All <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> s keep track of in-place operations applied to them, and
if the implementation detects that a tensor was saved for backward in one of
the functions, but it was modified in-place afterwards, an error will be raised
once backward pass is started. This ensures that if you’re using in-place
functions and not seeing any errors, you can be sure that the computed
gradients are correct.</p>
</div>
</div>
<div class="section" id="variable-deprecated">
<h3>Variable (deprecated)<a class="headerlink" href="#variable-deprecated" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The Variable API has been deprecated: Variables are no longer necessary to
use autograd with tensors. Autograd automatically supports Tensors with
<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> set to <code class="docutils literal notranslate"><span class="pre">True</span></code>. Below please find a quick guide on what
has changed:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Variable(tensor)</span></code> and <code class="docutils literal notranslate"><span class="pre">Variable(tensor,</span> <span class="pre">requires_grad)</span></code> still work as expected,
but they return Tensors instead of Variables.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">var.data</span></code> is the same thing as <code class="docutils literal notranslate"><span class="pre">tensor.data</span></code>.</p></li>
<li><p>Methods such as <code class="docutils literal notranslate"><span class="pre">var.backward(),</span> <span class="pre">var.detach(),</span> <span class="pre">var.register_hook()</span></code> now work on tensors
with the same method names.</p></li>
</ul>
<p>In addition, one can now create tensors with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> using factory
methods such as <a class="reference internal" href="index.html#torch.randn" title="torch.randn"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.randn()</span></code></a>, <a class="reference internal" href="index.html#torch.zeros" title="torch.zeros"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.zeros()</span></code></a>, <a class="reference internal" href="index.html#torch.ones" title="torch.ones"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.ones()</span></code></a>, and others
like the following:</p>
<p><code class="docutils literal notranslate"><span class="pre">autograd_tensor</span> <span class="pre">=</span> <span class="pre">torch.randn((2,</span> <span class="pre">3,</span> <span class="pre">4),</span> <span class="pre">requires_grad=True)</span></code></p>
</div>
</div>
<div class="section" id="tensor-autograd-functions">
<h3>Tensor autograd functions<a class="headerlink" href="#tensor-autograd-functions" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.Tensor">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><a class="headerlink" href="#torch.Tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="torch.Tensor.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>gradient=None</em>, <em>retain_graph=None</em>, <em>create_graph=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the gradient of current tensor w.r.t. graph leaves.</p>
<p>The graph is differentiated using the chain rule. If the tensor is
non-scalar (i.e. its data has more than one element) and requires
gradient, the function additionally requires specifying <code class="docutils literal notranslate"><span class="pre">gradient</span></code>.
It should be a tensor of matching type and location, that contains
the gradient of the differentiated function w.r.t. <code class="docutils literal notranslate"><span class="pre">self</span></code>.</p>
<p>This function accumulates gradients in the leaves - you might need to
zero them before calling it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gradient</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.7)"><em>None</em></a>) – Gradient w.r.t. the
tensor. If it is a tensor, it will be automatically converted
to a Tensor that does not require grad unless <code class="docutils literal notranslate"><span class="pre">create_graph</span></code> is True.
None values can be specified for scalar Tensors or ones that
don’t require grad. If a None value would be acceptable then
this argument is optional.</p></li>
<li><p><strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the graph used to compute
the grads will be freed. Note that in nearly all cases setting
this option to True is not needed and often can be worked around
in a much more efficient way. Defaults to the value of
<code class="docutils literal notranslate"><span class="pre">create_graph</span></code>.</p></li>
<li><p><strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, graph of the derivative will
be constructed, allowing to compute higher order derivative
products. Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.detach">
<code class="descname">detach</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.detach" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Tensor, detached from the current graph.</p>
<p>The result will never require gradient.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Returned Tensor shares the same storage with the original one.
In-place modifications on either of them will be seen, and may trigger
errors in correctness checks.
IMPORTANT NOTE: Previously, in-place size / stride / storage changes
(such as <cite>resize_</cite> / <cite>resize_as_</cite> / <cite>set_</cite> / <cite>transpose_</cite>) to the returned tensor
also update the original tensor. Now, these in-place changes will not update the
original tensor anymore, and will instead trigger an error.
For sparse tensors:
In-place indices / values changes (such as <cite>zero_</cite> / <cite>copy_</cite> / <cite>add_</cite>) to the
returned tensor will not update the original tensor anymore, and will instead
trigger an error.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.detach_">
<code class="descname">detach_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.detach_" title="Permalink to this definition">¶</a></dt>
<dd><p>Detaches the Tensor from the graph that created it, making it a leaf.
Views cannot be detached in-place.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.grad">
<code class="descname">grad</code><a class="headerlink" href="#torch.Tensor.grad" title="Permalink to this definition">¶</a></dt>
<dd><p>This attribute is <code class="docutils literal notranslate"><span class="pre">None</span></code> by default and becomes a Tensor the first time a call to
<a class="reference internal" href="#torch.Tensor.backward" title="torch.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> computes gradients for <code class="docutils literal notranslate"><span class="pre">self</span></code>.
The attribute will then contain the gradients computed and future calls to
<a class="reference internal" href="#torch.Tensor.backward" title="torch.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will accumulate (add) gradients into it.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.is_leaf">
<code class="descname">is_leaf</code><a class="headerlink" href="#torch.Tensor.is_leaf" title="Permalink to this definition">¶</a></dt>
<dd><p>All Tensors that have <a class="reference internal" href="#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">False</span></code> will be leaf Tensors by convention.</p>
<p>For Tensors that have <a class="reference internal" href="#torch.Tensor.requires_grad" title="torch.Tensor.requires_grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code></a> which is <code class="docutils literal notranslate"><span class="pre">True</span></code>, they will be leaf Tensors if they were
created by the user. This means that they are not the result of an operation and so
<code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_fn</span></code> is None.</p>
<p>Only leaf Tensors will have their <a class="reference internal" href="#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated during a call to <a class="reference internal" href="#torch.Tensor.backward" title="torch.Tensor.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a>.
To get <a class="reference internal" href="#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a> populated for non-leaf Tensors, you can use <a class="reference internal" href="#torch.Tensor.retain_grad" title="torch.Tensor.retain_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">retain_grad()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="go"># b was created by the operation that cast a cpu Tensor into a cuda Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="go"># c was created by the addition operation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># e requires gradients and has no operations creating it</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
<span class="go"># f requires grad, has not operation creating it</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.register_hook">
<code class="descname">register_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.register_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook.</p>
<p>The hook will be called every time a gradient with respect to the
Tensor is computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify its argument, but it can optionally return
a new gradient which will be used in place of <a class="reference internal" href="#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a>.</p>
<p>This function returns a handle with a method <code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="k">lambda</span> <span class="n">grad</span><span class="p">:</span> <span class="n">grad</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># double the gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">grad</span>

<span class="go"> 2</span>
<span class="go"> 4</span>
<span class="go"> 6</span>
<span class="go">[torch.FloatTensor of size (3,)]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># removes the hook</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.requires_grad">
<code class="descname">requires_grad</code><a class="headerlink" href="#torch.Tensor.requires_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Is <code class="docutils literal notranslate"><span class="pre">True</span></code> if gradients need to be computed for this Tensor, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The fact that gradients need to be computed for a Tensor do not mean that the <a class="reference internal" href="#torch.Tensor.grad" title="torch.Tensor.grad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">grad</span></code></a>
attribute will be populated, see <a class="reference internal" href="#torch.Tensor.is_leaf" title="torch.Tensor.is_leaf"><code class="xref py py-attr docutils literal notranslate"><span class="pre">is_leaf</span></code></a> for more details.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.retain_grad">
<code class="descname">retain_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.retain_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables .grad attribute for non-leaf Tensors.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="function">
<h3><span class="hidden-section">Function</span><a class="headerlink" href="#function" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.autograd.Function">
<em class="property">class </em><code class="descclassname">torch.autograd.</code><code class="descname">Function</code><a class="headerlink" href="#torch.autograd.Function" title="Permalink to this definition">¶</a></dt>
<dd><p>Records operation history and defines formulas for differentiating ops.</p>
<p>Every operation performed on <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> s creates a new function
object, that performs the computation, and records that it happened.
The history is retained in the form of a DAG of functions, with edges
denoting data dependencies (<code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">&lt;-</span> <span class="pre">output</span></code>). Then, when backward is
called, the graph is processed in the topological ordering, by calling
<a class="reference internal" href="#torch.autograd.backward" title="torch.autograd.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> methods of each <a class="reference internal" href="#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> object, and passing
returned gradients on to next <a class="reference internal" href="#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> s.</p>
<p>Normally, the only way users interact with functions is by creating
subclasses and defining new operations. This is a recommended way of
extending torch.autograd.</p>
<p>Each function object is meant to be used only once (in the forward pass).</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">result</span> <span class="o">=</span> <span class="n">i</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">result</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">result</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">result</span>
</pre></div>
</div>
<dl class="staticmethod">
<dt id="torch.autograd.Function.backward">
<em class="property">static </em><code class="descname">backward</code><span class="sig-paren">(</span><em>ctx</em>, <em>*grad_outputs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Function.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs did <a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> return, and it should return as many
tensors, as there were inputs to <a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the
gradient w.r.t the given output, and each returned value should be the
gradient w.r.t. the corresponding input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#torch.autograd.backward" title="torch.autograd.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="torch.autograd.Function.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>ctx</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Function.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="numerical-gradient-checking">
<span id="grad-check"></span><h3>Numerical gradient checking<a class="headerlink" href="#numerical-gradient-checking" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.autograd.gradcheck">
<code class="descclassname">torch.autograd.</code><code class="descname">gradcheck</code><span class="sig-paren">(</span><em>func</em>, <em>inputs</em>, <em>eps=1e-06</em>, <em>atol=1e-05</em>, <em>rtol=0.001</em>, <em>raise_exception=True</em>, <em>check_sparse_nnz=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.gradcheck" title="Permalink to this definition">¶</a></dt>
<dd><p>Check gradients computed via small finite differences against analytical
gradients w.r.t. tensors in <code class="xref py py-attr docutils literal notranslate"><span class="pre">inputs</span></code> that are of floating point type
and with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>.</p>
<p>The check between numerical and analytical gradients uses <a class="reference internal" href="index.html#torch.allclose" title="torch.allclose"><code class="xref py py-func docutils literal notranslate"><span class="pre">allclose()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The default values are designed for <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of double precision.
This check will likely fail if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is of less precision, e.g.,
<code class="docutils literal notranslate"><span class="pre">FloatTensor</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If any checked tensor in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> has overlapping memory, i.e.,
different indices pointing to the same memory address (e.g., from
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expand()</span></code>), this check will likely fail because the numerical
gradients computed by point perturbation at such indices will change
values at all other indices that share the same memory address.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns
a Tensor or a tuple of Tensors</p></li>
<li><p><strong>inputs</strong> (<em>tuple of Tensor</em><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – inputs to the function</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – perturbation for finite differences</p></li>
<li><p><strong>atol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – absolute tolerance</p></li>
<li><p><strong>rtol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – relative tolerance</p></li>
<li><p><strong>raise_exception</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – indicating whether to raise an exception if
the check fails. The exception gives more information about the
exact nature of the failure. This is helpful when debugging gradchecks.</p></li>
<li><p><strong>check_sparse_nnz</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if True, gradcheck allows for SparseTensor input,
and for any SparseTensor at input, gradcheck will perform check at nnz positions only.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if all differences satisfy allclose condition</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.autograd.gradgradcheck">
<code class="descclassname">torch.autograd.</code><code class="descname">gradgradcheck</code><span class="sig-paren">(</span><em>func</em>, <em>inputs</em>, <em>grad_outputs=None</em>, <em>eps=1e-06</em>, <em>atol=1e-05</em>, <em>rtol=0.001</em>, <em>gen_non_contig_grad_outputs=False</em>, <em>raise_exception=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.gradgradcheck" title="Permalink to this definition">¶</a></dt>
<dd><p>Check gradients of gradients computed via small finite differences
against analytical gradients w.r.t. tensors in <code class="xref py py-attr docutils literal notranslate"><span class="pre">inputs</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_outputs</span></code> that are of floating point type and with
<code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>.</p>
<p>This function checks that backpropagating through the gradients computed
to the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_outputs</span></code> are correct.</p>
<p>The check between numerical and analytical gradients uses <a class="reference internal" href="index.html#torch.allclose" title="torch.allclose"><code class="xref py py-func docutils literal notranslate"><span class="pre">allclose()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The default values are designed for <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_outputs</span></code> of double precision. This check will likely fail if
they are of less precision, e.g., <code class="docutils literal notranslate"><span class="pre">FloatTensor</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If any checked tensor in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_outputs</span></code> has
overlapping memory, i.e., different indices pointing to the same memory
address (e.g., from <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.expand()</span></code>), this check will likely fail
because the numerical gradients computed by point perturbation at such
indices will change values at all other indices that share the same
memory address.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns
a Tensor or a tuple of Tensors</p></li>
<li><p><strong>inputs</strong> (<em>tuple of Tensor</em><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – inputs to the function</p></li>
<li><p><strong>grad_outputs</strong> (<em>tuple of Tensor</em><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – The gradients with
respect to the function’s outputs.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – perturbation for finite differences</p></li>
<li><p><strong>atol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – absolute tolerance</p></li>
<li><p><strong>rtol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – relative tolerance</p></li>
<li><p><strong>gen_non_contig_grad_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_outputs</span></code> is
<code class="docutils literal notranslate"><span class="pre">None</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">gen_non_contig_grad_outputs</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the
randomly generated gradient outputs are made to be noncontiguous</p></li>
<li><p><strong>raise_exception</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – indicating whether to raise an exception if
the check fails. The exception gives more information about the
exact nature of the failure. This is helpful when debugging gradchecks.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if all differences satisfy allclose condition</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="profiler">
<h3>Profiler<a class="headerlink" href="#profiler" title="Permalink to this headline">¶</a></h3>
<p>Autograd includes a profiler that lets you inspect the cost of different
operators inside your model - both on the CPU and GPU. There are two modes
implemented at the moment - CPU-only using <a class="reference internal" href="#torch.autograd.profiler.profile" title="torch.autograd.profiler.profile"><code class="xref py py-class docutils literal notranslate"><span class="pre">profile</span></code></a>.
and nvprof based (registers both CPU and GPU activity) using
<a class="reference internal" href="#torch.autograd.profiler.emit_nvtx" title="torch.autograd.profiler.emit_nvtx"><code class="xref py py-class docutils literal notranslate"><span class="pre">emit_nvtx</span></code></a>.</p>
<dl class="class">
<dt id="torch.autograd.profiler.profile">
<em class="property">class </em><code class="descclassname">torch.autograd.profiler.</code><code class="descname">profile</code><span class="sig-paren">(</span><em>enabled=True</em>, <em>use_cuda=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.profiler.profile" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that manages autograd profiler state and holds a summary of results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>enabled</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Setting this to False makes this context manager a no-op.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>use_cuda</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Enables timing of CUDA events as well using the cudaEvent API.
Adds approximately 4us of overhead to each tensor operation.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">()</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>
<span class="gp">... </span>    <span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># NOTE: some columns were removed for brevity</span>
<span class="gp">... </span><span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="p">)</span>
<span class="go">-------------------------------------  ---------------  ---------------</span>
<span class="go">Name                                          CPU time        CUDA time</span>
<span class="go">-------------------------------------  ---------------  ---------------</span>
<span class="go">PowConstant                                  142.036us          0.000us</span>
<span class="go">N5torch8autograd9GraphRootE                   63.524us          0.000us</span>
<span class="go">PowConstantBackward                          184.228us          0.000us</span>
<span class="go">MulConstant                                   50.288us          0.000us</span>
<span class="go">PowConstant                                   28.439us          0.000us</span>
<span class="go">Mul                                           20.154us          0.000us</span>
<span class="go">N5torch8autograd14AccumulateGradE             13.790us          0.000us</span>
<span class="go">N5torch8autograd5CloneE                        4.088us          0.000us</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.autograd.profiler.profile.export_chrome_trace">
<code class="descname">export_chrome_trace</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.profiler.profile.export_chrome_trace" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports an EventList as a Chrome tracing tools file.</p>
<p>The checkpoint can be later loaded and inspected under <code class="docutils literal notranslate"><span class="pre">chrome://tracing</span></code> URL.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – Path where the trace will be written.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.profiler.profile.key_averages">
<code class="descname">key_averages</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.profiler.profile.key_averages" title="Permalink to this definition">¶</a></dt>
<dd><p>Averages all function events over their keys.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>An EventList containing FunctionEventAvg objects.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.profiler.profile.table">
<code class="descname">table</code><span class="sig-paren">(</span><em>sort_by=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.profiler.profile.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints an EventList as a nicely formatted table.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sort_by</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – Attribute used to sort entries. By default
they are printed in the same order as they were registered.
Valid keys include: <code class="docutils literal notranslate"><span class="pre">cpu_time</span></code>, <code class="docutils literal notranslate"><span class="pre">cuda_time</span></code>, <code class="docutils literal notranslate"><span class="pre">cpu_time_total</span></code>,
<code class="docutils literal notranslate"><span class="pre">cuda_time_total</span></code>, <code class="docutils literal notranslate"><span class="pre">count</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A string containing the table.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.profiler.profile.total_average">
<code class="descname">total_average</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.profiler.profile.total_average" title="Permalink to this definition">¶</a></dt>
<dd><p>Averages all events.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A FunctionEventAvg object.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.autograd.profiler.emit_nvtx">
<em class="property">class </em><code class="descclassname">torch.autograd.profiler.</code><code class="descname">emit_nvtx</code><span class="sig-paren">(</span><em>enabled=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.profiler.emit_nvtx" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that makes every autograd operation emit an NVTX range.</p>
<p>It is useful when running the program under nvprof:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nvprof</span> <span class="o">--</span><span class="n">profile</span><span class="o">-</span><span class="n">from</span><span class="o">-</span><span class="n">start</span> <span class="n">off</span> <span class="o">-</span><span class="n">o</span> <span class="n">trace_name</span><span class="o">.</span><span class="n">prof</span> <span class="o">--</span> <span class="o">&lt;</span><span class="n">regular</span> <span class="n">command</span> <span class="n">here</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Unfortunately, there’s no way to force nvprof to flush the data it collected
to disk, so for CUDA profiling one has to use this context manager to annotate
nvprof traces and wait for the process to exit before inspecting them.
Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or
<a class="reference internal" href="#torch.autograd.profiler.load_nvprof" title="torch.autograd.profiler.load_nvprof"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.profiler.load_nvprof()</span></code></a> can load the results for inspection
e.g. in Python REPL.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enabled</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Setting this to False makes this context manager a no-op.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># Warmup CUDA memory allocator and profiler</span>
<span class="gp">... </span>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">emit_nvtx</span><span class="p">():</span>
<span class="gp">... </span>        <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Forward-backward correlation</strong></p>
<p>When viewing a profile created using <a class="reference internal" href="#torch.autograd.profiler.emit_nvtx" title="torch.autograd.profiler.emit_nvtx"><code class="xref py py-class docutils literal notranslate"><span class="pre">emit_nvtx</span></code></a> in the Nvidia Visual Profiler,
correlating each backward-pass op with the corresponding forward-pass op can be difficult.
To ease this task, <a class="reference internal" href="#torch.autograd.profiler.emit_nvtx" title="torch.autograd.profiler.emit_nvtx"><code class="xref py py-class docutils literal notranslate"><span class="pre">emit_nvtx</span></code></a> appends sequence number information to the ranges it
generates.</p>
<p>During the forward pass, each function range is decorated with <code class="docutils literal notranslate"><span class="pre">seq=&lt;N&gt;</span></code>.  <code class="docutils literal notranslate"><span class="pre">seq</span></code> is a running
counter, incremented each time a new backward Function object is created and stashed for backward.
Thus, the <cite>seq=&lt;N&gt;</cite> annotation associated with each forward function range tells you that
if a backward Function object is created by this forward function,
the backward object will receive sequence number N.
During the backward pass, the top-level range wrapping each C++ backward Function’s
<code class="docutils literal notranslate"><span class="pre">apply()</span></code> call is decorated with <code class="docutils literal notranslate"><span class="pre">stashed</span> <span class="pre">seq=&lt;M&gt;</span></code>.  <code class="docutils literal notranslate"><span class="pre">M</span></code> is the sequence number that
the backward object was created with.  By comparing <code class="docutils literal notranslate"><span class="pre">stashed</span> <span class="pre">seq</span></code> numbers in backward with <code class="docutils literal notranslate"><span class="pre">seq</span></code>
numbers in forward, you can track down which forward op created each backward Function.</p>
<p>Any functions executed during the backward pass are also decorated with <code class="docutils literal notranslate"><span class="pre">seq=&lt;N&gt;</span></code>.  During
default backward (with <code class="docutils literal notranslate"><span class="pre">create_graph=False</span></code>) this information is irrelevant, and in fact,
<code class="docutils literal notranslate"><span class="pre">N</span></code> may simply be 0 for all such functions.  Only the top-level ranges associated with
backward Function objects’ <code class="docutils literal notranslate"><span class="pre">apply()</span></code> methods are useful, as a way to correlate these Function
objects with the earlier forward pass.</p>
<p><strong>Double-backward</strong></p>
<p>If, on the other hand, a backward pass with <code class="docutils literal notranslate"><span class="pre">create_graph=True</span></code> is underway (in other words,
if you are setting up for a double-backward), each function’s execution during backward
is given a nonzero, useful <code class="docutils literal notranslate"><span class="pre">seq=&lt;N&gt;</span></code>.  Those functions may themselves create Function objects
to be executed later during double-backward, just as the original functions in the forward pass did.
The relationship between backward and double-backward is conceptually the same as the relationship
between forward and backward: The functions still emit current-sequence-number-tagged ranges,
the Function objects they create still stash those sequence numbers, and during the eventual
double-backward, the Function objects’ <code class="docutils literal notranslate"><span class="pre">apply()</span></code> ranges are still tagged with <code class="docutils literal notranslate"><span class="pre">stashed</span> <span class="pre">seq</span></code>
numbers, which can be compared to <cite>seq</cite> numbers from the backward pass.</p>
</dd></dl>

<dl class="function">
<dt id="torch.autograd.profiler.load_nvprof">
<code class="descclassname">torch.autograd.profiler.</code><code class="descname">load_nvprof</code><span class="sig-paren">(</span><em>path</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.profiler.load_nvprof" title="Permalink to this definition">¶</a></dt>
<dd><p>Opens an nvprof trace file and parses autograd annotations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – path to nvprof trace</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="anomaly-detection">
<h3>Anomaly detection<a class="headerlink" href="#anomaly-detection" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.autograd.detect_anomaly">
<em class="property">class </em><code class="descclassname">torch.autograd.</code><code class="descname">detect_anomaly</code><a class="headerlink" href="#torch.autograd.detect_anomaly" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that enable anomaly detection for the autograd engine.</p>
<p>This does two things:
- Running the forward pass with detection enabled will allow the backward
pass to print the traceback of the forward operation that created the failing
backward function.
- Any backward computation that generate “nan” value will raise an error.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch</span> <span class="k">import</span> <span class="n">autograd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyFunc</span><span class="p">(</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">... </span>    <span class="nd">@staticmethod</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">inp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">... </span>    <span class="nd">@staticmethod</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">gO</span><span class="p">):</span>
<span class="gp">... </span>        <span class="c1"># Error during the backward pass</span>
<span class="gp">... </span>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Some error in backward&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">gO</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">run_fn</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">out</span> <span class="o">=</span> <span class="n">MyFunc</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">run_fn</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="go">    Traceback (most recent call last):</span>
<span class="go">      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span>
<span class="go">      File &quot;/your/pytorch/install/torch/tensor.py&quot;, line 93, in backward</span>
<span class="go">        torch.autograd.backward(self, gradient, retain_graph, create_graph)</span>
<span class="go">      File &quot;/your/pytorch/install/torch/autograd/__init__.py&quot;, line 90, in backward</span>
<span class="go">        allow_unreachable=True)  # allow_unreachable flag</span>
<span class="go">      File &quot;/your/pytorch/install/torch/autograd/function.py&quot;, line 76, in apply</span>
<span class="go">        return self._forward_cls.backward(self, *args)</span>
<span class="go">      File &quot;&lt;stdin&gt;&quot;, line 8, in backward</span>
<span class="go">    RuntimeError: Some error in backward</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">autograd</span><span class="o">.</span><span class="n">detect_anomaly</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">out</span> <span class="o">=</span> <span class="n">run_fn</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="go">    Traceback of forward call that caused the error:</span>
<span class="go">      File &quot;tmp.py&quot;, line 53, in &lt;module&gt;</span>
<span class="go">        out = run_fn(inp)</span>
<span class="go">      File &quot;tmp.py&quot;, line 44, in run_fn</span>
<span class="go">        out = MyFunc.apply(a)</span>
<span class="go">    Traceback (most recent call last):</span>
<span class="go">      File &quot;&lt;stdin&gt;&quot;, line 4, in &lt;module&gt;</span>
<span class="go">      File &quot;/your/pytorch/install/torch/tensor.py&quot;, line 93, in backward</span>
<span class="go">        torch.autograd.backward(self, gradient, retain_graph, create_graph)</span>
<span class="go">      File &quot;/your/pytorch/install/torch/autograd/__init__.py&quot;, line 90, in backward</span>
<span class="go">        allow_unreachable=True)  # allow_unreachable flag</span>
<span class="go">      File &quot;/your/pytorch/install/torch/autograd/function.py&quot;, line 76, in apply</span>
<span class="go">        return self._forward_cls.backward(self, *args)</span>
<span class="go">      File &quot;&lt;stdin&gt;&quot;, line 8, in backward</span>
<span class="go">    RuntimeError: Some error in backward</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="torch.autograd.set_detect_anomaly">
<em class="property">class </em><code class="descclassname">torch.autograd.</code><code class="descname">set_detect_anomaly</code><span class="sig-paren">(</span><em>mode</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.set_detect_anomaly" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that sets the anomaly detection for the autograd engine on or off.</p>
<p><code class="docutils literal notranslate"><span class="pre">set_detect_anomaly</span></code> will enable or disable the autograd anomaly detection
based on its argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
It can be used as a context-manager or as a function.</p>
<p>See <code class="docutils literal notranslate"><span class="pre">detect_anomaly</span></code> above for details of the anomaly detection behaviour.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – Flag whether to enable anomaly detection (<code class="docutils literal notranslate"><span class="pre">True</span></code>),
or disable (<code class="docutils literal notranslate"><span class="pre">False</span></code>).</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<span id="document-distributed"></span><div class="section" id="module-torch.distributed">
<span id="distributed-communication-package-torch-distributed"></span><h2>Distributed communication package - torch.distributed<a class="headerlink" href="#module-torch.distributed" title="Permalink to this headline">¶</a></h2>
<div class="section" id="backends">
<h3>Backends<a class="headerlink" href="#backends" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> supports three backends, each with
different capabilities. The table below shows which functions are available
for use with CPU / CUDA tensors.
MPI supports CUDA only if the implementation used to build PyTorch supports it.</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 29%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Backend</p></th>
<th class="head" colspan="2"><p><code class="docutils literal notranslate"><span class="pre">gloo</span></code></p></th>
<th class="head" colspan="2"><p><code class="docutils literal notranslate"><span class="pre">mpi</span></code></p></th>
<th class="head" colspan="2"><p><code class="docutils literal notranslate"><span class="pre">nccl</span></code></p></th>
</tr>
<tr class="row-even"><th class="head"><p>Device</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>send</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
<tr class="row-even"><td><p>recv</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
<tr class="row-odd"><td><p>broadcast</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>all_reduce</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>reduce</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>all_gather</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>gather</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
<tr class="row-even"><td><p>scatter</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
<tr class="row-odd"><td><p>barrier</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
</tbody>
</table>
<div class="section" id="backends-that-come-with-pytorch">
<h4>Backends that come with PyTorch<a class="headerlink" href="#backends-that-come-with-pytorch" title="Permalink to this headline">¶</a></h4>
<p>PyTorch distributed currently only supports Linux. By default, the Gloo and NCCL backends
are built and included in PyTorch distributed (NCCL only when building with CUDA).
MPI is an
optional backend that can only be included if you build PyTorch from source. (e.g.
building PyTorch on a host that has MPI installed.)</p>
</div>
<div class="section" id="which-backend-to-use">
<h4>Which backend to use?<a class="headerlink" href="#which-backend-to-use" title="Permalink to this headline">¶</a></h4>
<p>In the past, we were often asked: “which backend should I use?”.</p>
<ul class="simple">
<li><p>Rule of thumb</p>
<ul>
<li><p>Use the NCCL backend for distributed <strong>GPU</strong> training</p></li>
<li><p>Use the Gloo backend for distributed <strong>CPU</strong> training.</p></li>
</ul>
</li>
<li><p>GPU hosts with InfiniBand interconnect</p>
<ul>
<li><p>Use NCCL, since it’s the only backend that currently supports
InfiniBand and GPUDirect.</p></li>
</ul>
</li>
<li><p>GPU hosts with Ethernet interconnect</p>
<ul>
<li><p>Use NCCL, since it currently provides the best distributed GPU
training performance, especially for multiprocess single-node or
multi-node distributed training. If you encounter any problem with
NCCL, use Gloo as the fallback option. (Note that Gloo currently
runs slower than NCCL for GPUs.)</p></li>
</ul>
</li>
<li><p>CPU hosts with InfiniBand interconnect</p>
<ul>
<li><p>If your InfiniBand has enabled IP over IB, use Gloo, otherwise,
use MPI instead. We are planning on adding InfiniBand support for
Gloo in the upcoming releases.</p></li>
</ul>
</li>
<li><p>CPU hosts with Ethernet interconnect</p>
<ul>
<li><p>Use Gloo, unless you have specific reasons to use MPI.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="common-environment-variables">
<h4>Common environment variables<a class="headerlink" href="#common-environment-variables" title="Permalink to this headline">¶</a></h4>
<div class="section" id="choosing-the-network-interface-to-use">
<h5>Choosing the network interface to use<a class="headerlink" href="#choosing-the-network-interface-to-use" title="Permalink to this headline">¶</a></h5>
<p>By default, both NCCL and Gloo
backends will try to find the network interface to use for communication. However, this
is not always guaranteed to be successful from our experiences. Therefore, if you
encounter any problem on either backend not being able to find the correct network
interface. You can try to set the following environment variables (each one
applicable to its respective backend):</p>
<ul class="simple">
<li><p><strong>NCCL_SOCKET_IFNAME</strong>, for example <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">NCCL_SOCKET_IFNAME=eth0</span></code></p></li>
<li><p><strong>GLOO_SOCKET_IFNAME</strong>, for example <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLOO_SOCKET_IFNAME=eth0</span></code></p></li>
</ul>
</div>
<div class="section" id="other-nccl-environment-variables">
<h5>Other NCCL environment variables<a class="headerlink" href="#other-nccl-environment-variables" title="Permalink to this headline">¶</a></h5>
<p>NCCL has also provided a number of environment variables for fine-tuning purposes.</p>
<p>Commonly used ones include the following for debugging purposes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">NCCL_DEBUG=INFO</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">NCCL_DEBUG_SUBSYS=ALL</span></code></p></li>
</ul>
<p>For the full list of NCCL environment variables, please refer to
<a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html">NVIDIA NCCL’s official documentation</a></p>
</div>
</div>
</div>
<div class="section" id="basics">
<span id="distributed-basics"></span><h3>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h3>
<p>The <cite>torch.distributed</cite> package provides PyTorch support and communication primitives
for multiprocess parallelism across several computation nodes running on one or more
machines. The class <a class="reference internal" href="index.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code></a> builds on this
functionality to provide synchronous distributed training as a wrapper around any
PyTorch model. This differs from the kinds of parallelism provided by
<a class="reference internal" href="index.html#document-multiprocessing"><span class="doc">Multiprocessing package - torch.multiprocessing</span></a> and <a class="reference internal" href="index.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.DataParallel()</span></code></a> in that it supports
multiple network-connected machines and in that the user must explicitly launch a separate
copy of the main training script for each process.</p>
<p>In the single-machine synchronous case, <cite>torch.distributed</cite> or the
<a class="reference internal" href="index.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code></a> wrapper may still have advantages over other
approaches to data-parallelism, including <a class="reference internal" href="index.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.DataParallel()</span></code></a>:</p>
<ul class="simple">
<li><p>Each process maintains its own optimizer and performs a complete optimization step with each
iteration. While this may appear redundant, since the gradients have already been gathered
together and averaged across processes and are thus the same for every process, this means
that no parameter broadcast step is needed, reducing time spent transferring tensors between
nodes.</p></li>
<li><p>Each process contains an independent Python interpreter, eliminating the extra interpreter
overhead and “GIL-thrashing” that comes from driving several execution threads, model
replicas, or GPUs from a single Python process. This is especially important for models that
make heavy use of the Python runtime, including models with recurrent layers or many small
components.</p></li>
</ul>
</div>
<div class="section" id="initialization">
<h3>Initialization<a class="headerlink" href="#initialization" title="Permalink to this headline">¶</a></h3>
<p>The package needs to be initialized using the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.init_process_group()</span></code>
function before calling any other methods. This blocks until all processes have
joined.</p>
<hr class="docutils" />
<p>Currently three initialization methods are supported:</p>
<div class="section" id="tcp-initialization">
<h4>TCP initialization<a class="headerlink" href="#tcp-initialization" title="Permalink to this headline">¶</a></h4>
<p>There are two ways to initialize using TCP, both requiring a network address
reachable from all processes and a desired <code class="docutils literal notranslate"><span class="pre">world_size</span></code>. The first way
requires specifying an address that belongs to the rank 0 process. This
initialization method requires that all processes have manually specified ranks.</p>
<p>Note that multicast address is not supported anymore in the latest distributed
package. <code class="docutils literal notranslate"><span class="pre">group_name</span></code> is deprecated as well.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="c1"># Use address of one of the machines</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;tcp://10.1.1.20:23456&#39;</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="shared-file-system-initialization">
<h4>Shared file-system initialization<a class="headerlink" href="#shared-file-system-initialization" title="Permalink to this headline">¶</a></h4>
<p>Another initialization method makes use of a file system that is shared and
visible from all machines in a group, along with a desired <code class="docutils literal notranslate"><span class="pre">world_size</span></code>. The URL should start
with <code class="docutils literal notranslate"><span class="pre">file://</span></code> and contain a path to a non-existent file (in an existing
directory) on a shared file system. File-system initialization will automatically
create that file if it doesn’t exist, but will not delete the file. Therefore, it
is your responsibility to make sure that the file is cleaned up before the next
<code class="xref py py-func docutils literal notranslate"><span class="pre">init_process_group()</span></code> call on the same file path/name.</p>
<p>Note that automatic rank assignment is not supported anymore in the latest
distributed package and <code class="docutils literal notranslate"><span class="pre">group_name</span></code> is deprecated as well.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method assumes that the file system supports locking using <code class="docutils literal notranslate"><span class="pre">fcntl</span></code> - most
local systems and NFS support it.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method will always create the file and try its best to clean up and remove
the file at the end of the program. In other words, each initialization with
the file init method will need a brand new empty file in order for the initialization
to succeed. If the same file used by the previous initialization (which happens not
to get cleaned up) is used again, this is unexpected behavior and can often cause
deadlocks and failures. Therefore, even though this method will try its best to clean up
the file, if the auto-delete happens to be unsuccessful, it is your responsibility
to ensure that the file is removed at the end of the training to prevent the same
file to be reused again during the next time. This is especially important
if you plan to call <code class="xref py py-func docutils literal notranslate"><span class="pre">init_process_group()</span></code> multiple times on the same file name.
In other words, if the file is not removed/cleaned up and you call
<code class="xref py py-func docutils literal notranslate"><span class="pre">init_process_group()</span></code> again on that file, failures are expected.
The rule of thumb here is that, make sure that the file is non-existent or
empty everytime <code class="xref py py-func docutils literal notranslate"><span class="pre">init_process_group()</span></code> is called.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="c1"># rank should always be specified</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;file:///mnt/nfs/sharedfile&#39;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="environment-variable-initialization">
<h4>Environment variable initialization<a class="headerlink" href="#environment-variable-initialization" title="Permalink to this headline">¶</a></h4>
<p>This method will read the configuration from environment variables, allowing
one to fully customize how the information is obtained. The variables to be set
are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code> - required; has to be a free port on machine with rank 0</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> - required (except for rank 0); address of rank 0 node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WORLD_SIZE</span></code> - required; can be set either here, or in a call to init function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RANK</span></code> - required; can be set either here, or in a call to init function</p></li>
</ul>
<p>The machine with rank 0 will be used to set up all connections.</p>
<p>This is the default method, meaning that <code class="docutils literal notranslate"><span class="pre">init_method</span></code> does not have to be specified (or
can be <code class="docutils literal notranslate"><span class="pre">env://</span></code>).</p>
</div>
</div>
<div class="section" id="groups">
<h3>Groups<a class="headerlink" href="#groups" title="Permalink to this headline">¶</a></h3>
<p>By default collectives operate on the default group (also called the world) and
require all processes to enter the distributed function call. However, some workloads can benefit
from more fine-grained communication. This is where distributed groups come
into play. <code class="xref py py-func docutils literal notranslate"><span class="pre">new_group()</span></code> function can be
used to create new groups, with arbitrary subsets of all processes. It returns
an opaque group handle that can be given as a <code class="docutils literal notranslate"><span class="pre">group</span></code> argument to all collectives
(collectives are distributed functions to exchange information in certain well-known programming patterns).</p>
<p>Currently <cite>torch.distributed</cite> does not support creating groups with different backends.
In other words, each group being created will use the same backend as you specified in
<code class="xref py py-func docutils literal notranslate"><span class="pre">init_process_group()</span></code>.</p>
</div>
<div class="section" id="point-to-point-communication">
<h3>Point-to-point communication<a class="headerlink" href="#point-to-point-communication" title="Permalink to this headline">¶</a></h3>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">isend()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">irecv()</span></code>
return distributed request objects when used. In general, the type of this object is unspecified
as they should never be created manually, but they are guaranteed to support two methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">is_completed()</span></code> - returns True if the operation has finished</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wait()</span></code> - will block the process until the operation is finished.
<code class="docutils literal notranslate"><span class="pre">is_completed()</span></code> is guaranteed to return True once it returns.</p></li>
</ul>
</div>
<div class="section" id="synchronous-and-asynchronous-collective-operations">
<h3>Synchronous and asynchronous collective operations<a class="headerlink" href="#synchronous-and-asynchronous-collective-operations" title="Permalink to this headline">¶</a></h3>
<p>Every collective operation function supports the following two kinds of operations:</p>
<p>synchronous operation - the default mode, when <code class="docutils literal notranslate"><span class="pre">async_op</span></code> is set to False.
when the function returns, it is guaranteed that
the collective operation is performed (not necessarily completed if it’s a CUDA op since all
CUDA ops are asynchronous), and any further function calls depending on the data of the
collective operation can be called. In the synchronous mode, the collective function does not
return anything</p>
<p>asynchronous operation - when <code class="docutils literal notranslate"><span class="pre">async_op</span></code> is set to True. The collective operation function
returns a distributed request object. In general, you don’t need to create it manually and it
is guaranteed to support two methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">is_completed()</span></code> - returns True if the operation has finished</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wait()</span></code> - will block the process until the operation is finished.</p></li>
</ul>
</div>
<div class="section" id="collective-functions">
<h3>Collective functions<a class="headerlink" href="#collective-functions" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributed.reduce_op">
<em class="property">class </em><code class="descclassname">torch.distributed.</code><code class="descname">reduce_op</code><a class="headerlink" href="#torch.distributed.reduce_op" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated enum-like class for reduction operations: <code class="docutils literal notranslate"><span class="pre">SUM</span></code>, <code class="docutils literal notranslate"><span class="pre">PRODUCT</span></code>,
<code class="docutils literal notranslate"><span class="pre">MIN</span></code>, and <code class="docutils literal notranslate"><span class="pre">MAX</span></code>.</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">ReduceOp</span></code> is recommended to use instead.</p>
</dd></dl>

</div>
<div class="section" id="multi-gpu-collective-functions">
<h3>Multi-GPU collective functions<a class="headerlink" href="#multi-gpu-collective-functions" title="Permalink to this headline">¶</a></h3>
<p>If you have more than one GPU on each node, when using the NCCL and Gloo backend,
<code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast_multigpu()</span></code>
<code class="xref py py-func docutils literal notranslate"><span class="pre">all_reduce_multigpu()</span></code>
<code class="xref py py-func docutils literal notranslate"><span class="pre">reduce_multigpu()</span></code> and
<code class="xref py py-func docutils literal notranslate"><span class="pre">all_gather_multigpu()</span></code> support distributed collective
operations among multiple GPUs within each node. These functions can potentially
improve the overall distributed training performance and be easily used by
passing a list of tensors. Each Tensor in the passed tensor list needs
to be on a separate GPU device of the host where the function is called. Note
that the length of the tensor list needs to be identical among all the
distributed processes. Also note that currently the multi-GPU collective
functions are only supported by the NCCL backend.</p>
<p>For example, if the system we use for distributed training has 2 nodes, each
of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would
like to all-reduce. The following code can serve as a reference:</p>
<p>Code running on Node 0</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
                        <span class="n">init_method</span><span class="o">=</span><span class="s2">&quot;file:///distributed_test&quot;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">dev_idx</span><span class="p">))</span>

<span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</pre></div>
</div>
<p>Code running on Node 1</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
                        <span class="n">init_method</span><span class="o">=</span><span class="s2">&quot;file:///distributed_test&quot;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">dev_idx</span><span class="p">))</span>

<span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</pre></div>
</div>
<p>After the call, all 16 tensors on the two nodes will have the all-reduced value
of 16</p>
</div>
<div class="section" id="launch-utility">
<h3>Launch utility<a class="headerlink" href="#launch-utility" title="Permalink to this headline">¶</a></h3>
<p>The <cite>torch.distributed</cite> package also provides a launch utility in
<cite>torch.distributed.launch</cite>. This helper utility can be used to launch
multiple processes per node for distributed training. This utility also supports
both python2 and python3.</p>
<span class="target" id="module-torch.distributed.launch"></span><p><cite>torch.distributed.launch</cite> is a module that spawns up multiple distributed
training processes on each of the training nodes.</p>
<p>The utility can be used for single-node distributed training, in which one or
more processes per node will be spawned. The utility can be used for either
CPU training or GPU training. If the utility is used for GPU training,
each distributed process will be operating on a single GPU. This can achieve
well-improved single-node training performance. It can also be used in
multi-node distributed training, by spawning up multiple processes on each node
for well-improved multi-node distributed training performance as well.
This will especially be benefitial for systems with multiple Infiniband
interfaces that have direct-GPU support, since all of them can be utilized for
aggregated communication bandwidth.</p>
<p>In both cases of single-node distributed training or multi-node distributed
training, this utility will launch the given number of processes per node
(<code class="docutils literal notranslate"><span class="pre">--nproc_per_node</span></code>). If used for GPU training, this number needs to be less
or euqal to the number of GPUs on the current system (<code class="docutils literal notranslate"><span class="pre">nproc_per_node</span></code>),
and each process will be operating on a single GPU from <em>GPU 0 to
GPU (nproc_per_node - 1)</em>.</p>
<p><strong>How to use this module:</strong></p>
<ol class="arabic simple">
<li><p>Single-Node multi-process distributed training</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
<span class="go">           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other</span>
<span class="go">           arguments of your training script)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Multi-Node multi-process distributed training: (e.g. two nodes)</p></li>
</ol>
<p>Node 1: <em>(IP: 192.168.1.1, and has a free port: 1234)</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
<span class="go">           --nnodes=2 --node_rank=0 --master_addr=&quot;192.168.1.1&quot;</span>
<span class="go">           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3</span>
<span class="go">           and all other arguments of your training script)</span>
</pre></div>
</div>
<p>Node 2:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
<span class="go">           --nnodes=2 --node_rank=1 --master_addr=&quot;192.168.1.1&quot;</span>
<span class="go">           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3</span>
<span class="go">           and all other arguments of your training script)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>To look up what optional arguments this module offers:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">help</span>
</pre></div>
</div>
<p><strong>Important Notices:</strong></p>
<p>1. This utilty and multi-process distributed (single-node or
multi-node) GPU training currently only achieves the best performance using
the NCCL distributed backend. Thus NCCL backend is the recommended backend to
use for GPU training.</p>
<p>2. In your training program, you must parse the command-line argument:
<code class="docutils literal notranslate"><span class="pre">--local_rank=LOCAL_PROCESS_RANK</span></code>, which will be provided by this module.
If your training program uses GPUs, you should ensure that your code only
runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:</p>
<p>Parsing the local_rank argument</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--local_rank&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</pre></div>
</div>
<p>Set your device to local rank using either</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>  <span class="c1"># before your code runs</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">local_rank</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="c1"># your code to run</span>
</pre></div>
</div>
<p>3. In your training program, you are supposed to call the following function
at the beginning to start the distributed backend. You need to make sure that
the init_method uses <code class="docutils literal notranslate"><span class="pre">env://</span></code>, which is the only supported <code class="docutils literal notranslate"><span class="pre">init_method</span></code>
by this module.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;YOUR BACKEND&#39;</span><span class="p">,</span>
                                     <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>4. In your training program, you can either use regular distributed functions
or use <a class="reference internal" href="index.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code></a> module. If your
training program uses GPUs for training and you would like to use
<a class="reference internal" href="index.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code></a> module,
here is how to configure it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                                  <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">arg</span><span class="o">.</span><span class="n">local_rank</span><span class="p">],</span>
                                                  <span class="n">output_device</span><span class="o">=</span><span class="n">arg</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>
</pre></div>
</div>
<p>Please ensure that <code class="docutils literal notranslate"><span class="pre">device_ids</span></code> argument is set to be the only GPU device id
that your code will be operating on. This is generally the local rank of the
process. In other words, the <code class="docutils literal notranslate"><span class="pre">device_ids</span></code> needs to be <code class="docutils literal notranslate"><span class="pre">[args.local_rank]</span></code>,
and <code class="docutils literal notranslate"><span class="pre">output_device</span></code> needs to be <code class="docutils literal notranslate"><span class="pre">args.local_rank</span></code> in order to use this
utility</p>
<p>5. Another way to pass <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> to the subprocesses via environment variable
<code class="docutils literal notranslate"><span class="pre">LOCAL_RANK</span></code>. This behavior is enabled when you launch the script with
<code class="docutils literal notranslate"><span class="pre">--use_env=True</span></code>. You must adjust the subprocess example above to replace
<code class="docutils literal notranslate"><span class="pre">args.local_rank</span></code> with <code class="docutils literal notranslate"><span class="pre">os.environ['LOCAL_RANK']</span></code>; the launcher
will not pass <code class="docutils literal notranslate"><span class="pre">--local_rank</span></code> when you specify this flag.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">local_rank</span></code> is NOT globally unique: it is only unique per process
on a machine.  Thus, don’t use it to decide if you should, e.g.,
write to a networked filesystem.  See
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/12042">https://github.com/pytorch/pytorch/issues/12042</a> for an example of
how things can go wrong if you don’t do this correctly.</p>
</div>
</div>
<div class="section" id="spawn-utility">
<h3>Spawn utility<a class="headerlink" href="#spawn-utility" title="Permalink to this headline">¶</a></h3>
<p>The <span class="xref std std-doc">torch.multiprocessing</span> package also provides a <code class="docutils literal notranslate"><span class="pre">spawn</span></code>
function in <a class="reference internal" href="index.html#torch.multiprocessing.spawn" title="torch.multiprocessing.spawn"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multiprocessing.spawn()</span></code></a>. This helper function
can be used to spawn multiple processes. It works by passing in the
function that you want to run and spawns N processes to run it. This
can be used for multiprocess distributed training as well.</p>
<p>For references on how to use it, please refer to <a class="reference external" href="https://github.com/pytorch/examples/tree/master/imagenet">PyToch example - ImageNet
implementation</a></p>
<p>Note that this function requires Python 3.4 or higher.</p>
</div>
</div>
<span id="document-distributions"></span><div class="section" id="module-torch.distributions">
<span id="probability-distributions-torch-distributions"></span><h2>Probability distributions - torch.distributions<a class="headerlink" href="#module-torch.distributions" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">distributions</span></code> package contains parameterizable probability distributions
and sampling functions. This allows the construction of stochastic computation
graphs and stochastic gradient estimators for optimization. This package
generally follows the design of the <a class="reference external" href="https://arxiv.org/abs/1711.10604">TensorFlow Distributions</a> package.</p>
<p>It is not possible to directly backpropagate through random samples. However,
there are two main methods for creating surrogate functions that can be
backpropagated through. These are the score function estimator/likelihood ratio
estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly
seen as the basis for policy gradient methods in reinforcement learning, and the
pathwise derivative estimator is commonly seen in the reparameterization trick
in variational autoencoders. Whilst the score function only requires the value
of samples <span class="math">\(f(x)\)</span>, the pathwise derivative requires the derivative
<span class="math">\(f'(x)\)</span>. The next sections discuss these two in a reinforcement learning
example. For more details see
<a class="reference external" href="https://arxiv.org/abs/1506.05254">Gradient Estimation Using Stochastic Computation Graphs</a> .</p>
<div class="section" id="score-function">
<h3>Score function<a class="headerlink" href="#score-function" title="Permalink to this headline">¶</a></h3>
<p>When the probability density function is differentiable with respect to its
parameters, we only need <code class="xref py py-meth docutils literal notranslate"><span class="pre">sample()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">log_prob()</span></code> to implement REINFORCE:</p>
<div class="math">
\[\Delta\theta  = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}\]</div>
<p>where <span class="math">\(\theta\)</span> are the parameters, <span class="math">\(\alpha\)</span> is the learning rate,
<span class="math">\(r\)</span> is the reward and <span class="math">\(p(a|\pi^\theta(s))\)</span> is the probability of
taking action <span class="math">\(a\)</span> in state <span class="math">\(s\)</span> given policy <span class="math">\(\pi^\theta\)</span>.</p>
<p>In practice we would sample an action from the output of a network, apply this
action in an environment, and then use <code class="docutils literal notranslate"><span class="pre">log_prob</span></code> to construct an equivalent
loss function. Note that we use a negative because optimizers use gradient
descent, whilst the rule above assumes gradient ascent. With a categorical
policy, the code for implementing REINFORCE would be as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">policy_network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="c1"># Note that this is equivalent to what used to be called multinomial</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">m</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">*</span> <span class="n">reward</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="pathwise-derivative">
<h3>Pathwise derivative<a class="headerlink" href="#pathwise-derivative" title="Permalink to this headline">¶</a></h3>
<p>The other way to implement these stochastic/policy gradients would be to use the
reparameterization trick from the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">rsample()</span></code> method, where the
parameterized random variable can be constructed via a parameterized
deterministic function of a parameter-free random variable. The reparameterized
sample therefore becomes differentiable. The code for implementing the pathwise
derivative would be as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">policy_network</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
<span class="c1"># Any distribution with .has_rsample == True could work based on the application</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
<span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># Assuming that reward is differentiable</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">reward</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="distribution">
<h3><span class="hidden-section">Distribution</span><a class="headerlink" href="#distribution" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.distribution.Distribution">
<em class="property">class </em><code class="descclassname">torch.distributions.distribution.</code><code class="descname">Distribution</code><span class="sig-paren">(</span><em>batch_shape=torch.Size([])</em>, <em>event_shape=torch.Size([])</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Distribution is the abstract base class for probability distributions.</p>
<dl class="attribute">
<dt id="torch.distributions.distribution.Distribution.arg_constraints">
<code class="descname">arg_constraints</code><a class="headerlink" href="#torch.distributions.distribution.Distribution.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary from argument names to
<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> objects that
should be satisfied by each argument of this distribution. Args that
are not tensors need not appear in this dict.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.distribution.Distribution.batch_shape">
<code class="descname">batch_shape</code><a class="headerlink" href="#torch.distributions.distribution.Distribution.batch_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the shape over which parameters are batched.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.cdf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the cumulative density/mass function evaluated at
<cite>value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns entropy of distribution, batched over batch_shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Tensor of shape batch_shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.enumerate_support">
<code class="descname">enumerate_support</code><span class="sig-paren">(</span><em>expand=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns tensor containing all values supported by a discrete
distribution. The result will enumerate over dimension 0, so the shape
of the result will be <cite>(cardinality,) + batch_shape + event_shape</cite>
(where <cite>event_shape = ()</cite> for univariate distributions).</p>
<p>Note that this enumerates over all batched tensors in lock-step
<cite>[[0, 0], [1, 1], …]</cite>. With <cite>expand=False</cite>, enumeration happens
along dim 0, but with the remaining batch dimensions being
singleton dimensions, <cite>[[0], [1], ..</cite>.</p>
<p>To iterate over the full Cartesian product use
<cite>itertools.product(m.enumerate_support())</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>expand</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to expand the support over the
batch dims to match the distribution’s <cite>batch_shape</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor iterating over dimension 0.</p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.distribution.Distribution.event_shape">
<code class="descname">event_shape</code><a class="headerlink" href="#torch.distributions.distribution.Distribution.event_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the shape of a single sample (without batching).</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.expand" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new distribution instance (or populates an existing instance
provided by a derived class) with batch dimensions expanded to
<cite>batch_shape</cite>. This method calls <a class="reference internal" href="index.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-class docutils literal notranslate"><span class="pre">expand</span></code></a> on
the distribution’s parameters. As such, this does not allocate new
memory for the expanded distribution instance. Additionally,
this does not repeat any args checking or parameter broadcasting in
<cite>__init__.py</cite>, when an instance is first created.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_shape</strong> (<em>torch.Size</em>) – the desired expanded size.</p></li>
<li><p><strong>_instance</strong> – new instance provided by subclasses that
need to override <cite>.expand</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>New distribution instance with batch dimensions expanded to
<cite>batch_size</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.icdf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the inverse cumulative density/mass function evaluated at
<cite>value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the log of the probability density/mass function evaluated at
<cite>value</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.distribution.Distribution.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.distribution.Distribution.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the mean of the distribution.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.perplexity">
<code class="descname">perplexity</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.perplexity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns perplexity of distribution, batched over batch_shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Tensor of shape batch_shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.rsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a sample_shape shaped reparameterized sample or sample_shape
shaped batch of reparameterized samples if the distribution parameters
are batched.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a sample_shape shaped sample or sample_shape shaped batch of
samples if the distribution parameters are batched.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.distribution.Distribution.sample_n">
<code class="descname">sample_n</code><span class="sig-paren">(</span><em>n</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.distribution.Distribution.sample_n" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates n samples or n batches of samples if the distribution
parameters are batched.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.distribution.Distribution.stddev">
<code class="descname">stddev</code><a class="headerlink" href="#torch.distributions.distribution.Distribution.stddev" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the standard deviation of the distribution.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.distribution.Distribution.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.distribution.Distribution.support" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> object
representing this distribution’s support.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.distribution.Distribution.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.distribution.Distribution.variance" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the variance of the distribution.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="exponentialfamily">
<h3><span class="hidden-section">ExponentialFamily</span><a class="headerlink" href="#exponentialfamily" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.exp_family.ExponentialFamily">
<em class="property">class </em><code class="descclassname">torch.distributions.exp_family.</code><code class="descname">ExponentialFamily</code><span class="sig-paren">(</span><em>batch_shape=torch.Size([])</em>, <em>event_shape=torch.Size([])</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exp_family.ExponentialFamily" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>ExponentialFamily is the abstract base class for probability distributions belonging to an
exponential family, whose probability mass/density function has the form is defined below</p>
<div class="math">
\[p_{F}(x; \theta) = \exp(\langle t(x), \theta\rangle - F(\theta) + k(x))\]</div>
<p>where <span class="math">\(\theta\)</span> denotes the natural parameters, <span class="math">\(t(x)\)</span> denotes the sufficient statistic,
<span class="math">\(F(\theta)\)</span> is the log normalizer function for a given family and <span class="math">\(k(x)\)</span> is the carrier
measure.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class is an intermediary between the <cite>Distribution</cite> class and distributions which belong
to an exponential family mainly to check the correctness of the <cite>.entropy()</cite> and analytic KL
divergence methods. We use this class to compute the entropy and KL divergence using the AD
framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and
Cross-entropies of Exponential Families).</p>
</div>
<dl class="method">
<dt id="torch.distributions.exp_family.ExponentialFamily.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exp_family.ExponentialFamily.entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Method to compute the entropy using Bregman divergence of the log normalizer.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="bernoulli">
<h3><span class="hidden-section">Bernoulli</span><a class="headerlink" href="#bernoulli" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.bernoulli.Bernoulli">
<em class="property">class </em><code class="descclassname">torch.distributions.bernoulli.</code><code class="descname">Bernoulli</code><span class="sig-paren">(</span><em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.exp_family.ExponentialFamily</span></code></a></p>
<p>Creates a Bernoulli distribution parameterized by <a class="reference internal" href="#torch.distributions.bernoulli.Bernoulli.probs" title="torch.distributions.bernoulli.Bernoulli.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>
or <a class="reference internal" href="#torch.distributions.bernoulli.Bernoulli.logits" title="torch.distributions.bernoulli.Bernoulli.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both).</p>
<p>Samples are binary (0 or 1). They take the value <cite>1</cite> with probability <cite>p</cite>
and <cite>0</cite> with probability <cite>1 - p</cite>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># 30% chance 1; 70% chance 0</span>
<span class="go">tensor([ 0.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the probability of sampling <cite>1</cite></p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the log-odds of sampling <cite>1</cite></p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.bernoulli.Bernoulli.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</em><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.bernoulli.Bernoulli.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.bernoulli.Bernoulli.enumerate_support">
<code class="descname">enumerate_support</code><span class="sig-paren">(</span><em>expand=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.bernoulli.Bernoulli.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.bernoulli.Bernoulli.has_enumerate_support">
<code class="descname">has_enumerate_support</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.has_enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.bernoulli.Bernoulli.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.bernoulli.Bernoulli.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.bernoulli.Bernoulli.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.bernoulli.Bernoulli.param_shape">
<code class="descname">param_shape</code><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.param_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.bernoulli.Bernoulli.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.bernoulli.Bernoulli.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.bernoulli.Bernoulli.support">
<code class="descname">support</code><em class="property"> = Boolean()</em><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.bernoulli.Bernoulli.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.bernoulli.Bernoulli.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="beta">
<h3><span class="hidden-section">Beta</span><a class="headerlink" href="#beta" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.beta.Beta">
<em class="property">class </em><code class="descclassname">torch.distributions.beta.</code><code class="descname">Beta</code><span class="sig-paren">(</span><em>concentration1</em>, <em>concentration0</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.beta.Beta" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.exp_family.ExponentialFamily</span></code></a></p>
<p>Beta distribution parameterized by <a class="reference internal" href="#torch.distributions.beta.Beta.concentration1" title="torch.distributions.beta.Beta.concentration1"><code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration1</span></code></a> and <a class="reference internal" href="#torch.distributions.beta.Beta.concentration0" title="torch.distributions.beta.Beta.concentration0"><code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration0</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Beta distributed with concentration concentration1 and concentration0</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>concentration1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 1st concentration parameter of the distribution
(often referred to as alpha)</p></li>
<li><p><strong>concentration0</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 2nd concentration parameter of the distribution
(often referred to as beta)</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.beta.Beta.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.beta.Beta.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.beta.Beta.concentration0">
<code class="descname">concentration0</code><a class="headerlink" href="#torch.distributions.beta.Beta.concentration0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.beta.Beta.concentration1">
<code class="descname">concentration1</code><a class="headerlink" href="#torch.distributions.beta.Beta.concentration1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.beta.Beta.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.beta.Beta.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.beta.Beta.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.beta.Beta.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.beta.Beta.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.beta.Beta.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.beta.Beta.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.beta.Beta.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.beta.Beta.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.beta.Beta.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.beta.Beta.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=()</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.beta.Beta.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.beta.Beta.support">
<code class="descname">support</code><em class="property"> = Interval(lower_bound=0.0, upper_bound=1.0)</em><a class="headerlink" href="#torch.distributions.beta.Beta.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.beta.Beta.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.beta.Beta.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="binomial">
<h3><span class="hidden-section">Binomial</span><a class="headerlink" href="#binomial" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.binomial.Binomial">
<em class="property">class </em><code class="descclassname">torch.distributions.binomial.</code><code class="descname">Binomial</code><span class="sig-paren">(</span><em>total_count=1</em>, <em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.binomial.Binomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a Binomial distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code> and
either <a class="reference internal" href="#torch.distributions.binomial.Binomial.probs" title="torch.distributions.binomial.Binomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or <a class="reference internal" href="#torch.distributions.binomial.Binomial.logits" title="torch.distributions.binomial.Binomial.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both). <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code> must be
broadcastable with <a class="reference internal" href="#torch.distributions.binomial.Binomial.probs" title="torch.distributions.binomial.Binomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>/<a class="reference internal" href="#torch.distributions.binomial.Binomial.logits" title="torch.distributions.binomial.Binomial.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Binomial</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span> <span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="o">.</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([   0.,   22.,   71.,  100.])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Binomial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">5.</span><span class="p">],</span> <span class="p">[</span><span class="mf">10.</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([[ 4.,  5.],</span>
<span class="go">        [ 7.,  6.]])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – number of Bernoulli trials</p></li>
<li><p><strong>probs</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Event log-odds</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.binomial.Binomial.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)}</em><a class="headerlink" href="#torch.distributions.binomial.Binomial.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.binomial.Binomial.enumerate_support">
<code class="descname">enumerate_support</code><span class="sig-paren">(</span><em>expand=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.binomial.Binomial.enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.binomial.Binomial.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.binomial.Binomial.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.binomial.Binomial.has_enumerate_support">
<code class="descname">has_enumerate_support</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.binomial.Binomial.has_enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.binomial.Binomial.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.binomial.Binomial.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.binomial.Binomial.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.binomial.Binomial.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.binomial.Binomial.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.binomial.Binomial.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.binomial.Binomial.param_shape">
<code class="descname">param_shape</code><a class="headerlink" href="#torch.distributions.binomial.Binomial.param_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.binomial.Binomial.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.binomial.Binomial.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.binomial.Binomial.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.binomial.Binomial.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.binomial.Binomial.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.binomial.Binomial.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.binomial.Binomial.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.binomial.Binomial.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="categorical">
<h3><span class="hidden-section">Categorical</span><a class="headerlink" href="#categorical" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.categorical.Categorical">
<em class="property">class </em><code class="descclassname">torch.distributions.categorical.</code><code class="descname">Categorical</code><span class="sig-paren">(</span><em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.categorical.Categorical" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a categorical distribution parameterized by either <a class="reference internal" href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or
<a class="reference internal" href="#torch.distributions.categorical.Categorical.logits" title="torch.distributions.categorical.Categorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is equivalent to the distribution that <a class="reference internal" href="index.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a>
samples from.</p>
</div>
<p>Samples are integers from <span class="math">\(\{0, \ldots, K-1\}\)</span> where <cite>K</cite> is <code class="docutils literal notranslate"><span class="pre">probs.size(-1)</span></code>.</p>
<p>If <a class="reference internal" href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> is 1D with length-<cite>K</cite>, each element is the relative
probability of sampling the class at that index.</p>
<p>If <a class="reference internal" href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> is 2D, it is treated as a batch of relative probability
vectors.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> must be non-negative, finite and have a non-zero sum,
and it will be normalized to sum to 1.</p>
</div>
<p>See also: <a class="reference internal" href="index.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.multinomial()</span></code></a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span> <span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># equal probability of 0, 1, 2, 3</span>
<span class="go">tensor(3)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event log probabilities</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.categorical.Categorical.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Simplex()}</em><a class="headerlink" href="#torch.distributions.categorical.Categorical.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.categorical.Categorical.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.categorical.Categorical.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.categorical.Categorical.enumerate_support">
<code class="descname">enumerate_support</code><span class="sig-paren">(</span><em>expand=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.categorical.Categorical.enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.categorical.Categorical.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.categorical.Categorical.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.categorical.Categorical.has_enumerate_support">
<code class="descname">has_enumerate_support</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.categorical.Categorical.has_enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.categorical.Categorical.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.categorical.Categorical.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.categorical.Categorical.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.categorical.Categorical.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.categorical.Categorical.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.categorical.Categorical.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.categorical.Categorical.param_shape">
<code class="descname">param_shape</code><a class="headerlink" href="#torch.distributions.categorical.Categorical.param_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.categorical.Categorical.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.categorical.Categorical.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.categorical.Categorical.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.categorical.Categorical.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.categorical.Categorical.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.categorical.Categorical.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.categorical.Categorical.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.categorical.Categorical.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="cauchy">
<h3><span class="hidden-section">Cauchy</span><a class="headerlink" href="#cauchy" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.cauchy.Cauchy">
<em class="property">class </em><code class="descclassname">torch.distributions.cauchy.</code><code class="descname">Cauchy</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of
independent normally distributed random variables with means <cite>0</cite> follows a
Cauchy distribution.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Cauchy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Cauchy distribution with loc=0 and scale=1</span>
<span class="go">tensor([ 2.3214])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – mode or median of the distribution.</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – half width at half maximum.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.cauchy.Cauchy.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.cauchy.Cauchy.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.cdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.cauchy.Cauchy.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.cauchy.Cauchy.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.cauchy.Cauchy.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.cauchy.Cauchy.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.icdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.cauchy.Cauchy.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.cauchy.Cauchy.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.cauchy.Cauchy.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.cauchy.Cauchy.support">
<code class="descname">support</code><em class="property"> = Real()</em><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.cauchy.Cauchy.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.cauchy.Cauchy.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="chi2">
<h3><span class="hidden-section">Chi2</span><a class="headerlink" href="#chi2" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.chi2.Chi2">
<em class="property">class </em><code class="descclassname">torch.distributions.chi2.</code><code class="descname">Chi2</code><span class="sig-paren">(</span><em>df</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.chi2.Chi2" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.gamma.Gamma" title="torch.distributions.gamma.Gamma"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.gamma.Gamma</span></code></a></p>
<p>Creates a Chi2 distribution parameterized by shape parameter <a class="reference internal" href="#torch.distributions.chi2.Chi2.df" title="torch.distributions.chi2.Chi2.df"><code class="xref py py-attr docutils literal notranslate"><span class="pre">df</span></code></a>.
This is exactly equivalent to <code class="docutils literal notranslate"><span class="pre">Gamma(alpha=0.5*df,</span> <span class="pre">beta=0.5)</span></code></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Chi2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Chi2 distributed with shape df=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>df</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – shape parameter of the distribution</p>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.chi2.Chi2.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'df': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.chi2.Chi2.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.chi2.Chi2.df">
<code class="descname">df</code><a class="headerlink" href="#torch.distributions.chi2.Chi2.df" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.chi2.Chi2.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.chi2.Chi2.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="dirichlet">
<h3><span class="hidden-section">Dirichlet</span><a class="headerlink" href="#dirichlet" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.dirichlet.Dirichlet">
<em class="property">class </em><code class="descclassname">torch.distributions.dirichlet.</code><code class="descname">Dirichlet</code><span class="sig-paren">(</span><em>concentration</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.exp_family.ExponentialFamily</span></code></a></p>
<p>Creates a Dirichlet distribution parameterized by concentration <code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Dirichlet distributed with concentrarion concentration</span>
<span class="go">tensor([ 0.1046,  0.8954])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>concentration</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – concentration parameter of the distribution
(often referred to as alpha)</p>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.dirichlet.Dirichlet.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'concentration': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.dirichlet.Dirichlet.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.dirichlet.Dirichlet.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.dirichlet.Dirichlet.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.dirichlet.Dirichlet.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.dirichlet.Dirichlet.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.dirichlet.Dirichlet.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=()</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.dirichlet.Dirichlet.support">
<code class="descname">support</code><em class="property"> = Simplex()</em><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.dirichlet.Dirichlet.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.dirichlet.Dirichlet.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="exponential">
<h3><span class="hidden-section">Exponential</span><a class="headerlink" href="#exponential" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.exponential.Exponential">
<em class="property">class </em><code class="descclassname">torch.distributions.exponential.</code><code class="descname">Exponential</code><span class="sig-paren">(</span><em>rate</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exponential.Exponential" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.exp_family.ExponentialFamily</span></code></a></p>
<p>Creates a Exponential distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">rate</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Exponential distributed with rate=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – rate = 1 / scale of the distribution</p>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.exponential.Exponential.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'rate': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.exponential.Exponential.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.exponential.Exponential.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exponential.Exponential.cdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.exponential.Exponential.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exponential.Exponential.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.exponential.Exponential.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exponential.Exponential.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.exponential.Exponential.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.exponential.Exponential.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.exponential.Exponential.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exponential.Exponential.icdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.exponential.Exponential.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exponential.Exponential.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.exponential.Exponential.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.exponential.Exponential.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.exponential.Exponential.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.exponential.Exponential.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.exponential.Exponential.stddev">
<code class="descname">stddev</code><a class="headerlink" href="#torch.distributions.exponential.Exponential.stddev" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.exponential.Exponential.support">
<code class="descname">support</code><em class="property"> = GreaterThan(lower_bound=0.0)</em><a class="headerlink" href="#torch.distributions.exponential.Exponential.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.exponential.Exponential.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.exponential.Exponential.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="fishersnedecor">
<h3><span class="hidden-section">FisherSnedecor</span><a class="headerlink" href="#fishersnedecor" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor">
<em class="property">class </em><code class="descclassname">torch.distributions.fishersnedecor.</code><code class="descname">FisherSnedecor</code><span class="sig-paren">(</span><em>df1</em>, <em>df2</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a Fisher-Snedecor distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">df1</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">df2</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">FisherSnedecor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Fisher-Snedecor-distributed with df1=1 and df2=2</span>
<span class="go">tensor([ 0.2453])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>df1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – degrees of freedom parameter 1</p></li>
<li><p><strong>df2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – degrees of freedom parameter 2</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor.support">
<code class="descname">support</code><em class="property"> = GreaterThan(lower_bound=0.0)</em><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.fishersnedecor.FisherSnedecor.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.fishersnedecor.FisherSnedecor.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="gamma">
<h3><span class="hidden-section">Gamma</span><a class="headerlink" href="#gamma" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.gamma.Gamma">
<em class="property">class </em><code class="descclassname">torch.distributions.gamma.</code><code class="descname">Gamma</code><span class="sig-paren">(</span><em>concentration</em>, <em>rate</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gamma.Gamma" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.exp_family.ExponentialFamily</span></code></a></p>
<p>Creates a Gamma distribution parameterized by shape <code class="xref py py-attr docutils literal notranslate"><span class="pre">concentration</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">rate</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Gamma distributed with concentration=1 and rate=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – shape parameter of the distribution
(often referred to as alpha)</p></li>
<li><p><strong>rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – rate = 1 / scale of the distribution
(often referred to as beta)</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.gamma.Gamma.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.gamma.Gamma.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.gamma.Gamma.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gamma.Gamma.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.gamma.Gamma.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gamma.Gamma.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.gamma.Gamma.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.gamma.Gamma.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.gamma.Gamma.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gamma.Gamma.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.gamma.Gamma.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.gamma.Gamma.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.gamma.Gamma.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gamma.Gamma.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.gamma.Gamma.support">
<code class="descname">support</code><em class="property"> = GreaterThan(lower_bound=0.0)</em><a class="headerlink" href="#torch.distributions.gamma.Gamma.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.gamma.Gamma.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.gamma.Gamma.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="geometric">
<h3><span class="hidden-section">Geometric</span><a class="headerlink" href="#geometric" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.geometric.Geometric">
<em class="property">class </em><code class="descclassname">torch.distributions.geometric.</code><code class="descname">Geometric</code><span class="sig-paren">(</span><em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.geometric.Geometric" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a Geometric distribution parameterized by <a class="reference internal" href="#torch.distributions.geometric.Geometric.probs" title="torch.distributions.geometric.Geometric.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>,
where <a class="reference internal" href="#torch.distributions.geometric.Geometric.probs" title="torch.distributions.geometric.Geometric.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> is the probability of success of Bernoulli trials.
It represents the probability that in <span class="math">\(k + 1\)</span> Bernoulli trials, the
first <span class="math">\(k\)</span> trials failed, before seeing a success.</p>
<p>Samples are non-negative integers [0, <span class="math">\(\inf\)</span>).</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Geometric</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># underlying Bernoulli has 30% chance 1; 70% chance 0</span>
<span class="go">tensor([ 2.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the probability of sampling <cite>1</cite>. Must be in range (0, 1]</p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the log-odds of sampling <cite>1</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.geometric.Geometric.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</em><a class="headerlink" href="#torch.distributions.geometric.Geometric.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.geometric.Geometric.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.geometric.Geometric.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.geometric.Geometric.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.geometric.Geometric.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.geometric.Geometric.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.geometric.Geometric.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.geometric.Geometric.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.geometric.Geometric.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.geometric.Geometric.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.geometric.Geometric.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.geometric.Geometric.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.geometric.Geometric.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.geometric.Geometric.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.geometric.Geometric.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.geometric.Geometric.support">
<code class="descname">support</code><em class="property"> = IntegerGreaterThan(lower_bound=0)</em><a class="headerlink" href="#torch.distributions.geometric.Geometric.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.geometric.Geometric.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.geometric.Geometric.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="gumbel">
<h3><span class="hidden-section">Gumbel</span><a class="headerlink" href="#gumbel" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.gumbel.Gumbel">
<em class="property">class </em><code class="descclassname">torch.distributions.gumbel.</code><code class="descname">Gumbel</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gumbel.Gumbel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.transformed_distribution.TransformedDistribution</span></code></a></p>
<p>Samples from a Gumbel Distribution.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Gumbel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from Gumbel distribution with loc=1, scale=2</span>
<span class="go">tensor([ 1.0124])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Location parameter of the distribution</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Scale parameter of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.gumbel.Gumbel.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.gumbel.Gumbel.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.gumbel.Gumbel.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.gumbel.Gumbel.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.gumbel.Gumbel.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.gumbel.Gumbel.stddev">
<code class="descname">stddev</code><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.stddev" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.gumbel.Gumbel.support">
<code class="descname">support</code><em class="property"> = Real()</em><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.gumbel.Gumbel.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.gumbel.Gumbel.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="halfcauchy">
<h3><span class="hidden-section">HalfCauchy</span><a class="headerlink" href="#halfcauchy" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.half_cauchy.HalfCauchy">
<em class="property">class </em><code class="descclassname">torch.distributions.half_cauchy.</code><code class="descname">HalfCauchy</code><span class="sig-paren">(</span><em>scale</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.transformed_distribution.TransformedDistribution</span></code></a></p>
<p>Creates a half-normal distribution parameterized by <cite>scale</cite> where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">Cauchy</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="o">|</span><span class="n">X</span><span class="o">|</span> <span class="o">~</span> <span class="n">HalfCauchy</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">HalfCauchy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># half-cauchy distributed with scale=1</span>
<span class="go">tensor([ 2.3214])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – scale of the full Cauchy distribution</p>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.half_cauchy.HalfCauchy.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_cauchy.HalfCauchy.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.cdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_cauchy.HalfCauchy.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_cauchy.HalfCauchy.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_cauchy.HalfCauchy.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_cauchy.HalfCauchy.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>prob</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.icdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_cauchy.HalfCauchy.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_cauchy.HalfCauchy.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_cauchy.HalfCauchy.scale">
<code class="descname">scale</code><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_cauchy.HalfCauchy.support">
<code class="descname">support</code><em class="property"> = GreaterThan(lower_bound=0.0)</em><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_cauchy.HalfCauchy.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.half_cauchy.HalfCauchy.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="halfnormal">
<h3><span class="hidden-section">HalfNormal</span><a class="headerlink" href="#halfnormal" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.half_normal.HalfNormal">
<em class="property">class </em><code class="descclassname">torch.distributions.half_normal.</code><code class="descname">HalfNormal</code><span class="sig-paren">(</span><em>scale</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.transformed_distribution.TransformedDistribution</span></code></a></p>
<p>Creates a half-normal distribution parameterized by <cite>scale</cite> where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="o">|</span><span class="n">X</span><span class="o">|</span> <span class="o">~</span> <span class="n">HalfNormal</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">HalfNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># half-normal distributed with scale=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – scale of the full Normal distribution</p>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.half_normal.HalfNormal.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_normal.HalfNormal.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.cdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_normal.HalfNormal.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_normal.HalfNormal.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_normal.HalfNormal.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_normal.HalfNormal.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>prob</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.icdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.half_normal.HalfNormal.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_normal.HalfNormal.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_normal.HalfNormal.scale">
<code class="descname">scale</code><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_normal.HalfNormal.support">
<code class="descname">support</code><em class="property"> = GreaterThan(lower_bound=0.0)</em><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.half_normal.HalfNormal.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.half_normal.HalfNormal.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="independent">
<h3><span class="hidden-section">Independent</span><a class="headerlink" href="#independent" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.independent.Independent">
<em class="property">class </em><code class="descclassname">torch.distributions.independent.</code><code class="descname">Independent</code><span class="sig-paren">(</span><em>base_distribution</em>, <em>reinterpreted_batch_ndims</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.independent.Independent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Reinterprets some of the batch dims of a distribution as event dims.</p>
<p>This is mainly useful for changing the shape of the result of
<a class="reference internal" href="#torch.distributions.independent.Independent.log_prob" title="torch.distributions.independent.Independent.log_prob"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_prob()</span></code></a>. For example to create a diagonal Normal distribution with
the same shape as a Multivariate Normal distribution (so they are
interchangeable), you can:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mvn</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">scale</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">mvn</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">mvn</span><span class="o">.</span><span class="n">event_shape</span><span class="p">]</span>
<span class="go">[torch.Size(()), torch.Size((3,))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normal</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">normal</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">normal</span><span class="o">.</span><span class="n">event_shape</span><span class="p">]</span>
<span class="go">[torch.Size((3,)), torch.Size(())]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diagn</span> <span class="o">=</span> <span class="n">Independent</span><span class="p">(</span><span class="n">normal</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">diagn</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">diagn</span><span class="o">.</span><span class="n">event_shape</span><span class="p">]</span>
<span class="go">[torch.Size(()), torch.Size((3,))]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_distribution</strong> (<a class="reference internal" href="index.html#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>torch.distributions.distribution.Distribution</em></a>) – a
base distribution</p></li>
<li><p><strong>reinterpreted_batch_ndims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the number of batch dims to
reinterpret as event dims</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.independent.Independent.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {}</em><a class="headerlink" href="#torch.distributions.independent.Independent.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.independent.Independent.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.independent.Independent.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.independent.Independent.enumerate_support">
<code class="descname">enumerate_support</code><span class="sig-paren">(</span><em>expand=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.independent.Independent.enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.independent.Independent.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.independent.Independent.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.independent.Independent.has_enumerate_support">
<code class="descname">has_enumerate_support</code><a class="headerlink" href="#torch.distributions.independent.Independent.has_enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.independent.Independent.has_rsample">
<code class="descname">has_rsample</code><a class="headerlink" href="#torch.distributions.independent.Independent.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.independent.Independent.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.independent.Independent.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.independent.Independent.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.independent.Independent.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.independent.Independent.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.independent.Independent.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.independent.Independent.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.independent.Independent.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.independent.Independent.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.independent.Independent.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.independent.Independent.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.independent.Independent.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="laplace">
<h3><span class="hidden-section">Laplace</span><a class="headerlink" href="#laplace" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.laplace.Laplace">
<em class="property">class </em><code class="descclassname">torch.distributions.laplace.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.laplace.Laplace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a Laplace distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">loc</span></code> and :attr:’scale’.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Laplace</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Laplace distributed with loc=0, scale=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – mean of the distribution</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – scale of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.laplace.Laplace.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.laplace.Laplace.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.laplace.Laplace.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.laplace.Laplace.cdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.laplace.Laplace.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.laplace.Laplace.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.laplace.Laplace.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.laplace.Laplace.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.laplace.Laplace.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.laplace.Laplace.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.laplace.Laplace.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.laplace.Laplace.icdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.laplace.Laplace.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.laplace.Laplace.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.laplace.Laplace.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.laplace.Laplace.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.laplace.Laplace.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.laplace.Laplace.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.laplace.Laplace.stddev">
<code class="descname">stddev</code><a class="headerlink" href="#torch.distributions.laplace.Laplace.stddev" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.laplace.Laplace.support">
<code class="descname">support</code><em class="property"> = Real()</em><a class="headerlink" href="#torch.distributions.laplace.Laplace.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.laplace.Laplace.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.laplace.Laplace.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="lognormal">
<h3><span class="hidden-section">LogNormal</span><a class="headerlink" href="#lognormal" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.log_normal.LogNormal">
<em class="property">class </em><code class="descclassname">torch.distributions.log_normal.</code><code class="descname">LogNormal</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.log_normal.LogNormal" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.transformed_distribution.TransformedDistribution</span></code></a></p>
<p>Creates a log-normal distribution parameterized by
<a class="reference internal" href="#torch.distributions.log_normal.LogNormal.loc" title="torch.distributions.log_normal.LogNormal.loc"><code class="xref py py-attr docutils literal notranslate"><span class="pre">loc</span></code></a> and <a class="reference internal" href="#torch.distributions.log_normal.LogNormal.scale" title="torch.distributions.log_normal.LogNormal.scale"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code></a> where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">~</span> <span class="n">LogNormal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">LogNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># log-normal distributed with mean=0 and stddev=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – mean of log of distribution</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – standard deviation of log of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.log_normal.LogNormal.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.log_normal.LogNormal.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.log_normal.LogNormal.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.log_normal.LogNormal.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.log_normal.LogNormal.loc">
<code class="descname">loc</code><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.loc" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.log_normal.LogNormal.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.log_normal.LogNormal.scale">
<code class="descname">scale</code><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.scale" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.log_normal.LogNormal.support">
<code class="descname">support</code><em class="property"> = GreaterThan(lower_bound=0.0)</em><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.log_normal.LogNormal.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.log_normal.LogNormal.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="lowrankmultivariatenormal">
<h3><span class="hidden-section">LowRankMultivariateNormal</span><a class="headerlink" href="#lowrankmultivariatenormal" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal">
<em class="property">class </em><code class="descclassname">torch.distributions.lowrank_multivariate_normal.</code><code class="descname">LowRankMultivariateNormal</code><span class="sig-paren">(</span><em>loc</em>, <em>cov_factor</em>, <em>cov_diag</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a multivariate normal distribution with covariance matrix having a low-rank form
parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">cov_factor</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">cov_diag</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">cov_factor</span> <span class="o">@</span> <span class="n">cov_factor</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">cov_diag</span>
</pre></div>
</div>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">LowRankMultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># normally distributed with mean=`[0,0]`, cov_factor=`[1,0]`, cov_diag=`[1,1]`</span>
<span class="go">tensor([-0.2102, -0.5429])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – mean of the distribution with shape <cite>batch_shape + event_shape</cite></p></li>
<li><p><strong>cov_factor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – factor part of low-rank form of covariance matrix with shape
<cite>batch_shape + event_shape + (rank,)</cite></p></li>
<li><p><strong>cov_diag</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – diagonal part of low-rank form of covariance matrix with shape
<cite>batch_shape + event_shape</cite></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The computation for determinant and inverse of covariance matrix is avoided when
<cite>cov_factor.shape[1] &lt;&lt; cov_factor.shape[0]</cite> thanks to <a class="reference external" href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> and
<a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_determinant_lemma">matrix determinant lemma</a>.
Thanks to these formulas, we just need to compute the determinant and inverse of
the small size “capacitance” matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">capacitance</span> <span class="o">=</span> <span class="n">I</span> <span class="o">+</span> <span class="n">cov_factor</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv</span><span class="p">(</span><span class="n">cov_diag</span><span class="p">)</span> <span class="o">@</span> <span class="n">cov_factor</span>
</pre></div>
</div>
</div>
<dl class="attribute">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'cov_diag': GreaterThan(lower_bound=0.0), 'cov_factor': Real(), 'loc': Real()}</em><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix">
<code class="descname">covariance_matrix</code><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix">
<code class="descname">precision_matrix</code><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril">
<code class="descname">scale_tril</code><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support">
<code class="descname">support</code><em class="property"> = Real()</em><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="multinomial">
<h3><span class="hidden-section">Multinomial</span><a class="headerlink" href="#multinomial" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.multinomial.Multinomial">
<em class="property">class </em><code class="descclassname">torch.distributions.multinomial.</code><code class="descname">Multinomial</code><span class="sig-paren">(</span><em>total_count=1</em>, <em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a Multinomial distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code> and
either <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.logits" title="torch.distributions.multinomial.Multinomial.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both). The innermost dimension of
<a class="reference internal" href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> indexes over categories. All other dimensions index over batches.</p>
<p>Note that <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code> need not be specified if only <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.log_prob" title="torch.distributions.multinomial.Multinomial.log_prob"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_prob()</span></code></a> is
called (see example below)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> must be non-negative, finite and have a non-zero sum,
and it will be normalized to sum to 1.</p>
</div>
<ul class="simple">
<li><p><a class="reference internal" href="#torch.distributions.multinomial.Multinomial.sample" title="torch.distributions.multinomial.Multinomial.sample"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sample()</span></code></a> requires a single shared <cite>total_count</cite> for all
parameters and samples.</p></li>
<li><p><a class="reference internal" href="#torch.distributions.multinomial.Multinomial.log_prob" title="torch.distributions.multinomial.Multinomial.log_prob"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_prob()</span></code></a> allows different <cite>total_count</cite> for each parameter and
sample.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Multinomial</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># equal probability of 0, 1, 2, 3</span>
<span class="go">tensor([ 21.,  24.,  30.,  25.])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Multinomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">tensor([-4.1338])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of trials</p></li>
<li><p><strong>probs</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event log probabilities</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.multinomial.Multinomial.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Simplex()}</em><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.multinomial.Multinomial.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.multinomial.Multinomial.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multinomial.Multinomial.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multinomial.Multinomial.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multinomial.Multinomial.param_shape">
<code class="descname">param_shape</code><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.param_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multinomial.Multinomial.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.multinomial.Multinomial.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multinomial.Multinomial.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multinomial.Multinomial.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.multinomial.Multinomial.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="multivariatenormal">
<h3><span class="hidden-section">MultivariateNormal</span><a class="headerlink" href="#multivariatenormal" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal">
<em class="property">class </em><code class="descclassname">torch.distributions.multivariate_normal.</code><code class="descname">MultivariateNormal</code><span class="sig-paren">(</span><em>loc</em>, <em>covariance_matrix=None</em>, <em>precision_matrix=None</em>, <em>scale_tril=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a multivariate normal (also called Gaussian) distribution
parameterized by a mean vector and a covariance matrix.</p>
<p>The multivariate normal distribution can be parameterized either
in terms of a positive definite covariance matrix <span class="math">\(\mathbf{\Sigma}\)</span>
or a positive definite precision matrix <span class="math">\(\mathbf{\Sigma}^{-1}\)</span>
or a lower-triangular matrix <span class="math">\(\mathbf{L}\)</span> with positive-valued
diagonal entries, such that
<span class="math">\(\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top\)</span>. This triangular matrix
can be obtained via e.g. Cholesky decomposition of the covariance.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># normally distributed with mean=`[0,0]` and covariance_matrix=`I`</span>
<span class="go">tensor([-0.2102, -0.5429])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – mean of the distribution</p></li>
<li><p><strong>covariance_matrix</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – positive-definite covariance matrix</p></li>
<li><p><strong>precision_matrix</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – positive-definite precision matrix</p></li>
<li><p><strong>scale_tril</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – lower-triangular factor of covariance, with positive-valued diagonal</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only one of <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">covariance_matrix</span></code></a> or <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">precision_matrix</span></code></a> or
<a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a> can be specified.</p>
<p>Using <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a> will be more efficient: all computations internally
are based on <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_tril</span></code></a>. If <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">covariance_matrix</span></code></a> or
<a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code class="xref py py-attr docutils literal notranslate"><span class="pre">precision_matrix</span></code></a> is passed instead, it is only used to compute
the corresponding lower triangular matrices using a Cholesky decomposition.</p>
</div>
<dl class="attribute">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'covariance_matrix': PositiveDefinite(), 'loc': RealVector(), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}</em><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix">
<code class="descname">covariance_matrix</code><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix">
<code class="descname">precision_matrix</code><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril">
<code class="descname">scale_tril</code><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.support">
<code class="descname">support</code><em class="property"> = Real()</em><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.multivariate_normal.MultivariateNormal.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.multivariate_normal.MultivariateNormal.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="negativebinomial">
<h3><span class="hidden-section">NegativeBinomial</span><a class="headerlink" href="#negativebinomial" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.negative_binomial.NegativeBinomial">
<em class="property">class </em><code class="descclassname">torch.distributions.negative_binomial.</code><code class="descname">NegativeBinomial</code><span class="sig-paren">(</span><em>total_count</em>, <em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a Negative Binomial distribution, i.e. distribution
of the number of independent identical Bernoulli trials
needed before <code class="xref py py-attr docutils literal notranslate"><span class="pre">total_count</span></code> failures are achieved. The probability
of success of each Bernoulli trial is <a class="reference internal" href="#torch.distributions.negative_binomial.NegativeBinomial.probs" title="torch.distributions.negative_binomial.NegativeBinomial.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – non-negative number of negative Bernoulli
trials to stop, although the distribution is still valid for real
valued count</p></li>
<li><p><strong>probs</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Event probabilities of success in the half open interval [0, 1)</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Event log-odds for probabilities of success</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)}</em><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.param_shape">
<code class="descname">param_shape</code><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.param_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.support">
<code class="descname">support</code><em class="property"> = IntegerGreaterThan(lower_bound=0)</em><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.negative_binomial.NegativeBinomial.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.negative_binomial.NegativeBinomial.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="normal">
<h3><span class="hidden-section">Normal</span><a class="headerlink" href="#normal" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.normal.Normal">
<em class="property">class </em><code class="descclassname">torch.distributions.normal.</code><code class="descname">Normal</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.normal.Normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.exp_family.ExponentialFamily</span></code></a></p>
<p>Creates a normal (also called Gaussian) distribution parameterized by
<code class="xref py py-attr docutils literal notranslate"><span class="pre">loc</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># normally distributed with loc=0 and scale=1</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – mean of the distribution (often referred to as mu)</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – standard deviation of the distribution
(often referred to as sigma)</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.normal.Normal.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.normal.Normal.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.normal.Normal.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.normal.Normal.cdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.normal.Normal.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.normal.Normal.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.normal.Normal.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.normal.Normal.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.normal.Normal.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.normal.Normal.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.normal.Normal.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.normal.Normal.icdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.normal.Normal.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.normal.Normal.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.normal.Normal.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.normal.Normal.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.normal.Normal.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.normal.Normal.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.normal.Normal.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.normal.Normal.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.normal.Normal.stddev">
<code class="descname">stddev</code><a class="headerlink" href="#torch.distributions.normal.Normal.stddev" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.normal.Normal.support">
<code class="descname">support</code><em class="property"> = Real()</em><a class="headerlink" href="#torch.distributions.normal.Normal.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.normal.Normal.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.normal.Normal.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="onehotcategorical">
<h3><span class="hidden-section">OneHotCategorical</span><a class="headerlink" href="#onehotcategorical" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical">
<em class="property">class </em><code class="descclassname">torch.distributions.one_hot_categorical.</code><code class="descname">OneHotCategorical</code><span class="sig-paren">(</span><em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a one-hot categorical distribution parameterized by <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or
<a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>.</p>
<p>Samples are one-hot coded vectors of size <code class="docutils literal notranslate"><span class="pre">probs.size(-1)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> must be non-negative, finite and have a non-zero sum,
and it will be normalized to sum to 1.</p>
</div>
<p>See also: <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributions.Categorical()</span></code> for specifications of
<a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> and <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">OneHotCategorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span> <span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># equal probability of 0, 1, 2, 3</span>
<span class="go">tensor([ 0.,  0.,  0.,  1.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>probs</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event log probabilities</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Simplex()}</em><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support">
<code class="descname">enumerate_support</code><span class="sig-paren">(</span><em>expand=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support">
<code class="descname">has_enumerate_support</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.param_shape">
<code class="descname">param_shape</code><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.support">
<code class="descname">support</code><em class="property"> = Simplex()</em><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.one_hot_categorical.OneHotCategorical.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.one_hot_categorical.OneHotCategorical.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="pareto">
<h3><span class="hidden-section">Pareto</span><a class="headerlink" href="#pareto" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.pareto.Pareto">
<em class="property">class </em><code class="descclassname">torch.distributions.pareto.</code><code class="descname">Pareto</code><span class="sig-paren">(</span><em>scale</em>, <em>alpha</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.pareto.Pareto" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.transformed_distribution.TransformedDistribution</span></code></a></p>
<p>Samples from a Pareto Type 1 distribution.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Pareto</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Pareto distribution with scale=1 and alpha=1</span>
<span class="go">tensor([ 1.5623])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Scale parameter of the distribution</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Shape parameter of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.pareto.Pareto.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.pareto.Pareto.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.pareto.Pareto.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.pareto.Pareto.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.pareto.Pareto.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.pareto.Pareto.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.pareto.Pareto.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.pareto.Pareto.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.pareto.Pareto.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.pareto.Pareto.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.pareto.Pareto.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.pareto.Pareto.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="poisson">
<h3><span class="hidden-section">Poisson</span><a class="headerlink" href="#poisson" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.poisson.Poisson">
<em class="property">class </em><code class="descclassname">torch.distributions.poisson.</code><code class="descname">Poisson</code><span class="sig-paren">(</span><em>rate</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.poisson.Poisson" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.exp_family.ExponentialFamily</span></code></a></p>
<p>Creates a Poisson distribution parameterized by <code class="xref py py-attr docutils literal notranslate"><span class="pre">rate</span></code>, the rate parameter.</p>
<p>Samples are nonnegative integers, with a pmf given by</p>
<div class="math">
\[\mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!}

\]</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([ 3.])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>rate</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the rate parameter</p>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.poisson.Poisson.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'rate': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.poisson.Poisson.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.poisson.Poisson.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.poisson.Poisson.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.poisson.Poisson.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.poisson.Poisson.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.poisson.Poisson.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.poisson.Poisson.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.poisson.Poisson.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.poisson.Poisson.sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.poisson.Poisson.support">
<code class="descname">support</code><em class="property"> = IntegerGreaterThan(lower_bound=0)</em><a class="headerlink" href="#torch.distributions.poisson.Poisson.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.poisson.Poisson.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.poisson.Poisson.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="relaxedbernoulli">
<h3><span class="hidden-section">RelaxedBernoulli</span><a class="headerlink" href="#relaxedbernoulli" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli">
<em class="property">class </em><code class="descclassname">torch.distributions.relaxed_bernoulli.</code><code class="descname">RelaxedBernoulli</code><span class="sig-paren">(</span><em>temperature</em>, <em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.transformed_distribution.TransformedDistribution</span></code></a></p>
<p>Creates a RelaxedBernoulli distribution, parametrized by
<a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature"><code class="xref py py-attr docutils literal notranslate"><span class="pre">temperature</span></code></a>, and either <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>
(but not both). This is a relaxed version of the <cite>Bernoulli</cite> distribution,
so the values are in (0, 1), and has reparametrizable samples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">RelaxedBernoulli</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.2</span><span class="p">]),</span>
<span class="go">                         torch.tensor([0.1, 0.2, 0.3, 0.99]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([ 0.2951,  0.3442,  0.8918,  0.9021])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – relaxation temperature</p></li>
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the probability of sampling <cite>1</cite></p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the log-odds of sampling <cite>1</cite></p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support">
<code class="descname">support</code><em class="property"> = Interval(lower_bound=0.0, upper_bound=1.0)</em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature">
<code class="descname">temperature</code><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="logitrelaxedbernoulli">
<h3><span class="hidden-section">LogitRelaxedBernoulli</span><a class="headerlink" href="#logitrelaxedbernoulli" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli">
<em class="property">class </em><code class="descclassname">torch.distributions.relaxed_bernoulli.</code><code class="descname">LogitRelaxedBernoulli</code><span class="sig-paren">(</span><em>temperature</em>, <em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a LogitRelaxedBernoulli distribution parameterized by <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs" title="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a>
or <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits" title="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a> (but not both), which is the logit of a RelaxedBernoulli
distribution.</p>
<p>Samples are logits of values in (0, 1). See [1] for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – relaxation temperature</p></li>
<li><p><strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the probability of sampling <cite>1</cite></p></li>
<li><p><strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the log-odds of sampling <cite>1</cite></p></li>
</ul>
</dd>
</dl>
<p>[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random
Variables (Maddison et al, 2017)</p>
<p>[2] Categorical Reparametrization with Gumbel-Softmax
(Jang et al, 2017)</p>
<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape">
<code class="descname">param_shape</code><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support">
<code class="descname">support</code><em class="property"> = Real()</em><a class="headerlink" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="relaxedonehotcategorical">
<h3><span class="hidden-section">RelaxedOneHotCategorical</span><a class="headerlink" href="#relaxedonehotcategorical" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical">
<em class="property">class </em><code class="descclassname">torch.distributions.relaxed_categorical.</code><code class="descname">RelaxedOneHotCategorical</code><span class="sig-paren">(</span><em>temperature</em>, <em>probs=None</em>, <em>logits=None</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.transformed_distribution.TransformedDistribution</span></code></a></p>
<p>Creates a RelaxedOneHotCategorical distribution parametrized by
<a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature"><code class="xref py py-attr docutils literal notranslate"><span class="pre">temperature</span></code></a>, and either <a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">probs</span></code></a> or <a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logits</span></code></a>.
This is a relaxed version of the <code class="xref py py-class docutils literal notranslate"><span class="pre">OneHotCategorical</span></code> distribution, so
its samples are on simplex, and are reparametrizable.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">RelaxedOneHotCategorical</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.2</span><span class="p">]),</span>
<span class="go">                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="go">tensor([ 0.1294,  0.2324,  0.3859,  0.2523])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – relaxation temperature</p></li>
<li><p><strong>probs</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event probabilities</p></li>
<li><p><strong>logits</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the log probability of each event.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'logits': Real(), 'probs': Simplex()}</em><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits">
<code class="descname">logits</code><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs">
<code class="descname">probs</code><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support">
<code class="descname">support</code><em class="property"> = Simplex()</em><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature">
<code class="descname">temperature</code><a class="headerlink" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="studentt">
<h3><span class="hidden-section">StudentT</span><a class="headerlink" href="#studentt" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.studentT.StudentT">
<em class="property">class </em><code class="descclassname">torch.distributions.studentT.</code><code class="descname">StudentT</code><span class="sig-paren">(</span><em>df</em>, <em>loc=0.0</em>, <em>scale=1.0</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.studentT.StudentT" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Creates a Student’s t-distribution parameterized by degree of
freedom <code class="xref py py-attr docutils literal notranslate"><span class="pre">df</span></code>, mean <code class="xref py py-attr docutils literal notranslate"><span class="pre">loc</span></code> and scale <code class="xref py py-attr docutils literal notranslate"><span class="pre">scale</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">StudentT</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Student&#39;s t-distributed with degrees of freedom=2</span>
<span class="go">tensor([ 0.1046])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>df</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – degrees of freedom</p></li>
<li><p><strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – mean of the distribution</p></li>
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – scale of the distribution</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.studentT.StudentT.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.studentT.StudentT.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.studentT.StudentT.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.studentT.StudentT.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.studentT.StudentT.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.studentT.StudentT.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.studentT.StudentT.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.studentT.StudentT.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.studentT.StudentT.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.studentT.StudentT.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.studentT.StudentT.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.studentT.StudentT.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.studentT.StudentT.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.studentT.StudentT.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.studentT.StudentT.support">
<code class="descname">support</code><em class="property"> = Real()</em><a class="headerlink" href="#torch.distributions.studentT.StudentT.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.studentT.StudentT.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.studentT.StudentT.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="transformeddistribution">
<h3><span class="hidden-section">TransformedDistribution</span><a class="headerlink" href="#transformeddistribution" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution">
<em class="property">class </em><code class="descclassname">torch.distributions.transformed_distribution.</code><code class="descname">TransformedDistribution</code><span class="sig-paren">(</span><em>base_distribution</em>, <em>transforms</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Extension of the Distribution class, which applies a sequence of Transforms
to a base distribution.  Let f be the composition of transforms applied:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~</span> <span class="n">BaseDistribution</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">~</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">BaseDistribution</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
<span class="n">log</span> <span class="n">p</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">=</span> <span class="n">log</span> <span class="n">p</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">log</span> <span class="o">|</span><span class="n">det</span> <span class="p">(</span><span class="n">dX</span><span class="o">/</span><span class="n">dY</span><span class="p">)</span><span class="o">|</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">.event_shape</span></code> of a <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a> is the
maximum shape of its base distribution and its transforms, since transforms
can introduce correlations among events.</p>
<p>An example for the usage of <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedDistribution</span></code></a> would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Building a Logistic Distribution</span>
<span class="c1"># X ~ Uniform(0, 1)</span>
<span class="c1"># f = a + b * logit(X)</span>
<span class="c1"># Y ~ f(X) ~ Logistic(a, b)</span>
<span class="n">base_distribution</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">SigmoidTransform</span><span class="p">()</span><span class="o">.</span><span class="n">inv</span><span class="p">,</span> <span class="n">AffineTransform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">b</span><span class="p">)]</span>
<span class="n">logistic</span> <span class="o">=</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">base_distribution</span><span class="p">,</span> <span class="n">transforms</span><span class="p">)</span>
</pre></div>
</div>
<p>For more examples, please look at the implementations of
<a class="reference internal" href="#torch.distributions.gumbel.Gumbel" title="torch.distributions.gumbel.Gumbel"><code class="xref py py-class docutils literal notranslate"><span class="pre">Gumbel</span></code></a>,
<a class="reference internal" href="#torch.distributions.half_cauchy.HalfCauchy" title="torch.distributions.half_cauchy.HalfCauchy"><code class="xref py py-class docutils literal notranslate"><span class="pre">HalfCauchy</span></code></a>,
<a class="reference internal" href="#torch.distributions.half_normal.HalfNormal" title="torch.distributions.half_normal.HalfNormal"><code class="xref py py-class docutils literal notranslate"><span class="pre">HalfNormal</span></code></a>,
<a class="reference internal" href="#torch.distributions.log_normal.LogNormal" title="torch.distributions.log_normal.LogNormal"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogNormal</span></code></a>,
<a class="reference internal" href="#torch.distributions.pareto.Pareto" title="torch.distributions.pareto.Pareto"><code class="xref py py-class docutils literal notranslate"><span class="pre">Pareto</span></code></a>,
<a class="reference internal" href="#torch.distributions.weibull.Weibull" title="torch.distributions.weibull.Weibull"><code class="xref py py-class docutils literal notranslate"><span class="pre">Weibull</span></code></a>,
<a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli"><code class="xref py py-class docutils literal notranslate"><span class="pre">RelaxedBernoulli</span></code></a> and
<a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical"><code class="xref py py-class docutils literal notranslate"><span class="pre">RelaxedOneHotCategorical</span></code></a></p>
<dl class="attribute">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {}</em><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.cdf" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cumulative distribution function by inverting the
transform(s) and computing the score of the base distribution.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.has_rsample">
<code class="descname">has_rsample</code><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.icdf" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the inverse cumulative distribution function using
transform(s) and computing the score of the base distribution.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Scores the sample by inverting the transform(s) and computing the score
using the score of the base distribution and the log abs det jacobian.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.rsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a sample_shape shaped reparameterized sample or sample_shape
shaped batch of reparameterized samples if the distribution parameters
are batched. Samples first from base distribution and applies
<cite>transform()</cite> for every transform in the list.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a sample_shape shaped sample or sample_shape shaped batch of
samples if the distribution parameters are batched. Samples first from
base distribution and applies <cite>transform()</cite> for every transform in the
list.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.transformed_distribution.TransformedDistribution.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.transformed_distribution.TransformedDistribution.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="uniform">
<h3><span class="hidden-section">Uniform</span><a class="headerlink" href="#uniform" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.uniform.Uniform">
<em class="property">class </em><code class="descclassname">torch.distributions.uniform.</code><code class="descname">Uniform</code><span class="sig-paren">(</span><em>low</em>, <em>high</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.uniform.Uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.distribution.Distribution</span></code></a></p>
<p>Generates uniformly distributed random samples from the half-open interval
<code class="docutils literal notranslate"><span class="pre">[low,</span> <span class="pre">high)</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># uniformly distributed in the range [0.0, 5.0)</span>
<span class="go">tensor([ 2.3418])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>low</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – lower range (inclusive).</p></li>
<li><p><strong>high</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – upper range (exclusive).</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.uniform.Uniform.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'high': Dependent(), 'low': Dependent()}</em><a class="headerlink" href="#torch.distributions.uniform.Uniform.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.uniform.Uniform.cdf">
<code class="descname">cdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.uniform.Uniform.cdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.uniform.Uniform.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.uniform.Uniform.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.uniform.Uniform.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.uniform.Uniform.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.uniform.Uniform.has_rsample">
<code class="descname">has_rsample</code><em class="property"> = True</em><a class="headerlink" href="#torch.distributions.uniform.Uniform.has_rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.uniform.Uniform.icdf">
<code class="descname">icdf</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.uniform.Uniform.icdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.uniform.Uniform.log_prob">
<code class="descname">log_prob</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.uniform.Uniform.log_prob" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.uniform.Uniform.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.uniform.Uniform.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.uniform.Uniform.rsample">
<code class="descname">rsample</code><span class="sig-paren">(</span><em>sample_shape=torch.Size([])</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.uniform.Uniform.rsample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.uniform.Uniform.stddev">
<code class="descname">stddev</code><a class="headerlink" href="#torch.distributions.uniform.Uniform.stddev" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.uniform.Uniform.support">
<code class="descname">support</code><a class="headerlink" href="#torch.distributions.uniform.Uniform.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.uniform.Uniform.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.uniform.Uniform.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="weibull">
<h3><span class="hidden-section">Weibull</span><a class="headerlink" href="#weibull" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.weibull.Weibull">
<em class="property">class </em><code class="descclassname">torch.distributions.weibull.</code><code class="descname">Weibull</code><span class="sig-paren">(</span><em>scale</em>, <em>concentration</em>, <em>validate_args=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.weibull.Weibull" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.transformed_distribution.TransformedDistribution</span></code></a></p>
<p>Samples from a two-parameter Weibull distribution.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Weibull</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># sample from a Weibull distribution with scale=1, concentration=1</span>
<span class="go">tensor([ 0.4784])</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Scale parameter of distribution (lambda).</p></li>
<li><p><strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Concentration parameter of distribution (k/shape).</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.weibull.Weibull.arg_constraints">
<code class="descname">arg_constraints</code><em class="property"> = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}</em><a class="headerlink" href="#torch.distributions.weibull.Weibull.arg_constraints" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.weibull.Weibull.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.weibull.Weibull.entropy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.distributions.weibull.Weibull.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>batch_shape</em>, <em>_instance=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.weibull.Weibull.expand" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.weibull.Weibull.mean">
<code class="descname">mean</code><a class="headerlink" href="#torch.distributions.weibull.Weibull.mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.weibull.Weibull.support">
<code class="descname">support</code><em class="property"> = GreaterThan(lower_bound=0.0)</em><a class="headerlink" href="#torch.distributions.weibull.Weibull.support" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.distributions.weibull.Weibull.variance">
<code class="descname">variance</code><a class="headerlink" href="#torch.distributions.weibull.Weibull.variance" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-torch.distributions.kl">
<span id="kl-divergence"></span><h3><cite>KL Divergence</cite><a class="headerlink" href="#module-torch.distributions.kl" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.distributions.kl.kl_divergence">
<code class="descclassname">torch.distributions.kl.</code><code class="descname">kl_divergence</code><span class="sig-paren">(</span><em>p</em>, <em>q</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.kl.kl_divergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Kullback-Leibler divergence <span class="math">\(KL(p \| q)\)</span> between two distributions.</p>
<div class="math">
\[KL(p \| q) = \int p(x) \log\frac {p(x)} {q(x)} \,dx\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> (<a class="reference internal" href="index.html#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>Distribution</em></a>) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code> object.</p></li>
<li><p><strong>q</strong> (<a class="reference internal" href="index.html#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>Distribution</em></a>) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code> object.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A batch of KL divergences of shape <cite>batch_shape</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.7)"><strong>NotImplementedError</strong></a> – If the distribution types have not been registered via
    <a class="reference internal" href="#torch.distributions.kl.register_kl" title="torch.distributions.kl.register_kl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_kl()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributions.kl.register_kl">
<code class="descclassname">torch.distributions.kl.</code><code class="descname">register_kl</code><span class="sig-paren">(</span><em>type_p</em>, <em>type_q</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.kl.register_kl" title="Permalink to this definition">¶</a></dt>
<dd><p>Decorator to register a pairwise function with <a class="reference internal" href="#torch.distributions.kl.kl_divergence" title="torch.distributions.kl.kl_divergence"><code class="xref py py-meth docutils literal notranslate"><span class="pre">kl_divergence()</span></code></a>.
Usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_kl</span><span class="p">(</span><span class="n">Normal</span><span class="p">,</span> <span class="n">Normal</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kl_normal_normal</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="c1"># insert implementation here</span>
</pre></div>
</div>
<p>Lookup returns the most specific (type,type) match ordered by subclass. If
the match is ambiguous, a <cite>RuntimeWarning</cite> is raised. For example to
resolve the ambiguous situation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_kl</span><span class="p">(</span><span class="n">BaseP</span><span class="p">,</span> <span class="n">DerivedQ</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kl_version1</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span> <span class="o">...</span>
<span class="nd">@register_kl</span><span class="p">(</span><span class="n">DerivedP</span><span class="p">,</span> <span class="n">BaseQ</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kl_version2</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>you should register a third most-specific implementation, e.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">register_kl</span><span class="p">(</span><span class="n">DerivedP</span><span class="p">,</span> <span class="n">DerivedQ</span><span class="p">)(</span><span class="n">kl_version1</span><span class="p">)</span>  <span class="c1"># Break the tie.</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>type_p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a>) – A subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code>.</p></li>
<li><p><strong>type_q</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a>) – A subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">Distribution</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-torch.distributions.transforms">
<span id="transforms"></span><h3><cite>Transforms</cite><a class="headerlink" href="#module-torch.distributions.transforms" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.distributions.transforms.Transform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">Transform</code><span class="sig-paren">(</span><em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.Transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract class for invertable transformations with computable log
det jacobians. They are primarily used in
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.TransformedDistribution</span></code>.</p>
<p>Caching is useful for tranforms whose inverses are either expensive or
numerically unstable. Note that care must be taken with memoized values
since the autograd graph may be reversed. For example while the following
works with or without caching:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">t</span><span class="o">.</span><span class="n">log_abs_det_jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># x will receive gradients.</span>
</pre></div>
</div>
<p>However the following will error when caching due to dependency reversal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="p">[</span><span class="n">y</span><span class="p">])</span>  <span class="c1"># error because z is x</span>
</pre></div>
</div>
<p>Derived classes should implement one or both of <code class="xref py py-meth docutils literal notranslate"><span class="pre">_call()</span></code> or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">_inverse()</span></code>. Derived classes that set <cite>bijective=True</cite> should also
implement <a class="reference internal" href="#torch.distributions.transforms.Transform.log_abs_det_jacobian" title="torch.distributions.transforms.Transform.log_abs_det_jacobian"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log_abs_det_jacobian()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cache_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Size of cache. If zero, no caching is done. If one,
the latest single value is cached. Only 0 and 1 are supported.</p>
</dd>
<dt class="field-even">Variables</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>~Transform.domain</strong> (<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>) – The constraint representing valid inputs to this transform.</p></li>
<li><p><strong>~Transform.codomain</strong> (<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>) – The constraint representing valid outputs to this transform
which are inputs to the inverse transform.</p></li>
<li><p><strong>~Transform.bijective</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – Whether this transform is bijective. A transform
<code class="docutils literal notranslate"><span class="pre">t</span></code> is bijective iff <code class="docutils literal notranslate"><span class="pre">t.inv(t(x))</span> <span class="pre">==</span> <span class="pre">x</span></code> and
<code class="docutils literal notranslate"><span class="pre">t(t.inv(y))</span> <span class="pre">==</span> <span class="pre">y</span></code> for every <code class="docutils literal notranslate"><span class="pre">x</span></code> in the domain and <code class="docutils literal notranslate"><span class="pre">y</span></code> in
the codomain. Transforms that are not bijective should at least
maintain the weaker pseudoinverse properties
<code class="docutils literal notranslate"><span class="pre">t(t.inv(t(x))</span> <span class="pre">==</span> <span class="pre">t(x)</span></code> and <code class="docutils literal notranslate"><span class="pre">t.inv(t(t.inv(y)))</span> <span class="pre">==</span> <span class="pre">t.inv(y)</span></code>.</p></li>
<li><p><strong>~Transform.sign</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – For bijective univariate transforms, this
should be +1 or -1 depending on whether transform is monotone
increasing or decreasing.</p></li>
<li><p><strong>~Transform.event_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of dimensions that are correlated together in
the transform <code class="docutils literal notranslate"><span class="pre">event_shape</span></code>. This should be 0 for pointwise
transforms, 1 for transforms that act jointly on vectors, 2 for
transforms that act jointly on matrices, etc.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="torch.distributions.transforms.Transform.inv">
<code class="descname">inv</code><a class="headerlink" href="#torch.distributions.transforms.Transform.inv" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the inverse <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> of this transform.
This should satisfy <code class="docutils literal notranslate"><span class="pre">t.inv.inv</span> <span class="pre">is</span> <span class="pre">t</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.transforms.Transform.sign">
<code class="descname">sign</code><a class="headerlink" href="#torch.distributions.transforms.Transform.sign" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the sign of the determinant of the Jacobian, if applicable.
In general this only makes sense for bijective transforms.</p>
</dd></dl>

<dl class="method">
<dt id="torch.distributions.transforms.Transform.log_abs_det_jacobian">
<code class="descname">log_abs_det_jacobian</code><span class="sig-paren">(</span><em>x</em>, <em>y</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.Transform.log_abs_det_jacobian" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the log det jacobian <cite>log |dy/dx|</cite> given input and output.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.ComposeTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">ComposeTransform</code><span class="sig-paren">(</span><em>parts</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.ComposeTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Composes multiple transforms in a chain.
The transforms being composed are responsible for caching.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>parts</strong> (list of <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a>) – A list of transforms to compose.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.ExpTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">ExpTransform</code><span class="sig-paren">(</span><em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.ExpTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform via the mapping <span class="math">\(y = \exp(x)\)</span>.</p>
</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.PowerTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">PowerTransform</code><span class="sig-paren">(</span><em>exponent</em>, <em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.PowerTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform via the mapping <span class="math">\(y = x^{\text{exponent}}\)</span>.</p>
</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.SigmoidTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">SigmoidTransform</code><span class="sig-paren">(</span><em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.SigmoidTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform via the mapping <span class="math">\(y = \frac{1}{1 + \exp(-x)}\)</span> and <span class="math">\(x = \text{logit}(y)\)</span>.</p>
</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.AbsTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">AbsTransform</code><span class="sig-paren">(</span><em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.AbsTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform via the mapping <span class="math">\(y = |x|\)</span>.</p>
</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.AffineTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">AffineTransform</code><span class="sig-paren">(</span><em>loc</em>, <em>scale</em>, <em>event_dim=0</em>, <em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.AffineTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform via the pointwise affine mapping <span class="math">\(y = \text{loc} + \text{scale} \times x\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loc</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Location parameter.</p></li>
<li><p><strong>scale</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Scale parameter.</p></li>
<li><p><strong>event_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Optional size of <cite>event_shape</cite>. This should be zero
for univariate random variables, 1 for distributions over vectors,
2 for distributions over matrices, etc.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.SoftmaxTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">SoftmaxTransform</code><span class="sig-paren">(</span><em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.SoftmaxTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform from unconstrained space to the simplex via <span class="math">\(y = \exp(x)\)</span> then
normalizing.</p>
<p>This is not bijective and cannot be used for HMC. However this acts mostly
coordinate-wise (except for the final normalization), and thus is
appropriate for coordinate-wise optimization algorithms.</p>
</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.StickBreakingTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">StickBreakingTransform</code><span class="sig-paren">(</span><em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.StickBreakingTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform from unconstrained space to the simplex of one additional
dimension via a stick-breaking process.</p>
<p>This transform arises as an iterated sigmoid transform in a stick-breaking
construction of the <cite>Dirichlet</cite> distribution: the first logit is
transformed via sigmoid to the first probability and the probability of
everything else, and then the process recurses.</p>
<p>This is bijective and appropriate for use in HMC; however it mixes
coordinates together and is less appropriate for optimization.</p>
</dd></dl>

<dl class="class">
<dt id="torch.distributions.transforms.LowerCholeskyTransform">
<em class="property">class </em><code class="descclassname">torch.distributions.transforms.</code><code class="descname">LowerCholeskyTransform</code><span class="sig-paren">(</span><em>cache_size=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.transforms.LowerCholeskyTransform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform from unconstrained matrices to lower-triangular matrices with
nonnegative diagonal entries.</p>
<p>This is useful for parameterizing positive definite matrices in terms of
their Cholesky factorization.</p>
</dd></dl>

</div>
<div class="section" id="module-torch.distributions.constraints">
<span id="constraints"></span><h3><cite>Constraints</cite><a class="headerlink" href="#module-torch.distributions.constraints" title="Permalink to this headline">¶</a></h3>
<p>The following constraints are implemented:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.boolean</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.dependent</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.greater_than(lower_bound)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.integer_interval(lower_bound,</span> <span class="pre">upper_bound)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.interval(lower_bound,</span> <span class="pre">upper_bound)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.lower_cholesky</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.lower_triangular</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.nonnegative_integer</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.positive</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.positive_definite</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.positive_integer</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.real</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.real_vector</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.simplex</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">constraints.unit_interval</span></code></p></li>
</ul>
<dl class="class">
<dt id="torch.distributions.constraints.Constraint">
<em class="property">class </em><code class="descclassname">torch.distributions.constraints.</code><code class="descname">Constraint</code><a class="headerlink" href="#torch.distributions.constraints.Constraint" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract base class for constraints.</p>
<p>A constraint object represents a region over which a variable is valid,
e.g. within which a variable can be optimized.</p>
<dl class="method">
<dt id="torch.distributions.constraints.Constraint.check">
<code class="descname">check</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.constraints.Constraint.check" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a byte tensor of <cite>sample_shape + batch_shape</cite> indicating
whether each event in value satisfies this constraint.</p>
</dd></dl>

</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.constraints.dependent_property">
<code class="descclassname">torch.distributions.constraints.</code><code class="descname">dependent_property</code><a class="headerlink" href="#torch.distributions.constraints.dependent_property" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.constraints._DependentProperty</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.constraints.integer_interval">
<code class="descclassname">torch.distributions.constraints.</code><code class="descname">integer_interval</code><a class="headerlink" href="#torch.distributions.constraints.integer_interval" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.constraints._IntegerInterval</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.constraints.greater_than">
<code class="descclassname">torch.distributions.constraints.</code><code class="descname">greater_than</code><a class="headerlink" href="#torch.distributions.constraints.greater_than" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.constraints._GreaterThan</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.constraints.greater_than_eq">
<code class="descclassname">torch.distributions.constraints.</code><code class="descname">greater_than_eq</code><a class="headerlink" href="#torch.distributions.constraints.greater_than_eq" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.constraints._GreaterThanEq</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.constraints.less_than">
<code class="descclassname">torch.distributions.constraints.</code><code class="descname">less_than</code><a class="headerlink" href="#torch.distributions.constraints.less_than" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.constraints._LessThan</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.constraints.interval">
<code class="descclassname">torch.distributions.constraints.</code><code class="descname">interval</code><a class="headerlink" href="#torch.distributions.constraints.interval" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.constraints._Interval</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="torch.distributions.constraints.half_open_interval">
<code class="descclassname">torch.distributions.constraints.</code><code class="descname">half_open_interval</code><a class="headerlink" href="#torch.distributions.constraints.half_open_interval" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributions.constraints._HalfOpenInterval</span></code></p>
</dd></dl>

</div>
<div class="section" id="module-torch.distributions.constraint_registry">
<span id="constraint-registry"></span><h3><cite>Constraint Registry</cite><a class="headerlink" href="#module-torch.distributions.constraint_registry" title="Permalink to this headline">¶</a></h3>
<p>PyTorch provides two global <a class="reference internal" href="#torch.distributions.constraint_registry.ConstraintRegistry" title="torch.distributions.constraint_registry.ConstraintRegistry"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConstraintRegistry</span></code></a> objects that link
<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a> objects to
<a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> objects. These objects both
input constraints and return transforms, but they have different guarantees on
bijectivity.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">biject_to(constraint)</span></code> looks up a bijective
<a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> from <code class="docutils literal notranslate"><span class="pre">constraints.real</span></code>
to the given <code class="docutils literal notranslate"><span class="pre">constraint</span></code>. The returned transform is guaranteed to have
<code class="docutils literal notranslate"><span class="pre">.bijective</span> <span class="pre">=</span> <span class="pre">True</span></code> and should implement <code class="docutils literal notranslate"><span class="pre">.log_abs_det_jacobian()</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transform_to(constraint)</span></code> looks up a not-necessarily bijective
<a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> from <code class="docutils literal notranslate"><span class="pre">constraints.real</span></code>
to the given <code class="docutils literal notranslate"><span class="pre">constraint</span></code>. The returned transform is not guaranteed to
implement <code class="docutils literal notranslate"><span class="pre">.log_abs_det_jacobian()</span></code>.</p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">transform_to()</span></code> registry is useful for performing unconstrained
optimization on constrained parameters of probability distributions, which are
indicated by each distribution’s <code class="docutils literal notranslate"><span class="pre">.arg_constraints</span></code> dict. These transforms often
overparameterize a space in order to avoid rotation; they are thus more
suitable for coordinate-wise optimization algorithms like Adam:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">unconstrained</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">transform_to</span><span class="p">(</span><span class="n">Normal</span><span class="o">.</span><span class="n">arg_constraints</span><span class="p">[</span><span class="s1">&#39;scale&#39;</span><span class="p">])(</span><span class="n">unconstrained</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">biject_to()</span></code> registry is useful for Hamiltonian Monte Carlo, where
samples from a probability distribution with constrained <code class="docutils literal notranslate"><span class="pre">.support</span></code> are
propagated in an unconstrained space, and algorithms are typically rotation
invariant.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
<span class="n">unconstrained</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">biject_to</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">support</span><span class="p">)(</span><span class="n">unconstrained</span><span class="p">)</span>
<span class="n">potential_energy</span> <span class="o">=</span> <span class="o">-</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An example where <code class="docutils literal notranslate"><span class="pre">transform_to</span></code> and <code class="docutils literal notranslate"><span class="pre">biject_to</span></code> differ is
<code class="docutils literal notranslate"><span class="pre">constraints.simplex</span></code>: <code class="docutils literal notranslate"><span class="pre">transform_to(constraints.simplex)</span></code> returns a
<a class="reference internal" href="#torch.distributions.transforms.SoftmaxTransform" title="torch.distributions.transforms.SoftmaxTransform"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftmaxTransform</span></code></a> that simply
exponentiates and normalizes its inputs; this is a cheap and mostly
coordinate-wise operation appropriate for algorithms like SVI. In
contrast, <code class="docutils literal notranslate"><span class="pre">biject_to(constraints.simplex)</span></code> returns a
<a class="reference internal" href="#torch.distributions.transforms.StickBreakingTransform" title="torch.distributions.transforms.StickBreakingTransform"><code class="xref py py-class docutils literal notranslate"><span class="pre">StickBreakingTransform</span></code></a> that
bijects its input down to a one-fewer-dimensional space; this a more
expensive less numerically stable transform but is needed for algorithms
like HMC.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">biject_to</span></code> and <code class="docutils literal notranslate"><span class="pre">transform_to</span></code> objects can be extended by user-defined
constraints and transforms using their <code class="docutils literal notranslate"><span class="pre">.register()</span></code> method either as a
function on singleton constraints:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">transform_to</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">my_constraint</span><span class="p">,</span> <span class="n">my_transform</span><span class="p">)</span>
</pre></div>
</div>
<p>or as a decorator on parameterized constraints:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@transform_to</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">MyConstraintClass</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">my_factory</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">constraint</span><span class="p">,</span> <span class="n">MyConstraintClass</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MyTransform</span><span class="p">(</span><span class="n">constraint</span><span class="o">.</span><span class="n">param1</span><span class="p">,</span> <span class="n">constraint</span><span class="o">.</span><span class="n">param2</span><span class="p">)</span>
</pre></div>
</div>
<p>You can create your own registry by creating a new <a class="reference internal" href="#torch.distributions.constraint_registry.ConstraintRegistry" title="torch.distributions.constraint_registry.ConstraintRegistry"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConstraintRegistry</span></code></a>
object.</p>
<dl class="class">
<dt id="torch.distributions.constraint_registry.ConstraintRegistry">
<em class="property">class </em><code class="descclassname">torch.distributions.constraint_registry.</code><code class="descname">ConstraintRegistry</code><a class="headerlink" href="#torch.distributions.constraint_registry.ConstraintRegistry" title="Permalink to this definition">¶</a></dt>
<dd><p>Registry to link constraints to transforms.</p>
<dl class="method">
<dt id="torch.distributions.constraint_registry.ConstraintRegistry.register">
<code class="descname">register</code><span class="sig-paren">(</span><em>constraint</em>, <em>factory=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributions.constraint_registry.ConstraintRegistry.register" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>
subclass in this registry. Usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@my_registry</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">MyConstraintClass</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">construct_transform</span><span class="p">(</span><span class="n">constraint</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">constraint</span><span class="p">,</span> <span class="n">MyConstraint</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">MyTransform</span><span class="p">(</span><span class="n">constraint</span><span class="o">.</span><span class="n">arg_constraints</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>constraint</strong> (subclass of <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>) – A subclass of <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Constraint</span></code></a>, or
a singleton object of the desired class.</p></li>
<li><p><strong>factory</strong> (<em>callable</em>) – A callable that inputs a constraint object and returns
a  <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code class="xref py py-class docutils literal notranslate"><span class="pre">Transform</span></code></a> object.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<span id="document-jit"></span><div class="section" id="torchscript">
<h2>TorchScript<a class="headerlink" href="#torchscript" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#creating-torchscript-code" id="id1">Creating TorchScript Code</a></p></li>
<li><p><a class="reference internal" href="#mixing-tracing-and-scripting" id="id2">Mixing Tracing and Scripting</a></p></li>
<li><p><a class="reference internal" href="#torchscript-language-reference" id="id3">TorchScript Language Reference</a></p>
<ul>
<li><p><a class="reference internal" href="#types" id="id4">Types</a></p></li>
<li><p><a class="reference internal" href="#expressions" id="id5">Expressions</a></p></li>
<li><p><a class="reference internal" href="#statements" id="id6">Statements</a></p></li>
<li><p><a class="reference internal" href="#variable-resolution" id="id7">Variable Resolution</a></p></li>
<li><p><a class="reference internal" href="#use-of-python-values" id="id8">Use of Python Values</a></p></li>
<li><p><a class="reference internal" href="#debugging" id="id9">Debugging</a></p></li>
<li><p><a class="reference internal" href="#builtin-functions" id="id10">Builtin Functions</a></p>
<ul>
<li><p><a class="reference internal" href="#supported-functions" id="id11">Supported Functions</a></p></li>
<li><p><a class="reference internal" href="#supported-methods" id="id12">Supported Methods</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#frequently-asked-questions" id="id13">Frequently Asked Questions</a></p></li>
</ul>
</div>
<span class="target" id="module-torch.jit"></span><p>TorchScript is a way to create serializable and optimizable models from PyTorch code.
Any code written in TorchScript can be saved from your Python
process and loaded in a process where there is no Python dependency.</p>
<p>We provide tools to incrementally transition a model from being a pure Python program
to a TorchScript program that can be run independently from Python, for instance, in a standalone C++ program.
This makes it possible to train models in PyTorch using familiar tools and then export
the model to a production environment where it is not a good idea to run models as Python programs
for performance and multi-threading reasons.</p>
<div class="section" id="creating-torchscript-code">
<h3><a class="toc-backref" href="#id1">Creating TorchScript Code</a><a class="headerlink" href="#creating-torchscript-code" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.jit.ScriptModule">
<em class="property">class </em><code class="descclassname">torch.jit.</code><code class="descname">ScriptModule</code><span class="sig-paren">(</span><em>optimize=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.jit.ScriptModule" title="Permalink to this definition">¶</a></dt>
<dd><p>The core data structure in TorchScript is the <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code>. It is an
analogue of torch’s nn.Module and represents an entire model as a tree of
submodules. Like normal modules, each individual module in a ScriptModule can
have submodules, parameters, and methods. In nn.Modules methods are implemented
as Python functions, but in ScriptModules methods typically implemented as
<em>TorchScript</em> functions,  a statically-typed subset of Python that contains all
of PyTorch’s built-in Tensor operations. This difference allows your
ScriptModules code to run without the need for a Python interpreter.</p>
<p>ScriptModules and the TorchScript functions inside of them can be created in
two ways:</p>
<p><strong>Tracing:</strong></p>
<blockquote>
<div><p>Using <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code>, you can take an existing module or python
function, provide example inputs, and we run the function, recording the
operations performed on all the tensors. We turn the resulting recording
into a TorchScript method that is installed as the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method of a
ScriptModule. This module also contains any parameters that the original
module had as well.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">traced_foo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">foo</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Tracing a <em>function</em> will produce a <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code> with a single
<code class="docutils literal notranslate"><span class="pre">forward</span></code> method that implements that function, and that contains
no parameters.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="n">traced_net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(),</span>
                             <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Tracing only records operations done when the given function is run on the given
tensors. Therefore, the returned <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code> will always run the same traced
graph on any input. This has some important implications when your module is
expected to run different sets of operations, depending on the input and/or the
module state. For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>Tracing will not record any control-flow like if statements or loops. When
this control-flow is constant across your module, this is fine and it often
just inlines configuration decisions. But sometimes the control-flow is
actually part of the model itself. For instance, a recurrent network is
a loop over the (possibly dynamic) length of an input sequence.</p></li>
<li><p>In the returned <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code>, operations that have different behaviors
in <code class="docutils literal notranslate"><span class="pre">training</span></code> and <code class="docutils literal notranslate"><span class="pre">eval</span></code> modes will always behave as if it is in the
mode it was in during tracing, no matter which mode the <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code>
is in.</p></li>
</ul>
</div></blockquote>
<p>In cases like these, tracing would not be appropriate and scripting is a better
choice.</p>
</div>
</div></blockquote>
<p><strong>Scripting:</strong></p>
<blockquote>
<div><p>You can write TorchScript code directly using Python syntax. You do this
using the <code class="docutils literal notranslate"><span class="pre">torch.jit.script</span></code> annotation (for functions) or
<code class="docutils literal notranslate"><span class="pre">torch.jit.script_method</span></code> annotation (for methods) on subclasses of
ScriptModule. With this annotation the body of the annotated function is
directly translated into TorchScript. TorchScript itself is a subset of
the Python language, so not all features in python work, but we provide
enough functionality to compute on tensors and do control-dependent
operations.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">():</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">r</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A script <em>function</em> annotation will construct a ScriptModule
with a single <code class="docutils literal notranslate"><span class="pre">forward</span></code> method that implements that function,
and that contains no parameters.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.jit</span> <span class="k">import</span> <span class="n">ScriptModule</span><span class="p">,</span> <span class="n">script_method</span><span class="p">,</span> <span class="n">trace</span>

<span class="k">class</span> <span class="nc">MyScriptModule</span><span class="p">(</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyScriptModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># trace produces a ScriptModule&#39;s conv1 and conv2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">trace</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">trace</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>

    <span class="nd">@script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
      <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
      <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
      <span class="k">return</span> <span class="nb">input</span>
</pre></div>
</div>
</div></blockquote>
<dl class="method">
<dt id="torch.jit.ScriptModule.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>filename</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.jit.ScriptModule.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save an offline version of this module for use in a separate process. The saved
module serializes all of the methods and parameters of this module. It can be
loaded into the C++ API using <code class="docutils literal notranslate"><span class="pre">torch::jit::load(filename)</span></code> or into the Python
API with <code class="docutils literal notranslate"><span class="pre">torch.jit.load(filename)</span></code>.</p>
<p>To be able to save a module, it must not make any calls to native python functions.
This means that all submodules must be subclasses of ScriptModules as well.</p>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>All modules, no matter their device, are always loaded onto the CPU during loading.
This is different from <a class="reference internal" href="index.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>’s semantics and may change in the future.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="torch.jit.load">
<code class="descclassname">torch.jit.</code><code class="descname">load</code><span class="sig-paren">(</span><em>f</em>, <em>map_location=None</em>, <em>_extra_files=ExtraFilesMap{}</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.jit.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code> previously saved with <code class="xref py py-func docutils literal notranslate"><span class="pre">save</span></code></p>
<p>All previously saved modules, no matter their device, are first loaded onto CPU,
and then are moved to the devices they were saved from. If this fails (e.g. because
the run time system doesn’t have certain devices), an exception is raised.
However, storages can be dynamically remapped to an alternative set of devices
using the <cite>map_location</cite> argument. Comparing to <a class="reference internal" href="index.html#torch.load" title="torch.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>, <cite>map_location</cite>
in this function is simplified, which only accepts a string (e.g., ‘cpu’, ‘cuda:0’),
or torch.device (e.g., torch.device(‘cpu’))</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>f</strong> – a file-like object (has to implement read, readline, tell, and seek),
or a string containing a file name</p></li>
<li><p><strong>map_location</strong> – can a string (e.g., ‘cpu’, ‘cuda:0’), a device (e.g.,
torch.device(‘cpu’))</p></li>
<li><p><strong>_extra_files</strong> – map from filename to content. The extra
filenames given in the map would be loaded and their content
would be stored in the provided map.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code> object.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;scriptmodule.pt&#39;</span><span class="p">)</span>
<span class="go"># Load ScriptModule from io.BytesIO object</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;scriptmodule.pt&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="go">        buffer = io.BytesIO(f.read())</span>
<span class="go"># Load all tensors to the original device</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
<span class="go"># Load all tensors onto CPU, using a device</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span>
<span class="go"># Load all tensors onto CPU, using a string</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="go"># Load with extra files.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">files</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;metadata.json&#39;</span> <span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;scriptmodule.pt&#39;</span><span class="p">,</span> <span class="n">_extra_files</span> <span class="o">=</span> <span class="n">files</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="p">(</span><span class="n">files</span><span class="p">[</span><span class="s1">&#39;metadata.json&#39;</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.jit.trace">
<code class="descclassname">torch.jit.</code><code class="descname">trace</code><span class="sig-paren">(</span><em>func</em>, <em>example_inputs</em>, <em>optimize=True</em>, <em>check_trace=True</em>, <em>check_inputs=None</em>, <em>check_tolerance=1e-05</em>, <em>_force_outplace=False</em>, <em>_module_class=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.jit.trace" title="Permalink to this definition">¶</a></dt>
<dd><p>Trace a function and return an executable trace that will be optimized
using just-in-time compilation.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Tracing only correctly records functions and modules which are not data
dependent (e.g., have conditionals on data in tensors) and do not have
any untracked external dependencies (e.g., perform input/output or
access global variables). If you trace such models, you may silently get
incorrect results on subsequent invocations of the model. The tracer
will try to emit warnings when doing something that may cause an
incorrect trace to be produced.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>func</strong> (<em>callable</em><em> or </em><a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>torch.nn.Module</em></a>) – a python function or torch.nn.Module
that will be run with example_inputs.
arguments and returns to func must be Tensors
or (possibly nested) tuples that
contain tensors.</p></li>
<li><p><strong>example_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – a tuple of example inputs that will be passed to the function
while tracing. The resulting trace can be run with
inputs of different types and shapes assuming the traced operations
support those types and shapes. example_inputs may also be a single
Tensor in which case it is automatically wrapped in a tuple</p></li>
</ul>
</dd>
<dt class="field-even">Keyword Arguments</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>optimize</strong> (<a class="reference internal" href="index.html#torch.FloatStorage.bool" title="torch.FloatStorage.bool"><em>bool</em></a><em>, </em><em>optional</em>) – whether or not to apply optimizations.  Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>check_trace</strong> (<a class="reference internal" href="index.html#torch.FloatStorage.bool" title="torch.FloatStorage.bool"><em>bool</em></a><em>, </em><em>optional</em>) – check if the same inputs run through
traced code produce the same outputs. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>. You might want
to disable this if, for example, your network contains non-
deterministic ops or if you are sure that the network is correct despite
a checker failure.</p></li>
<li><p><strong>check_inputs</strong> (<em>list of tuples</em><em>, </em><em>optional</em>) – A list of tuples of input arguments that should be used
to check the trace against what is expected. Each tuple
is equivalent to a seet of input arguments that would
be specified in <code class="docutils literal notranslate"><span class="pre">args</span></code>. For best results, pass in a
set of checking inputs representative of the space of
shapes and types of inputs you expect the network to see.
If not specified, the original <code class="docutils literal notranslate"><span class="pre">args</span></code> is used for checking</p></li>
<li><p><strong>check_tolerance</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Floating-point comparison tolerance to use in the checker procedure.
This can be used to relax the checker strictness in the event that
results diverge numerically for a known reason, such as operator fusion.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code> object with a single <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method containing the traced code.
When func is a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>, the returned <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code> will have the same set of
sub-modules and parameters as func.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">traced_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="mixing-tracing-and-scripting">
<h3><a class="toc-backref" href="#id2">Mixing Tracing and Scripting</a><a class="headerlink" href="#mixing-tracing-and-scripting" title="Permalink to this headline">¶</a></h3>
<p>In many cases either tracing or script is an easier approach for converting a model.
We allow you to compose tracing and scripting to suit the particular requirements
of a part of a model.</p>
<p>Scripted functions can call traced ones. This is particularly useful when you need
to use control-flow around a simple feed-forward model. For instance the beam search
of a sequence to sequence model will typically be written in script but can call an
encoder module generated using tracing.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">traced_foo</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">foo</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">bar</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">traced_foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Traced functions can call script functions. This is useful when a small part of
a model requires some control-flow even though most of the model is just a feed-forward
network. Control-flow inside of a script function called by a traced function is
preserved correctly:</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">():</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">r</span>


<span class="k">def</span> <span class="nf">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">z</span>

<span class="n">traced_bar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">bar</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>This composition also works for modules as well, where it can be used to generate
a submodule using tracing that can be called from the methods of a script module:</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="k">class</span> <span class="nc">MyScriptModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyScriptModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">103.939</span><span class="p">,</span> <span class="mf">116.779</span><span class="p">,</span> <span class="mf">123.68</span><span class="p">])</span>
                                        <span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resnet</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">resnet</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="torchscript-language-reference">
<h3><a class="toc-backref" href="#id3">TorchScript Language Reference</a><a class="headerlink" href="#torchscript-language-reference" title="Permalink to this headline">¶</a></h3>
<p>TorchScript is a subset of Python that can either be written directly (using
the &#64;script annotations) or generated automatically from Python code via
tracing. When using tracing, code is automatically converted into this subset of
Python by recording only the actual operators on tensors and simply executing and
discarding the other surrounding Python code.</p>
<p>When writing TorchScript directly using &#64;script annotations, the programmer must
only use the subset of Python supported in TorchScript. This section documents
what is supported in TorchScript as if it were a language reference for a stand
alone language. Any features of Python not mentioned in this reference are not
part of TorchScript.</p>
<p>As a subset of Python any valid TorchScript function is also a valid Python
function. This makes it possible to remove the &#64;script annotations and debug the
function using standard Python tools like pdb. The reverse is not true: there
are many valid python programs that are not valid TorchScript programs.
Instead, TorchScript focuses specifically on the features of Python that are
needed to represent neural network models in Torch.</p>
<dl class="envvar">
<dt id="envvar-PYTORCH_JIT=1">
<code class="descname">PYTORCH_JIT=1</code><a class="headerlink" href="#envvar-PYTORCH_JIT=1" title="Permalink to this definition">¶</a></dt>
<dd><p>Setting the environment variable <code class="docutils literal notranslate"><span class="pre">PYTORCH_JIT=0</span></code> will disable all script
and tracing annotations. If there is hard-to-debug error in one of your
ScriptModules, you can use this flag to force everything to run using native
Python. This allows the use of tools like <code class="docutils literal notranslate"><span class="pre">pdb</span></code> to debug code.</p>
</dd></dl>

<div class="section" id="types">
<h4><a class="toc-backref" href="#id4">Types</a><a class="headerlink" href="#types" title="Permalink to this headline">¶</a></h4>
<p>The largest difference between TorchScript and the full Python language is that
TorchScript only support a small set of types that are needed to express neural
net models. In particular TorchScript supports:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">Tensor</span></code></dt><dd><p>A PyTorch tensor of any dtype, dimension, or backend.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Tuple[T0,</span> <span class="pre">T1,</span> <span class="pre">...]</span></code></dt><dd><p>A tuple containing subtypes <code class="docutils literal notranslate"><span class="pre">T0</span></code>, <code class="docutils literal notranslate"><span class="pre">T1</span></code>, etc. (e.g. <code class="docutils literal notranslate"><span class="pre">Tuple[Tensor,</span> <span class="pre">Tensor]</span></code>)</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">bool</span></code></dt><dd><p>A boolean value</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">int</span></code></dt><dd><p>A scalar integer</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">float</span></code></dt><dd><p>A scalar floating point number</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">List[T]</span></code></dt><dd><p>A list of which all members are type <code class="docutils literal notranslate"><span class="pre">T</span></code></p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">Optional[T]</span></code></dt><dd><p>A value which is either None or type <code class="docutils literal notranslate"><span class="pre">T</span></code></p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">`Dict[K,</span> <span class="pre">V]</span></code></dt><dd><p>A dict with key type <code class="docutils literal notranslate"><span class="pre">K</span></code> and value type <code class="docutils literal notranslate"><span class="pre">V</span></code>. Only <code class="docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">int</span></code>, and
<code class="docutils literal notranslate"><span class="pre">float</span></code> are allowed as key types.</p>
</dd>
</dl>
<p>Unlike Python, each variable in TorchScript function must have a single static type.
This makes it easier to optimize TorchScript functions.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">an_error</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="k">return</span> <span class="n">r</span> <span class="c1"># Type mismatch: r is set to type Tensor in the true branch</span>
             <span class="c1"># and type int in the false branch</span>
</pre></div>
</div>
<p>There are 2 scenarios in which you can annotate:</p>
<ol class="arabic simple">
<li><p>Function Argument Type Annotation</p></li>
</ol>
<p>By default, all parameters to a TorchScript function are assumed to be Tensor
because this is the most common type used in modules. To specify that an
argument to a TorchScript function is another type, it is possible to use
MyPy-style type annotations using the types listed above:</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tup</span><span class="p">):</span>
    <span class="c1"># type: (int, Tuple[Tensor, Tensor]) -&gt; Tensor</span>
    <span class="n">t0</span><span class="p">,</span> <span class="n">t1</span> <span class="o">=</span> <span class="n">tup</span>
    <span class="k">return</span> <span class="n">t0</span> <span class="o">+</span> <span class="n">t1</span> <span class="o">+</span> <span class="n">x</span>

<span class="nb">print</span><span class="p">(</span><span class="n">foo</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">))))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is also possible to annotate types with Python 3 type annotations.
In our examples, we use comment-based annotations to ensure Python 2
compatibility as well.</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Variable Type Annotation</p></li>
</ol>
<p>A list by default is assumed to be <code class="docutils literal notranslate"><span class="pre">List[Tensor]</span></code> and empty dicts
<code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Tensor]</span></code>. To instantiate an empty list or dict of other types,
use <code class="docutils literal notranslate"><span class="pre">torch.jit.annotate</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.jit</span> <span class="k">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="k">class</span> <span class="nc">EmptyDataStructures</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EmptyDataStructures</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># type: (Tensor) -&gt; Tuple[List[Tuple[Tensor, Tensor]], Dict[int, Tensor]]</span>

        <span class="c1"># This annotates the list to be a `List[Tuple[Tensor, Tensor]]`</span>
        <span class="n">list_of_tuple</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]],</span> <span class="p">[])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">list_of_tuple</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

            <span class="c1"># This annotates the list to be a `Dict[int, Tensor]`</span>
        <span class="n">int_tensor_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="p">{})</span>
        <span class="k">return</span> <span class="n">list_of_tuple</span><span class="p">,</span> <span class="n">int_tensor_dict</span>
</pre></div>
</div>
<p>Optional Type Refinement:</p>
<p>TorchScript will refine the type of a variable of type Optional[T] when
a comparison to None is made inside the conditional of an if statement.
The compiler can reason about multiple None checks that are combined with
AND, OR, or NOT. Refinement will also occur for else blocks of if statements
that are not explicitly written.</p>
<p>The expression must be emitted within the conditional; assigning
a None check to a variable and using it in the conditional will not refine types.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">opt_unwrap</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
  <span class="c1"># type: (Optional[int], Optional[int], Optional[int]) -&gt; int</span>
  <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

  <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span>
  <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
<div class="section" id="expressions">
<h4><a class="toc-backref" href="#id5">Expressions</a><a class="headerlink" href="#expressions" title="Permalink to this headline">¶</a></h4>
<p>The following Python Expressions are supported</p>
<dl>
<dt>Literals</dt><dd><p><code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">'string</span> <span class="pre">literals'</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;string</span> <span class="pre">literals&quot;</span></code>,
number literals <code class="docutils literal notranslate"><span class="pre">3</span></code> (interpreted as int) <code class="docutils literal notranslate"><span class="pre">3.4</span></code> (interpreter as a float)</p>
</dd>
<dt>Variables</dt><dd><p><code class="docutils literal notranslate"><span class="pre">a</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="#variable-resolution">Variable Resolution</a> for how variables are resolved.</p>
</div>
</dd>
<dt>Tuple Construction</dt><dd><p><code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">4)</span></code>, <code class="docutils literal notranslate"><span class="pre">(3,)</span></code></p>
</dd>
<dt>List Construction</dt><dd><p><code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">4]</span></code>, <code class="docutils literal notranslate"><span class="pre">[]</span></code>, <code class="docutils literal notranslate"><span class="pre">[torch.rand(3),</span> <span class="pre">torch.rand(4)]</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>an empty list is assumed have type <code class="docutils literal notranslate"><span class="pre">List[Tensor]</span></code>.
The types of other list literals are derived from the type of the members.</p>
</div>
</dd>
<dt>Dict Construction</dt><dd><p><code class="docutils literal notranslate"><span class="pre">{'hello':</span> <span class="pre">3}</span></code>, <code class="docutils literal notranslate"><span class="pre">{}</span></code>, <code class="docutils literal notranslate"><span class="pre">{'a':</span> <span class="pre">torch.rand(3),</span> <span class="pre">'b':</span> <span class="pre">torch.rand(4)}</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>an empty dict is assumed have type <code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">Tensor]</span></code>.
The types of other dict literals are derived from the type of the members.</p>
</div>
</dd>
<dt>Arithmetic Operators</dt><dd><p><code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">-</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">/</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">^</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&#64;</span> <span class="pre">b</span></code></p>
</dd>
<dt>Comparison Operators</dt><dd><p><code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">==</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">!=</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&lt;</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&gt;</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&lt;=</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&gt;=</span> <span class="pre">b</span></code></p>
</dd>
<dt>Logical Operators</dt><dd><p><code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">and</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">or</span> <span class="pre">b</span></code>
<code class="docutils literal notranslate"><span class="pre">not</span> <span class="pre">b</span></code></p>
</dd>
<dt>Subscripts</dt><dd><p><code class="docutils literal notranslate"><span class="pre">t[0]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[-1]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[0:2]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[1:]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[:1]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[:]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[0,</span> <span class="pre">1]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[0,</span> <span class="pre">1:2]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[0,</span> <span class="pre">:1]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[-1,</span> <span class="pre">1:,</span> <span class="pre">0]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[1:,</span> <span class="pre">-1,</span> <span class="pre">0]</span></code>
<code class="docutils literal notranslate"><span class="pre">t[i:j,</span> <span class="pre">i]</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>TorchScript currently does not support mutating tensors in place, so any
tensor indexing can only appear on the right-hand size of an expression.</p>
</div>
</dd>
<dt>Function calls</dt><dd><p>Calls to built-in functions: <code class="docutils literal notranslate"><span class="pre">torch.rand(3,</span> <span class="pre">dtype=torch.int)</span></code></p>
<p>Calls to other script functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">bar</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">foo</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>Method calls</dt><dd><p>Calls to methods of builtin types like tensor: <code class="docutils literal notranslate"><span class="pre">x.mm(y)</span></code></p>
<p>When defining a Script method inside of a ScriptModule, the <code class="docutils literal notranslate"><span class="pre">&#64;script_method</span></code>
annotation is used. Inside of these methods it is possible to call other methods
of this class or access methods on the submodules.</p>
<p>Calling a submodule directly (e.g. <code class="docutils literal notranslate"><span class="pre">self.resnet(input)</span></code>) is equivalent to
calling its <code class="docutils literal notranslate"><span class="pre">forward</span></code> method (e.g. <code class="docutils literal notranslate"><span class="pre">self.resnet.forward(input)</span></code>)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">MyScriptModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyScriptModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">103.939</span><span class="p">,</span> <span class="mf">116.779</span><span class="p">,</span> <span class="mf">123.68</span><span class="p">])</span>
                                        <span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resnet</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(),</span>
                                      <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script_method</span>
    <span class="k">def</span> <span class="nf">helper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">resnet</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">helper</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>If expressions</dt><dd><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">if</span> <span class="pre">x</span> <span class="pre">&gt;</span> <span class="pre">y</span> <span class="pre">else</span> <span class="pre">y</span></code></p>
</dd>
<dt>Casts</dt><dd><p><code class="docutils literal notranslate"><span class="pre">float(ten)</span></code>, <code class="docutils literal notranslate"><span class="pre">int(3.5)</span></code>, <code class="docutils literal notranslate"><span class="pre">bool(ten)</span></code></p>
</dd>
<dt>Accessing Module Parameters</dt><dd><p><code class="docutils literal notranslate"><span class="pre">self.my_parameter</span></code> <code class="docutils literal notranslate"><span class="pre">self.my_submodule.my_parameter</span></code></p>
</dd>
</dl>
</div>
<div class="section" id="statements">
<h4><a class="toc-backref" href="#id6">Statements</a><a class="headerlink" href="#statements" title="Permalink to this headline">¶</a></h4>
<p>TorchScript supports the following types of statements:</p>
<p>Simple Assignments</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">b</span>
<span class="n">a</span> <span class="o">+=</span> <span class="n">b</span> <span class="c1"># short-hand for a = a + b, does not operate in-place on a</span>
<span class="n">a</span> <span class="o">-=</span> <span class="n">b</span>
</pre></div>
</div>
</div></blockquote>
<p>Pattern Matching Assignments</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">tuple_or_list</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="o">*</span><span class="n">c</span> <span class="o">=</span> <span class="n">a_tuple</span>
</pre></div>
</div>
</div></blockquote>
<p>Print Statements</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">print(&quot;the</span> <span class="pre">result</span> <span class="pre">of</span> <span class="pre">an</span> <span class="pre">add:&quot;,</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">b)</span></code></p>
</div></blockquote>
<p>If Statements</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="o">-</span><span class="n">a</span>
<span class="k">elif</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">a</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">a</span>
</pre></div>
</div>
</div></blockquote>
<p>While Loops</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div></blockquote>
<p>For loops with <code class="docutils literal notranslate"><span class="pre">range</span></code></p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="n">i</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Script currently does not support iterating over generic iterable
objects like lists or tensors. Script currently does not support start or
increment parameters to range. These will be added in a future version.</p>
</div>
</div></blockquote>
<p>For loops over tuples:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tup</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>for loops over tuples will unroll the loop, generating a body for
each member of the tuple. The body must type-check correctly for each member.</p>
</div>
</div></blockquote>
<p>For loops over constant <code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code></p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SubModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sub</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">+</span> <span class="nb">input</span>

<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mods&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mods</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">SubModule</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script_method</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mods</span><span class="p">:</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use a module list inside a <code class="docutils literal notranslate"><span class="pre">&#64;script_method</span></code> it must be marked
constant by adding the name of the attribute to the <code class="docutils literal notranslate"><span class="pre">__constants__</span></code>
list for the type. For loops over a ModuleList will unroll the body of the
loop at compile time, with each member of the constant module list.</p>
</div>
</div></blockquote>
<dl>
<dt>Return</dt><dd><p><code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">a,</span> <span class="pre">b</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<dl class="simple">
<dt>TorchScript allows returns in the following circumstances:</dt><dd><ol class="arabic simple">
<li><p>At the end of a function</p></li>
<li><p>In an if-statement where &lt;true&gt; and &lt;false&gt; both return</p></li>
<li><p>In an if-statement where &lt;true&gt; returns and &lt;false&gt; is empty (an early return)</p></li>
</ol>
</dd>
</dl>
</div>
</dd>
</dl>
</div>
<div class="section" id="variable-resolution">
<h4><a class="toc-backref" href="#id7">Variable Resolution</a><a class="headerlink" href="#variable-resolution" title="Permalink to this headline">¶</a></h4>
<p>TorchScript supports a subset of Python’s variable resolution (i.e. scoping)
rules. Local variables behave the same as in Python, except for the restriction
that a variable must have the same type along all paths through a function.
If a variable has a different type on different sides of an if statement, it
is an error to use it after the end of the if statement.</p>
<p>Similarly, a variable is not allowed to be used if it is only <em>defined</em> along some
paths through the function.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># Error: undefined value y</span>
</pre></div>
</div>
<p>Non-local variables are resolved to Python values at compile time when the
function is defined. These values are then converted into TorchScript values using
the rules described in <a class="reference internal" href="#use-of-python-values">Use of Python Values</a>.</p>
</div>
<div class="section" id="use-of-python-values">
<h4><a class="toc-backref" href="#id8">Use of Python Values</a><a class="headerlink" href="#use-of-python-values" title="Permalink to this headline">¶</a></h4>
<p>To make writing TorchScript more convenient, we allow script code to refer
to Python values in the surrounding scope. For instance, any time there is a
reference to <code class="docutils literal notranslate"><span class="pre">torch</span></code>, the TorchScript compiler is actually resolving it to the
<code class="docutils literal notranslate"><span class="pre">torch</span></code> Python module when the function is declared.  These Python values are
not a first class part of TorchScript. Instead they are desugared at compile-time
into the primitive types that TorchScript supports. This section describes the
rules that are used when accessing Python values in TorchScript. They depend
on the dynamic type of the python valued referenced.</p>
<dl>
<dt>Functions</dt><dd><p>TorchScript can call python functions. This functionality is very useful when
incrementally converting a model into script. The model can be moved function-by-function
to script, leaving calls to Python functions in place. This way you can incrementally
check the correctness of the model as you go.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;I am called with </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
  <span class="kn">import</span> <span class="nn">pdb</span><span class="p">;</span> <span class="n">pdb</span><span class="o">.</span><span class="n">set_trace</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">x</span>

<span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">bar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">foo</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Attempting to call <code class="docutils literal notranslate"><span class="pre">save</span></code> on a ScriptModule that contains calls to Python
functions will fail. The intention is that this pathway is used for debugging
and the calls removed or turned into script functions before saving.</p>
</div>
</dd>
<dt>Attribute Lookup On Python Modules</dt><dd><p>TorchScript can lookup attributes on modules. Builtin functions like <code class="docutils literal notranslate"><span class="pre">torch.add</span></code>
are accessed this way. This allows TorchScript to call functions defined in
other modules.</p>
</dd>
<dt>Python-defined Constants</dt><dd><p>TorchScript also provides a way to use constants that are defined in Python.
These can be used to hard-code hyper-parameters into the function, or to
define universal constants. There are two ways of specifying that a Python
value should be treated as a constant.</p>
<ol class="arabic">
<li><p>Values looked up as attributes of a module are assumed to be constant.
Example: <code class="docutils literal notranslate"><span class="pre">math.pi</span></code></p></li>
<li><p>Attributes of a ScriptModule can be marked constant by listing them
as a member of the <code class="docutils literal notranslate"><span class="pre">__constants__</span></code> property of the class:</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Foo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">4</span>

   <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span>
   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">+</span> <span class="nb">input</span>
</pre></div>
</div>
</li>
</ol>
<p>Supported constant Python Values are</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bool</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.device</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.layout</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code></p></li>
<li><p>tuples containing supported types</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> which can be used in a TorchScript for loop</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="debugging">
<h4><a class="toc-backref" href="#id9">Debugging</a><a class="headerlink" href="#debugging" title="Permalink to this headline">¶</a></h4>
<dl>
<dt>Disable JIT for Debugging</dt><dd><p>If you want to disable all JIT modes (tracing and scripting) so you can
debug your program in raw Python, you can use the <code class="docutils literal notranslate"><span class="pre">PYTORCH_JIT</span></code> environment
variable. <code class="docutils literal notranslate"><span class="pre">PYTORCH_JIT</span></code> can be used to globally disable the
JIT by setting its value to <code class="docutils literal notranslate"><span class="pre">0</span></code>. Given an example script:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">scripted_fn</span><span class="p">(</span><span class="n">x</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="kn">import</span> <span class="nn">pdb</span><span class="p">;</span> <span class="n">pdb</span><span class="o">.</span><span class="n">set_trace</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">scripted_fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">traced_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),))</span>

<span class="n">traced_fn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
<p>Debugging this script with PDB works except for when we invoke the &#64;script
function. We can globally disable JIT, so that we can call the &#64;script
function as a normal python function and not compile it. If the above script
is called <code class="docutils literal notranslate"><span class="pre">disable_jit_example.py</span></code>, we can invoke it like so:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ PYTORCH_JIT=0 python disable_jit_example.py
</pre></div>
</div>
<p>and we will be able to step into the &#64;script function as a normal Python
function.</p>
</dd>
<dt>Interpreting Graphs</dt><dd><p>TorchScript uses a static single assignment (SSA) intermediate representation
(IR) to represent computation. The instructions in this format consist of
ATen (the C++ backend of PyTorch) operators and other primitive operators,
including control flow operators for loops and conditionals. As an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="nb">len</span><span class="p">):</span>
  <span class="c1"># type: (int) -&gt; torch.Tensor</span>
  <span class="n">rv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">:</span>
        <span class="n">rv</span> <span class="o">=</span> <span class="n">rv</span> <span class="o">-</span> <span class="mf">1.0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">rv</span> <span class="o">=</span> <span class="n">rv</span> <span class="o">+</span> <span class="mf">1.0</span>
  <span class="k">return</span> <span class="n">rv</span>

<span class="nb">print</span><span class="p">(</span><span class="n">foo</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<p>A <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code> with a single <code class="docutils literal notranslate"><span class="pre">forward</span></code> method will have an attribute
<code class="docutils literal notranslate"><span class="pre">graph</span></code>, which you can use to inspect the IR representing the computation.
If the ScriptModule has more than one method, you will need to access
<code class="docutils literal notranslate"><span class="pre">.graph</span></code> on the method itself and not the module. We can inspect the
graph of a method named <code class="docutils literal notranslate"><span class="pre">bar</span></code> on a ScriptModule by accessing <code class="docutils literal notranslate"><span class="pre">.bar.graph</span></code>.</p>
<p>The example script above produces the graph:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span><span class="p">(</span><span class="o">%</span><span class="nb">len</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="p">{</span>
  <span class="o">%</span><span class="mi">15</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">9</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">7</span> <span class="p">:</span> <span class="n">Device</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">6</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">5</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">6</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">1</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">3</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">2</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">4</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">11</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">10</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">14</span> <span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">4</span> <span class="p">:</span> <span class="nb">int</span><span class="p">[]</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">ListConstruct</span><span class="p">(</span><span class="o">%</span><span class="mi">1</span><span class="p">,</span> <span class="o">%</span><span class="mi">2</span><span class="p">)</span>
  <span class="o">%</span><span class="n">rv</span><span class="o">.</span><span class="mi">1</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">aten</span><span class="p">::</span><span class="n">zeros</span><span class="p">(</span><span class="o">%</span><span class="mi">4</span><span class="p">,</span> <span class="o">%</span><span class="mi">5</span><span class="p">,</span> <span class="o">%</span><span class="mi">6</span><span class="p">,</span> <span class="o">%</span><span class="mi">7</span><span class="p">)</span>
  <span class="o">%</span><span class="n">rv</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Loop</span><span class="p">(</span><span class="o">%</span><span class="nb">len</span><span class="p">,</span> <span class="o">%</span><span class="mi">9</span><span class="p">,</span> <span class="o">%</span><span class="n">rv</span><span class="o">.</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">block0</span><span class="p">(</span><span class="o">%</span><span class="n">i</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">%</span><span class="mi">13</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="p">{</span>
      <span class="o">%</span><span class="mi">12</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">aten</span><span class="p">::</span><span class="n">lt</span><span class="p">(</span><span class="o">%</span><span class="n">i</span><span class="p">,</span> <span class="o">%</span><span class="mi">11</span><span class="p">)</span>
      <span class="o">%</span><span class="n">rv</span><span class="o">.</span><span class="mi">4</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">If</span><span class="p">(</span><span class="o">%</span><span class="mi">12</span><span class="p">)</span>
        <span class="n">block0</span><span class="p">()</span> <span class="p">{</span>
          <span class="o">%</span><span class="n">rv</span><span class="o">.</span><span class="mi">2</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">aten</span><span class="p">::</span><span class="n">sub</span><span class="p">(</span><span class="o">%</span><span class="mi">13</span><span class="p">,</span> <span class="o">%</span><span class="mi">14</span><span class="p">,</span> <span class="o">%</span><span class="mi">15</span><span class="p">)</span>
          <span class="o">-&gt;</span> <span class="p">(</span><span class="o">%</span><span class="n">rv</span><span class="o">.</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="n">block1</span><span class="p">()</span> <span class="p">{</span>
          <span class="o">%</span><span class="n">rv</span><span class="o">.</span><span class="mi">3</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">aten</span><span class="p">::</span><span class="n">add</span><span class="p">(</span><span class="o">%</span><span class="mi">13</span><span class="p">,</span> <span class="o">%</span><span class="mi">14</span><span class="p">,</span> <span class="o">%</span><span class="mi">15</span><span class="p">)</span>
          <span class="o">-&gt;</span> <span class="p">(</span><span class="o">%</span><span class="n">rv</span><span class="o">.</span><span class="mi">3</span><span class="p">)</span>
        <span class="p">}</span>
      <span class="o">-&gt;</span> <span class="p">(</span><span class="o">%</span><span class="mi">9</span><span class="p">,</span> <span class="o">%</span><span class="n">rv</span><span class="o">.</span><span class="mi">4</span><span class="p">)</span>
    <span class="p">}</span>
  <span class="k">return</span> <span class="p">(</span><span class="o">%</span><span class="n">rv</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Take the instruction <code class="docutils literal notranslate"><span class="pre">%rv.1</span> <span class="pre">:</span> <span class="pre">Dynamic</span> <span class="pre">=</span> <span class="pre">aten::zeros(%3,</span> <span class="pre">%4,</span> <span class="pre">%5,</span> <span class="pre">%6)</span></code> for
example. <code class="docutils literal notranslate"><span class="pre">%rv.1</span> <span class="pre">:</span> <span class="pre">Dynamic</span></code> means we assign the output to a (unique)
value named <code class="docutils literal notranslate"><span class="pre">rv.1</span></code>, and that value is of <code class="docutils literal notranslate"><span class="pre">Dynamic</span></code> type, i.e. we do
not know its concrete shape. <code class="docutils literal notranslate"><span class="pre">aten::zeros</span></code> is the operator (equivalent
to <code class="docutils literal notranslate"><span class="pre">torch.zeros</span></code>) and the input list <code class="docutils literal notranslate"><span class="pre">(%3,</span> <span class="pre">%4,</span> <span class="pre">%5,</span> <span class="pre">%6)</span></code> specifies which
values in scope should be passed as inputs. The schema for built-in functions
like <code class="docutils literal notranslate"><span class="pre">aten::zeros</span></code> can be found at <a class="reference internal" href="#builtin-functions">Builtin Functions</a>.</p>
<p>Notice that operators can also have associated <code class="docutils literal notranslate"><span class="pre">blocks</span></code>, namely the
<code class="docutils literal notranslate"><span class="pre">prim::Loop</span></code> and <code class="docutils literal notranslate"><span class="pre">prim::If</span></code> operators. In the graph print-out, these
operators are formatted to reflect their equivalent source code forms
to facilitate easy debugging.</p>
<p>Graphs can be inspected as shown to confirm that the computation described
by a <code class="docutils literal notranslate"><span class="pre">ScriptModule</span></code> is correct, in both automated and manual fashion, as
described below.</p>
</dd>
<dt>Tracing Edge Cases</dt><dd><p>There are some edge cases that exist where the trace of a given Python
function/module will not be representative of the underlying code. These
cases can include:</p>
<ul class="simple">
<li><p>Tracing of control flow that is dependent on inputs (e.g. tensor shapes)</p></li>
<li><p>Tracing of in-place operations of tensor views (e.g. indexing on the
left-hand side of an assignment)</p></li>
</ul>
<p>Note that these cases may in fact be traceable in the future.</p>
</dd>
<dt>Automatic Trace Checking</dt><dd><p>One way to automatically catch many errors in traces is by using <code class="docutils literal notranslate"><span class="pre">check_inputs</span></code>
on the <code class="docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code> API. <code class="docutils literal notranslate"><span class="pre">check_inputs</span></code> takes a list of tuples
of inputs that will be used to re-trace the computation and verify the
results. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loop_in_traced_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),)</span>
<span class="n">check_inputs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">),),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),)]</span>

<span class="n">traced</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">loop_in_traced_fn</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">check_inputs</span><span class="o">=</span><span class="n">check_inputs</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Gives us the following diagnostic information::</dt><dd><p>ERROR: Graphs differed across invocations!
Graph diff:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  graph(%x : Tensor) {
    %1 : int = prim::Constant[value=0]()
    %2 : int = prim::Constant[value=0]()
    %result.1 : Tensor = aten::select(%x, %1, %2)
    %4 : int = prim::Constant[value=0]()
    %5 : int = prim::Constant[value=0]()
    %6 : Tensor = aten::select(%x, %4, %5)
    %result.2 : Tensor = aten::mul(%result.1, %6)
    %8 : int = prim::Constant[value=0]()
    %9 : int = prim::Constant[value=1]()
    %10 : Tensor = aten::select(%x, %8, %9)
-   %result : Tensor = aten::mul(%result.2, %10)
+   %result.3 : Tensor = aten::mul(%result.2, %10)
?          ++
    %12 : int = prim::Constant[value=0]()
    %13 : int = prim::Constant[value=2]()
    %14 : Tensor = aten::select(%x, %12, %13)
+   %result : Tensor = aten::mul(%result.3, %14)
+   %16 : int = prim::Constant[value=0]()
+   %17 : int = prim::Constant[value=3]()
+   %18 : Tensor = aten::select(%x, %16, %17)
-   %15 : Tensor = aten::mul(%result, %14)
?     ^                                 ^
+   %19 : Tensor = aten::mul(%result, %18)
?     ^                                 ^
-   return (%15);
?             ^
+   return (%19);
?             ^
  }
</pre></div>
</div>
</dd>
</dl>
<p>This message indicates to us that the computation differed between when
we first traced it and when we traced it with the <code class="docutils literal notranslate"><span class="pre">check_inputs</span></code>. Indeed,
the loop within the body of <code class="docutils literal notranslate"><span class="pre">loop_in_traced_fn</span></code> depends on the shape
of the input <code class="docutils literal notranslate"><span class="pre">x</span></code>, and thus when we try another <code class="docutils literal notranslate"><span class="pre">x</span></code> with a different
shape, the trace differs.</p>
<p>In this case, data-dependent control flow like this can be captured using
script instead:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),)</span>
<span class="n">check_inputs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">),),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),)]</span>

<span class="n">scripted_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scripted_fn</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>

<span class="k">for</span> <span class="n">input_tuple</span> <span class="ow">in</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">check_inputs</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">input_tuple</span><span class="p">),</span> <span class="n">scripted_fn</span><span class="p">(</span><span class="o">*</span><span class="n">input_tuple</span><span class="p">))</span>
</pre></div>
</div>
<p>Which produces:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span><span class="p">(</span><span class="o">%</span><span class="n">x</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="p">{</span>
  <span class="o">%</span><span class="mi">5</span> <span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">]()</span>
  <span class="o">%</span><span class="mi">1</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">]()</span>
  <span class="o">%</span><span class="n">result</span><span class="o">.</span><span class="mi">1</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">aten</span><span class="p">::</span><span class="n">select</span><span class="p">(</span><span class="o">%</span><span class="n">x</span><span class="p">,</span> <span class="o">%</span><span class="mi">1</span><span class="p">,</span> <span class="o">%</span><span class="mi">1</span><span class="p">)</span>
  <span class="o">%</span><span class="mi">4</span> <span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">aten</span><span class="p">::</span><span class="n">size</span><span class="p">(</span><span class="o">%</span><span class="n">x</span><span class="p">,</span> <span class="o">%</span><span class="mi">1</span><span class="p">)</span>
  <span class="o">%</span><span class="n">result</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">prim</span><span class="p">::</span><span class="n">Loop</span><span class="p">(</span><span class="o">%</span><span class="mi">4</span><span class="p">,</span> <span class="o">%</span><span class="mi">5</span><span class="p">,</span> <span class="o">%</span><span class="n">result</span><span class="o">.</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">block0</span><span class="p">(</span><span class="o">%</span><span class="n">i</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">%</span><span class="mi">7</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="p">{</span>
      <span class="o">%</span><span class="mi">10</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">aten</span><span class="p">::</span><span class="n">select</span><span class="p">(</span><span class="o">%</span><span class="n">x</span><span class="p">,</span> <span class="o">%</span><span class="mi">1</span><span class="p">,</span> <span class="o">%</span><span class="n">i</span><span class="p">)</span>
      <span class="o">%</span><span class="n">result</span><span class="o">.</span><span class="mi">2</span> <span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">aten</span><span class="p">::</span><span class="n">mul</span><span class="p">(</span><span class="o">%</span><span class="mi">7</span><span class="p">,</span> <span class="o">%</span><span class="mi">10</span><span class="p">)</span>
      <span class="o">-&gt;</span> <span class="p">(</span><span class="o">%</span><span class="mi">5</span><span class="p">,</span> <span class="o">%</span><span class="n">result</span><span class="o">.</span><span class="mi">2</span><span class="p">)</span>
    <span class="p">}</span>
  <span class="k">return</span> <span class="p">(</span><span class="o">%</span><span class="n">result</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt>Tracer Warnings</dt><dd><p>The tracer produces warnings for several problematic patterns in traced
computation. As an example, take a trace of a function that contains an
in-place assignment on a slice (a view) of a Tensor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fill_row_zero</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">traced</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">fill_row_zero</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">traced</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<p>Produces several warnings and a graph which simply returns the input:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.
  x[0] = torch.rand(*x.shape[1:2])
fill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:
Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)
  traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))
graph(%0 : Float(3, 4)) {
  return (%0);
}
</pre></div>
</div>
<p>We can fix this by modifying the code to not use the in-place update, but
rather build up the result tensor out-of-place with <cite>torch.cat</cite>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fill_row_zero</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">traced</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">fill_row_zero</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">traced</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="builtin-functions">
<h4><a class="toc-backref" href="#id10">Builtin Functions</a><a class="headerlink" href="#builtin-functions" title="Permalink to this headline">¶</a></h4>
<p>TorchScript supports a subset of the builtin tensor and neural network
functions that PyTorch provides. Most methods on Tensor as well as functions in
the <code class="docutils literal notranslate"><span class="pre">torch</span></code> namespace, all functions in <code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code> and all
modules from <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> are supported in TorchScript, excluding those in the
table below. For unsupported modules, we suggest using <a class="reference internal" href="#torch.jit.trace" title="torch.jit.trace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code></a>.</p>
<p>Unsupported <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> Modules</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">adaptive</span><span class="o">.</span><span class="n">AdaptiveLogSoftmaxWithLoss</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">normalization</span><span class="o">.</span><span class="n">CrossMapLRN2d</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">fold</span><span class="o">.</span><span class="n">Fold</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">fold</span><span class="o">.</span><span class="n">Unfold</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">GRU</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">LSTM</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">RNN</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">GRUCell</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">LSTMCell</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">RNNCell</span>
</pre></div>
</div>
<span class="target" id="module-torch.jit.supported_ops"></span><div class="section" id="supported-functions">
<h5><a class="toc-backref" href="#id11">Supported Functions</a><a class="headerlink" href="#supported-functions" title="Permalink to this headline">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sizes</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">abs_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">acos</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">acos</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">acos_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">adaptive_avg_pool1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">adaptive_max_pool1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
          <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
          <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
          <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">],</span>
          <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addbmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addbmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addmv</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mat</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addmv</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mat</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addmv_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">mat</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addr</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">vec1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">addr</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">vec1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">affine_grid_generator</span><span class="p">(</span><span class="n">theta</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">rtol</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
               <span class="n">atol</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
               <span class="n">equal_nan</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">alpha_dropout</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                    <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">alpha_dropout_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                     <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">step</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">step</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">descending</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                 <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                 <span class="n">storage_offset</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">storage_offset</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">asin</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">asin</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">asin_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">atan2</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">atan2</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">atan_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                 <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[],</span>
                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                 <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">count_include_pad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">baddbmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">baddbmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bartlett_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bartlett_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">periodic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                      <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                 <span class="n">momentum</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">cudnn_enabled</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">batch_norm_backward_elemt</span><span class="p">(</span><span class="n">grad_out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">mean</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">invstd</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                <span class="n">mean_dy</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">mean_dy_xmu</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">batch_norm_backward_reduce</span><span class="p">(</span><span class="n">grad_out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">mean</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">invstd</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">input_g</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                 <span class="n">weight_g</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                 <span class="n">bias_g</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">batch_norm_elemt</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                       <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                       <span class="n">mean</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">invstd</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">batch_norm_gather_stats</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">mean</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">invstd</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                              <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                              <span class="n">momentum</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                              <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                              <span class="n">count</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">batch_norm_stats</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">batch_norm_update_stats</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                              <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                              <span class="n">momentum</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">],</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bilinear</span><span class="p">(</span><span class="n">input1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                       <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                       <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                       <span class="n">pos_weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                       <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">weights</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
               <span class="n">minlength</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">blackman_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">blackman_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">periodic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                      <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="n">tensors</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">btrifact</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">pivot</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">btrifact_with_info</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">pivot</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">btrisolve</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">LU_data</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">LU_pivots</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">btrisolve</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">LU_data</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">LU_pivots</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cartesian_prod</span><span class="p">(</span><span class="n">tensors</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tensors</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tensors</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">x1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">x2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ceil_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">celu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">celu_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">chain_matmul</span><span class="p">(</span><span class="n">matrices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">chunks</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="nb">min</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
            <span class="nb">max</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="nb">min</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
            <span class="nb">max</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="nb">min</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
             <span class="nb">max</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp_max_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clamp_min_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">r</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                   <span class="n">with_replacement</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">constant_pad_nd</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">pad</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
             <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
             <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
             <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
             <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
             <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
             <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
             <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
             <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
             <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
             <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
             <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
             <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">conv_tbc</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">bias</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">pad</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                       <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                       <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                       <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                       <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                       <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                       <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                       <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                       <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                       <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                       <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                  <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">transposed</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                  <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cos_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cosh_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                            <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">x1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">x2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">targets</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">input_lengths</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">target_lengths</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">blank</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">zero_infinity</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">targets</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">input_lengths</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">target_lengths</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">blank</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">zero_infinity</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cudnn_affine_grid_generator</span><span class="p">(</span><span class="n">theta</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">N</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                  <span class="n">C</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                  <span class="n">H</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                  <span class="n">W</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cudnn_batch_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                       <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                       <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                       <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                       <span class="n">exponential_average_factor</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                       <span class="n">epsilon</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cudnn_convolution</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">benchmark</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                        <span class="n">deterministic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cudnn_convolution_transpose</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                  <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                  <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                  <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                  <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                  <span class="n">benchmark</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                  <span class="n">deterministic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cudnn_grid_sampler</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">grid</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cudnn_is_acceptable</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">detach</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">detach_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Device</span>

<span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">offset</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dim1</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">dim2</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">diagflat</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">offset</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">offset</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">dim1</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">dim2</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">digamma</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">digamma</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">tensor</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">tensor</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
              <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">dropout_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
               <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">eigenvectors</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="n">equation</span> <span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
             <span class="n">tensors</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">padding_idx</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">scale_grad_by_freq</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">sparse</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span><span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">offsets</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">scale_grad_by_freq</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">mode</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">sparse</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">embedding_renorm_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">max_norm</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                        <span class="n">norm_type</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">empty_strided</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">erf_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">erfc</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">erfc</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">erfc_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">erfinv</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">erfinv</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">exp_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">expm1_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">m</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">m</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fbgemm_is_cpu_supported</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fbgemm_linear_int8_weight</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">packed</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">col_offsets</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">weight_scale</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                                <span class="n">weight_zero_point</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                                <span class="n">bias</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fbgemm_linear_quantize_weight</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fbgemm_pack_quantized_matrix</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">K</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                   <span class="n">N</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">feature_alpha_dropout</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                            <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">feature_alpha_dropout_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                             <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">feature_dropout</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                      <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">feature_dropout_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                       <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">signal_ndim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">value</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">start_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
              <span class="n">end_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dims</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">floor_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">frac</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">frac</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">frobenius_norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">frobenius_norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">frobenius_norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">fill_value</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">fill_value</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">fill_value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">fill_value</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">sparse_grad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">sparse_grad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gels</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">A</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">geqrf</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ger</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ger</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">get_device</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">get_device</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">get_device</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">grid_sampler</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">grid</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">interpolation_mode</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">padding_mode</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">grid_sampler_2d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">grid</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">interpolation_mode</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">padding_mode</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">grid_sampler_3d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">grid</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">interpolation_mode</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">padding_mode</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">num_groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
                 <span class="n">cudnn_enabled</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">data</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">batch_sizes</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
          <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
          <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
          <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
          <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
          <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
          <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
          <span class="n">batch_first</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gru_cell</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">w_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">w_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">b_ih</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
               <span class="n">b_hh</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hamming_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hamming_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">periodic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                     <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hamming_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">periodic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                     <span class="n">alpha</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                     <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hamming_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">periodic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                     <span class="n">alpha</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                     <span class="n">beta</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                     <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                     <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hann_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hann_window</span><span class="p">(</span><span class="n">window_length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">periodic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                  <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hardshrink</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">lambd</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                           <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">bins</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">bins</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hspmm</span><span class="p">(</span><span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">hspmm</span><span class="p">(</span><span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ifft</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">signal_ndim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_add</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_copy</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_fill</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">value</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_fill</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_put</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">indices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span>
                <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_put</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">indices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_put_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">indices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span>
                 <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_put_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">indices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">instance_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">use_input_stats</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                    <span class="n">momentum</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                    <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                    <span class="n">cudnn_enabled</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">irfft</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">signal_ndim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">onesided</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">signal_sizes</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">is_distributed</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">is_nonzero</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">is_same_size</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">is_signed</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">rtol</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
              <span class="n">atol</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
              <span class="n">equal_nan</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">kthvalue</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">k</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">normalized_shape</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                 <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
                 <span class="n">cudnn_enable</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">weight</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">weight</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">steps</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
               <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">steps</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log10_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log1p_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log2_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">logdet</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">steps</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
               <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">steps</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">batch_sizes</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">hx</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
           <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
           <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
           <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">hx</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
           <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
           <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
           <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">batch_first</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lstm_cell</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">hx</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                <span class="n">w_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">w_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">b_ih</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                <span class="n">b_hh</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                          <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">value</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">masked_scatter</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">matrix_power</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">symmetric</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">tol</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                  <span class="n">symmetric</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">max_pool1d_with_indices</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                              <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[],</span>
                              <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                              <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                              <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">tensors</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">miopen_batch_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                        <span class="n">exponential_average_factor</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                        <span class="n">epsilon</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">miopen_convolution</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">benchmark</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                         <span class="n">deterministic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">miopen_convolution_transpose</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                   <span class="n">benchmark</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                   <span class="n">deterministic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">miopen_depthwise_convolution</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                   <span class="n">benchmark</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                   <span class="n">deterministic</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mkldnn_convolution</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mkldnn_convolution_backward_weights</span><span class="p">(</span><span class="n">weight_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                          <span class="n">grad_output</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                          <span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                          <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                          <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                          <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                          <span class="n">groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                          <span class="n">bias_defined</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
          <span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
          <span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
          <span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">],</span>
          <span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">l</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">num_samples</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">replacement</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                  <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">num_samples</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">replacement</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                  <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">],</span>
                  <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">mvlgamma</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">p</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">start</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">native_batch_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                        <span class="n">momentum</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                        <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">native_clone</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">native_norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">native_pow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">exponent</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">native_pow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">exponent</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">native_resize_as_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">the_template</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">native_zero_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
         <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
         <span class="n">b</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
         <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">norm_except_dim</span><span class="p">(</span><span class="n">v</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="nb">pow</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                      <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">std</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
             <span class="n">std</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">std</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
             <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">std</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">],</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
             <span class="n">std</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">],</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">std</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
             <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">],</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nuclear_norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nuclear_norm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">orgqr</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">orgqr</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ormqr</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">input3</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">left</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">transpose</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">ormqr</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">input3</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">left</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">transpose</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pairwise_distance</span><span class="p">(</span><span class="n">x1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">x2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                        <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span>
                        <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pdist</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">rcond</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pixel_shuffle</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">upscale_factor</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">polygamma</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">polygamma</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">potri</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">potri</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">exponent</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">exponent</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">exponent</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">exponent</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">exponent</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">exponent</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pstrf</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">tol</span> <span class="p">:</span> <span class="n">number</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">quantized_gru_cell</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">w_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">w_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">b_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">b_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">packed_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">packed_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">col_offsets_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">col_offsets_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">scale_ih</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                         <span class="n">scale_hh</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                         <span class="n">zero_point_ih</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                         <span class="n">zero_point_hh</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">quantized_lstm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">hx</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                     <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                     <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                     <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                     <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                     <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                     <span class="n">batch_first</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">quantized_lstm_cell</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">hx</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                          <span class="n">w_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">w_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">b_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">b_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">packed_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">packed_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">col_offsets_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">col_offsets_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">scale_ih</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                          <span class="n">scale_hh</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                          <span class="n">zero_point_ih</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                          <span class="n">zero_point_hh</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">quantized_rnn_relu_cell</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">w_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">w_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">b_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">b_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">packed_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">packed_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">col_offsets_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">col_offsets_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">scale_ih</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                              <span class="n">scale_hh</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                              <span class="n">zero_point_ih</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                              <span class="n">zero_point_hh</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">quantized_rnn_tanh_cell</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">w_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">w_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">b_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">b_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">packed_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">packed_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">col_offsets_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">col_offsets_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">scale_ih</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                              <span class="n">scale_hh</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                              <span class="n">zero_point_ih</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                              <span class="n">zero_point_hh</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">high</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">high</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randint_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">high</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randint_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">low</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">high</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randint_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">high</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randint_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">low</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">high</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
               <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">step</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">start</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">end</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">step</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">relu_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">renorm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">maxnorm</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">renorm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">maxnorm</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">shape</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">resize_as_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">the_template</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">signal_ndim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">onesided</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rnn_relu</span><span class="p">(</span><span class="n">data</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">batch_sizes</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
               <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
               <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rnn_relu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
               <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
               <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">batch_first</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rnn_relu_cell</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">w_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">w_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">b_ih</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">b_hh</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rnn_tanh</span><span class="p">(</span><span class="n">data</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">batch_sizes</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
               <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
               <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rnn_tanh</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">params</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
               <span class="n">has_biases</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">num_layers</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dropout</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
               <span class="n">train</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">bidirectional</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
               <span class="n">batch_first</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rnn_tanh_cell</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">hx</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">w_ih</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">w_hh</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">b_ih</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">b_hh</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">shifts</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">dims</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rot90</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">k</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">dims</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">round_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rrelu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">lower</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.125</span><span class="p">,</span>
            <span class="n">upper</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.3333333333333333</span><span class="p">,</span>
            <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rrelu_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">lower</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.125</span><span class="p">,</span>
             <span class="n">upper</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.3333333333333333</span><span class="p">,</span>
             <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rsqrt_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rsub</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rsub</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">s_copy_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">src</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">non_blocking</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">s_native_addmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">s_native_addmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">s_native_addmm_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">scalar_tensor</span><span class="p">(</span><span class="n">s</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                    <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">src</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">scatter_add</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">src</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">index</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">list</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
             <span class="n">idx</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
             <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
             <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">torch</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">list</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">],</span>
             <span class="n">idx</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">t</span>

<span class="n">torch</span><span class="o">.</span><span class="n">selu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">selu_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sigmoid_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sin_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sinh_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">slogdet</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">smm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">A</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">descending</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">split_size</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">split_sizes</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">split_with_sizes</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">split_sizes</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                       <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sqrt_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sspaddmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sspaddmm</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">stft</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">n_fft</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">hop_length</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">win_length</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">window</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
           <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">onesided</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">a</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
          <span class="n">b</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">some</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">compute_uv</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">symeig</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">eigenvectors</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tan_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tanh_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">t</span><span class="p">],</span>
             <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">dims_self</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                <span class="n">dims_other</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">threshold</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">threshold</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">threshold_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">threshold</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                 <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">k</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">largest</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="nb">sorted</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">dim0</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">dim1</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">col</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">offset</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                   <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                   <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                   <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">positive</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">negative</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                          <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                          <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span>
                          <span class="n">swap</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="n">row</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">col</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">offset</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                   <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                   <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                   <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">trtrs</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">A</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">transpose</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">unitriangular</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">trunc</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">trunc</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">trunc_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Future</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">t</span>

<span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">zero_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">layout</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_max_pool2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_max_pool3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[],</span>
                        <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                        <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">count_include_pad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[],</span>
                        <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                        <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">count_include_pad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[],</span>
                        <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                        <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">count_include_pad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[],</span>
                        <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                        <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">count_include_pad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                  <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">scale</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">input_scale</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">scale</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">input_scale</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">scale</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">input_scale</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">fractional_max_pool2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">random_samples</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">fractional_max_pool3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">random_samples</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">min_val</span> <span class="p">:</span> <span class="n">number</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">max_val</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">min_val</span> <span class="p">:</span> <span class="n">number</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">max_val</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">min_val</span> <span class="p">:</span> <span class="n">number</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">max_val</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">negative_slope</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">negative_slope</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                        <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">negative_slope</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">log_sigmoid</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">log_sigmoid</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool2d_with_indices</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                     <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                     <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[],</span>
                                     <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                     <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                     <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool3d_with_indices</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                     <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                     <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[],</span>
                                     <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                     <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                     <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                          <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                          <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                          <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                          <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">margin</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">margin</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                    <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                    <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                    <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                    <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                      <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">ignore_index</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                      <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">ignore_index</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                      <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">ignore_index</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">ignore_index</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                        <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">num_classes</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">rrelu_with_noise</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">noise</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">lower</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.125</span><span class="p">,</span>
                              <span class="n">upper</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.3333333333333333</span><span class="p">,</span>
                              <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">],</span>
                              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">rrelu_with_noise</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">noise</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">lower</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.125</span><span class="p">,</span>
                              <span class="n">upper</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.3333333333333333</span><span class="p">,</span>
                              <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">rrelu_with_noise_</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">noise</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">lower</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.125</span><span class="p">,</span>
                               <span class="n">upper</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.3333333333333333</span><span class="p">,</span>
                               <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                               <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">reduction</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">threshold</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                      <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">threshold</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softshrink</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">lambd</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softshrink</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">lambd</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                        <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_col2im</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                         <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_depthwise2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_depthwise2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_dilated2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                 <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                 <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_dilated2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                 <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                 <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_dilated3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                 <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                 <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_dilated3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                 <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                 <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_transpose2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_transpose2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_transpose3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_conv_transpose3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                   <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                                   <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">output_padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                   <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">thnn_im2col</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_bicubic2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                <span class="n">align_corners</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_bicubic2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                <span class="n">align_corners</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_bilinear2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">align_corners</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_bilinear2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">align_corners</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_linear1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                               <span class="n">align_corners</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_linear1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                               <span class="n">align_corners</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest1d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest2d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_trilinear3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                  <span class="n">align_corners</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                  <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_trilinear3d</span><span class="p">(</span><span class="bp">self</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                  <span class="n">align_corners</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">adaptive_max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                     <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                                     <span class="n">return_indices</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">adaptive_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                     <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                                     <span class="n">return_indices</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">adaptive_max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                     <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                                     <span class="n">return_indices</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">affine_grid</span><span class="p">(</span><span class="n">theta</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">alpha_dropout</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                                  <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                               <span class="n">momentum</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                               <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">bilinear</span><span class="p">(</span><span class="n">input1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                         <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                         <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                         <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                         <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                         <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                     <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                     <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                                     <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                                     <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                                     <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span>
                                                     <span class="n">pos_weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">celu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">alpha</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                         <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                          <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                          <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                          <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                          <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                          <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                          <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                  <span class="n">ignore_index</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                                  <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                  <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">targets</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">input_lengths</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">target_lengths</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">blank</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                             <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span>
                             <span class="n">zero_infinity</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                            <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout2d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                              <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">dropout3d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                              <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">alpha</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                        <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">padding_idx</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                              <span class="n">max_norm</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                              <span class="n">norm_type</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                              <span class="n">scale_grad_by_freq</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">sparse</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">offsets</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">max_norm</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                                  <span class="n">norm_type</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                                  <span class="n">scale_grad_by_freq</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">mode</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span>
                                  <span class="n">sparse</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">feature_alpha_dropout</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                          <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                                          <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                          <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">fold</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">output_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                         <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                         <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">fractional_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                       <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                                       <span class="n">output_size</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                                                       <span class="n">output_ratio</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]],</span>
                                                       <span class="n">return_indices</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                       <span class="n">_random_samples</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">fractional_max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                       <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                                       <span class="n">output_size</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                                                       <span class="n">output_ratio</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]],</span>
                                                       <span class="n">return_indices</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                       <span class="n">_random_samples</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">grid_sample</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">grid</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">mode</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">bilinear</span><span class="p">,</span>
                                <span class="n">padding_mode</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">zeros</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">num_groups</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                               <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">tau</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                   <span class="n">hard</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                   <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
                                   <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">hardshrink</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">lambd</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">min_val</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                             <span class="n">max_val</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                             <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                         <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                         <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                         <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                         <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                         <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">instance_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                  <span class="n">running_mean</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">running_var</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                  <span class="n">use_input_stats</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">momentum</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                  <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                           <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                           <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                            <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                            <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">normalized_shape</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                               <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                               <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                               <span class="n">negative_slope</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                               <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">bias</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">local_response_norm</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">size</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                        <span class="n">alpha</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                                        <span class="n">beta</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>
                                        <span class="n">k</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                <span class="n">dim</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                <span class="n">_stacklevel</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">lp_pool1d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">norm_type</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                              <span class="n">kernel_size</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                              <span class="n">stride</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                              <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">lp_pool2d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">norm_type</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                              <span class="n">kernel_size</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                              <span class="n">stride</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                              <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                        <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                        <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                        <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                            <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                            <span class="n">stride</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                                            <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                            <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                            <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">return_indices</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                            <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                            <span class="n">stride</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                                            <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                            <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                            <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">return_indices</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                            <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                            <span class="n">stride</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                                            <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                            <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                            <span class="n">ceil_mode</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                            <span class="n">return_indices</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">max_unpool1d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">stride</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">stride</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">max_unpool3d</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">indices</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                 <span class="n">stride</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
                                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                 <span class="n">output_size</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                             <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                             <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                      <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                      <span class="n">p</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                      <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                      <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                      <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                      <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                           <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                           <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                           <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                           <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                                <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                                <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                                <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                                <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">weight</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                             <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                             <span class="n">ignore_index</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                             <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                             <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span>
                              <span class="n">out</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">pad</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">mode</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">constant</span><span class="p">,</span>
                        <span class="n">value</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad_circular</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pairwise_distance</span><span class="p">(</span><span class="n">x1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                      <span class="n">x2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                      <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                                      <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span>
                                      <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">poisson_nll_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                     <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                     <span class="n">log_input</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                     <span class="n">full</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                     <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                     <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
                                     <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                     <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu6</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">rrelu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                          <span class="n">lower</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.125</span><span class="p">,</span>
                          <span class="n">upper</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.3333333333333333</span><span class="p">,</span>
                          <span class="n">training</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">selu</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                   <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                   <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                     <span class="n">target</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                     <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                     <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                     <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">dim</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                            <span class="n">_stacklevel</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                            <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmin</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                            <span class="n">dim</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                            <span class="n">_stacklevel</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                            <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softsign</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">tanhshrink</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                              <span class="n">threshold</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                              <span class="n">value</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                              <span class="n">inplace</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">positive</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">negative</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                        <span class="n">margin</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                        <span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                                        <span class="n">eps</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span>
                                        <span class="n">swap</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                        <span class="n">size_average</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                        <span class="n">reduce</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">],</span>
                                        <span class="n">reduction</span> <span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">mean</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="nb">input</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">kernel_size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                           <span class="n">dilation</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">padding</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
</div>
<div class="section" id="supported-methods">
<h5><a class="toc-backref" href="#id12">Supported Methods</a><a class="headerlink" href="#supported-methods" title="Permalink to this headline">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__and__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__and__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__iand__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__iand__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__ilshift__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__ilshift__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__ior__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__ior__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__irshift__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__irshift__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__ixor__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__ixor__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__lshift__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__lshift__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__or__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__or__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__rshift__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__rshift__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__xor__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="fm">__xor__</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">abs_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">acos</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">acos</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">acos_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addbmm</span><span class="p">(</span><span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addbmm</span><span class="p">(</span><span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addbmm_</span><span class="p">(</span><span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">tensor1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">tensor2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addmm_</span><span class="p">(</span><span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addmv</span><span class="p">(</span><span class="n">mat</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addmv</span><span class="p">(</span><span class="n">mat</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addmv_</span><span class="p">(</span><span class="n">mat</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addr</span><span class="p">(</span><span class="n">vec1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addr</span><span class="p">(</span><span class="n">vec1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">addr_</span><span class="p">(</span><span class="n">vec1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">rtol</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
                <span class="n">atol</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
                <span class="n">equal_nan</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">argmin</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">descending</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                  <span class="n">storage_offset</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">as_strided_</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                   <span class="n">stride</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                   <span class="n">storage_offset</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">asin</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">asin</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">asin_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">atan</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">atan2</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">atan2</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">atan2_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">atan_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">baddbmm</span><span class="p">(</span><span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">baddbmm</span><span class="p">(</span><span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">baddbmm_</span><span class="p">(</span><span class="n">batch1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">batch2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">],</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">bernoulli_</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">bernoulli_</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                  <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">weights</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                <span class="n">minlength</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">btrifact</span><span class="p">(</span><span class="n">pivot</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">btrifact_with_info</span><span class="p">(</span><span class="n">pivot</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">btrisolve</span><span class="p">(</span><span class="n">LU_data</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">LU_pivots</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">btrisolve</span><span class="p">(</span><span class="n">LU_data</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">LU_pivots</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cauchy_</span><span class="p">(</span><span class="n">median</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
               <span class="n">sigma</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ceil</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ceil_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">chunks</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
             <span class="nb">max</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
             <span class="nb">max</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">min</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
              <span class="nb">max</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp_max_</span><span class="p">(</span><span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clamp_min_</span><span class="p">(</span><span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cos</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cos_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cosh</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cosh_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">det</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">detach_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">offset</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">dim1</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span>
                  <span class="n">dim2</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">diagflat</span><span class="p">(</span><span class="n">offset</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">offset</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">dim1</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">dim2</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">digamma</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">digamma</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">digamma_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">tensor</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">tensor</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">eigenvectors</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">eq_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">eq_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erf</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erf_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erfc</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erfc</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erfc_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erfinv</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erfinv</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">erfinv_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">exp_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
              <span class="n">implicit</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">expm1</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">expm1_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">exponential_</span><span class="p">(</span><span class="n">lambd</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                    <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">signal_ndim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">value</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">end_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">dims</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">floor</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">floor_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fmod_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">fmod_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">frac</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">frac</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">frac_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">sparse_grad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">sparse_grad</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ge_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ge_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gels</span><span class="p">(</span><span class="n">A</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">geometric_</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                  <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">geqrf</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ger</span><span class="p">(</span><span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ger</span><span class="p">(</span><span class="n">vec2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gt_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">gt_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">hardshrink</span><span class="p">(</span><span class="n">lambd</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="n">bins</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
             <span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="n">bins</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
             <span class="nb">min</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="nb">max</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ifft</span><span class="p">(</span><span class="n">signal_ndim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_add</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_copy</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_copy_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_fill</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">value</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_fill</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">value</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_put</span><span class="p">(</span><span class="n">indices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span>
                 <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_put</span><span class="p">(</span><span class="n">indices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_put_</span><span class="p">(</span><span class="n">indices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]],</span>
                  <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_put_</span><span class="p">(</span><span class="n">indices</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                  <span class="n">values</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">indices</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">inverse</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">irfft</span><span class="p">(</span><span class="n">signal_ndim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">onesided</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">signal_sizes</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">is_coalesced</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">is_complex</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">is_distributed</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">is_nonzero</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">is_same_size</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">is_set_to</span><span class="p">(</span><span class="n">tensor</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">is_signed</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">rtol</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span>
               <span class="n">atol</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
               <span class="n">equal_nan</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">number</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">kthvalue</span><span class="p">(</span><span class="n">k</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">le_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">le_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">weight</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">weight</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span><span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span><span class="n">end</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">weight</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lgamma</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lgamma_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log10</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log10_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log1p</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log1p_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log2</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log2_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log_normal_</span><span class="p">(</span><span class="n">mean</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                   <span class="n">std</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                   <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">logdet</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                 <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                 <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lt_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">lt_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">value</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">value</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">masked_scatter</span><span class="p">(</span><span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                      <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">masked_scatter_</span><span class="p">(</span><span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                     <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">matrix_power</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">median</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">num_samples</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">replacement</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">num_samples</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">replacement</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                   <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">],</span>
                   <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">vec</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mvlgamma</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">mvlgamma_</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">start</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">narrow_copy</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">start</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">length</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
          <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ne_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ne_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">neg_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">number</span><span class="p">],</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">mean</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
               <span class="n">std</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">orgqr</span><span class="p">(</span><span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">orgqr</span><span class="p">(</span><span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ormqr</span><span class="p">(</span><span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">input3</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">left</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">transpose</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">ormqr</span><span class="p">(</span><span class="n">input2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">input3</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">left</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">transpose</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="n">dims</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pinverse</span><span class="p">(</span><span class="n">rcond</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">polygamma_</span><span class="p">(</span><span class="n">n</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">potri</span><span class="p">(</span><span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">potri</span><span class="p">(</span><span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">exponent</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">exponent</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">exponent</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">exponent</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pow_</span><span class="p">(</span><span class="n">exponent</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pow_</span><span class="p">(</span><span class="n">exponent</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="n">weight</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
            <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">pstrf</span><span class="p">(</span><span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">tol</span> <span class="p">:</span> <span class="n">number</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">put_</span><span class="p">(</span><span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">accumulate</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">qr</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="n">to</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="kn">from</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">to</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">reciprocal_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">relu</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">relu_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
                 <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">remainder_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">remainder_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">renorm</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">maxnorm</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
              <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">renorm</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
              <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">maxnorm</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">renorm_</span><span class="p">(</span><span class="n">p</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
               <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">maxnorm</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeats</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">reshape_as</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">resize_as_</span><span class="p">(</span><span class="n">the_template</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">rfft</span><span class="p">(</span><span class="n">signal_ndim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">onesided</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">shifts</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">dims</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">rot90</span><span class="p">(</span><span class="n">k</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">dims</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">round</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">round_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">rsqrt_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">src</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
               <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">src</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">value</span> <span class="p">:</span> <span class="n">number</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">scatter_add</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">src</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">scatter_add_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">src</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">index</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">source</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sigmoid_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sign</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sign_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sin_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sinh</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sinh_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">slogdet</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">smm</span><span class="p">(</span><span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
               <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">descending</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sparse_dim</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sparse_resize_</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                      <span class="n">sparse_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">dense_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sparse_resize_and_clear_</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                                <span class="n">sparse_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                <span class="n">dense_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_size</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_sizes</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
             <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">split_with_sizes</span><span class="p">(</span><span class="n">split_sizes</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sqrt_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sspaddmm</span><span class="p">(</span><span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sspaddmm</span><span class="p">(</span><span class="n">mat1</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">mat2</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                <span class="n">beta</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">stft</span><span class="p">(</span><span class="n">n_fft</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">hop_length</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">win_length</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
            <span class="n">window</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
            <span class="n">normalized</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">onesided</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
           <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">number</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="p">:</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
           <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">sum_to_size</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">some</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">compute_uv</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">symeig</span><span class="p">(</span><span class="n">eigenvectors</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
              <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">t_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">index</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tan</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tan_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tanh_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
          <span class="n">non_blocking</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">copy</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">non_blocking</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">copy</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">non_blocking</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">copy</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">layout</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
          <span class="n">device</span> <span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
          <span class="n">non_blocking</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">copy</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Device</span><span class="p">],</span>
          <span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">non_blocking</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">copy</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
          <span class="n">non_blocking</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">copy</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">non_blocking</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">copy</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">(</span><span class="n">sparse_dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">k</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
            <span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">largest</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="nb">sorted</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">trace</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">dim1</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">transpose_</span><span class="p">(</span><span class="n">dim0</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">dim1</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">tril_</span><span class="p">(</span><span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">triu_</span><span class="p">(</span><span class="n">diagonal</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">trtrs</span><span class="p">(</span><span class="n">A</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
             <span class="n">upper</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">transpose</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">unitriangular</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">trunc</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">trunc</span><span class="p">(</span><span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">trunc_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="n">dimension</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">size</span> <span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">step</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="kn">from</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                <span class="n">to</span> <span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                <span class="n">generator</span> <span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Generator</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">out</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
           <span class="n">unbiased</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">keepdim</span> <span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">size</span> <span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">other</span> <span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>

<span class="n">Tensor</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="frequently-asked-questions">
<h3><a class="toc-backref" href="#id13">Frequently Asked Questions</a><a class="headerlink" href="#frequently-asked-questions" title="Permalink to this headline">¶</a></h3>
<p>Q: I would like to train a model on GPU and do inference on CPU. What are the
best practices?</p>
<blockquote>
<div><p>First convert your model from GPU to CPU and then save it, like so:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cpu_model</span> <span class="o">=</span> <span class="n">gpu_model</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">sample_input_cpu</span> <span class="o">=</span> <span class="n">sample_input_gpu</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">traced_cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">traced_cpu</span><span class="p">,</span> <span class="n">sample_input_cpu</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">traced_cpu</span><span class="p">,</span> <span class="s2">&quot;cpu.pth&quot;</span><span class="p">)</span>

<span class="n">traced_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">traced_gpu</span><span class="p">,</span> <span class="n">sample_input_gpu</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">traced_gpu</span><span class="p">,</span> <span class="s2">&quot;gpu.pth&quot;</span><span class="p">)</span>

<span class="c1"># ... later, when using the model:</span>

<span class="k">if</span> <span class="n">use_gpu</span><span class="p">:</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;gpu.pth&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;cpu.pth&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>This is recommended because the tracer may witness tensor creation on a
specific device, so casting an already-loaded model may have unexpected
effects. Casting the model <em>before</em> saving it ensures that the tracer has
the correct device information.</p>
</div></blockquote>
</div>
</div>
<span id="document-multiprocessing"></span><div class="section" id="module-torch.multiprocessing">
<span id="multiprocessing-package-torch-multiprocessing"></span><h2>Multiprocessing package - torch.multiprocessing<a class="headerlink" href="#module-torch.multiprocessing" title="Permalink to this headline">¶</a></h2>
<p>torch.multiprocessing is a wrapper around the native <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing" title="(in Python v3.7)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">multiprocessing</span></code></a>
module. It registers custom reducers, that use shared memory to provide shared
views on the same data in different processes. Once the tensor/storage is moved
to shared_memory (see <a class="reference internal" href="index.html#torch.Tensor.share_memory_" title="torch.Tensor.share_memory_"><code class="xref py py-func docutils literal notranslate"><span class="pre">share_memory_()</span></code></a>), it will be possible
to send it to other processes without making any copies.</p>
<p>The API is 100% compatible with the original module - it’s enough to change
<code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">multiprocessing</span></code> to <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch.multiprocessing</span></code> to have all the
tensors sent through the queues or shared via other mechanisms, moved to shared
memory.</p>
<p>Because of the similarity of APIs we do not document most of this package
contents, and we recommend referring to very good docs of the original module.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the main process exits abruptly (e.g. because of an incoming signal),
Python’s <code class="docutils literal notranslate"><span class="pre">multiprocessing</span></code> sometimes fails to clean up its children.
It’s a known caveat, so if you’re seeing any resource leaks after
interrupting the interpreter, it probably means that this has just happened
to you.</p>
</div>
<div class="section" id="strategy-management">
<h3>Strategy management<a class="headerlink" href="#strategy-management" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.multiprocessing.get_all_sharing_strategies">
<code class="descclassname">torch.multiprocessing.</code><code class="descname">get_all_sharing_strategies</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.multiprocessing.get_all_sharing_strategies" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a set of sharing strategies supported on a current system.</p>
</dd></dl>

<dl class="function">
<dt id="torch.multiprocessing.get_sharing_strategy">
<code class="descclassname">torch.multiprocessing.</code><code class="descname">get_sharing_strategy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.multiprocessing.get_sharing_strategy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current strategy for sharing CPU tensors.</p>
</dd></dl>

<dl class="function">
<dt id="torch.multiprocessing.set_sharing_strategy">
<code class="descclassname">torch.multiprocessing.</code><code class="descname">set_sharing_strategy</code><span class="sig-paren">(</span><em>new_strategy</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.multiprocessing.set_sharing_strategy" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the strategy for sharing CPU tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_strategy</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – Name of the selected strategy. Should be one of
the values returned by <a class="reference internal" href="#torch.multiprocessing.get_all_sharing_strategies" title="torch.multiprocessing.get_all_sharing_strategies"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_all_sharing_strategies()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="sharing-cuda-tensors">
<h3>Sharing CUDA tensors<a class="headerlink" href="#sharing-cuda-tensors" title="Permalink to this headline">¶</a></h3>
<p>Sharing CUDA tensors between processes is supported only in Python 3, using
a <code class="docutils literal notranslate"><span class="pre">spawn</span></code> or <code class="docutils literal notranslate"><span class="pre">forkserver</span></code> start methods. <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing" title="(in Python v3.7)"><code class="docutils literal notranslate"><span class="pre">multiprocessing</span></code></a> in
Python 2 can only create subprocesses using <code class="docutils literal notranslate"><span class="pre">fork</span></code>, and it’s not supported
by the CUDA runtime.</p>
<p>Unlike CPU tensors, the sending process is required to keep the original tensor
as long as the receiving process retains a copy of the tensor.
This shouldn’t be a problem for sharing model parameters (which stay live
for the entire execution of the model), but passing other
kinds of data should be done with care.</p>
<p>Here is an example program which handles these requirements correctly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_default_tensor_type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sender</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">e</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">s_sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">q</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">s_sample</span><span class="p">)</span>
        <span class="n">e</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
        <span class="k">del</span> <span class="n">s_sample</span>
        <span class="n">e</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;spawn&quot;</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">Queue</span><span class="p">()</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">sender</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">e</span><span class="p">))</span>
    <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=== ITER </span><span class="si">{}</span><span class="s1"> ===&quot;.format(i))</span>
        <span class="n">r_sample</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">del</span> <span class="n">r_sample</span>
        <span class="n">e</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>

    <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
<p>In the example above, calling <cite>e.wait()</cite>
on sender side ensures tensor <cite>s_sample</cite> doesn’t get deleted while
receiver is working on it.  The receiver signals when it is done
with the tensor using <cite>e.set()</cite>, being careful to <cite>del</cite> its reference
to the received tensor first.  It is INSUFFICIENT to promise never to call
<cite>r_sample</cite> again; while <cite>r_sample</cite> is live, it may be confused with
any subsequent tensors allocated by the source process at the same address.</p>
<p>If a receiver wants to save the data of <cite>r_sample</cite> for future use while
letting the source process deallocate the original, it must
<cite>clone()</cite> it.</p>
<p>This behavior is very confusing, and we are tracking a fix for it
at <a class="reference external" href="https://github.com/pytorch/pytorch/issues/16141">https://github.com/pytorch/pytorch/issues/16141</a></p>
</div>
<div class="section" id="sharing-strategies">
<h3>Sharing strategies<a class="headerlink" href="#sharing-strategies" title="Permalink to this headline">¶</a></h3>
<p>This section provides a brief overview into how different sharing strategies
work. Note that it applies only to CPU tensor - CUDA tensors will always use
the CUDA API, as that’s the only way they can be shared.</p>
<div class="section" id="file-descriptor-file-descriptor">
<h4>File descriptor - <code class="docutils literal notranslate"><span class="pre">file_descriptor</span></code><a class="headerlink" href="#file-descriptor-file-descriptor" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is the default strategy (except for macOS and OS X where it’s not
supported).</p>
</div>
<p>This strategy will use file descriptors as shared memory handles. Whenever a
storage is moved to shared memory, a file descriptor obtained from <code class="docutils literal notranslate"><span class="pre">shm_open</span></code>
is cached with the object, and when it’s going to be sent to other processes,
the file descriptor will be transferred (e.g. via UNIX sockets) to it. The
receiver will also cache the file descriptor and <code class="docutils literal notranslate"><span class="pre">mmap</span></code> it, to obtain a shared
view onto the storage data.</p>
<p>Note that if there will be a lot of tensors shared, this strategy will keep a
large number of file descriptors open most of the time. If your system has low
limits for the number of open file descriptors, and you can’t raise them, you
should use the <code class="docutils literal notranslate"><span class="pre">file_system</span></code> strategy.</p>
</div>
<div class="section" id="file-system-file-system">
<h4>File system - <code class="docutils literal notranslate"><span class="pre">file_system</span></code><a class="headerlink" href="#file-system-file-system" title="Permalink to this headline">¶</a></h4>
<p>This strategy will use file names given to <code class="docutils literal notranslate"><span class="pre">shm_open</span></code> to identify the shared
memory regions. This has a benefit of not requiring the implementation to cache
the file descriptors obtained from it, but at the same time is prone to shared
memory leaks. The file can’t be deleted right after its creation, because other
processes need to access it to open their views. If the processes fatally
crash, or are killed, and don’t call the storage destructors, the files will
remain in the system. This is very serious, because they keep using up the
memory until the system is restarted, or they’re freed manually.</p>
<p>To counter the problem of shared memory file leaks, <a class="reference internal" href="#module-torch.multiprocessing" title="torch.multiprocessing"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code></a>
will spawn a daemon named <code class="docutils literal notranslate"><span class="pre">torch_shm_manager</span></code> that will isolate itself from
the current process group, and will keep track of all shared memory allocations.
Once all processes connected to it exit, it will wait a moment to ensure there
will be no new connections, and will iterate over all shared memory files
allocated by the group. If it finds that any of them still exist, they will be
deallocated. We’ve tested this method and it proved to be robust to various
failures. Still, if your system has high enough limits, and <code class="docutils literal notranslate"><span class="pre">file_descriptor</span></code>
is a supported strategy, we do not recommend switching to this one.</p>
</div>
</div>
<div class="section" id="spawning-subprocesses">
<h3>Spawning subprocesses<a class="headerlink" href="#spawning-subprocesses" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Available for Python &gt;= 3.4.</p>
<p>This depends on the <code class="docutils literal notranslate"><span class="pre">spawn</span></code> start method in Python’s
<code class="docutils literal notranslate"><span class="pre">multiprocessing</span></code> package.</p>
</div>
<p>Spawning a number of subprocesses to perform some function can be done
by creating <code class="docutils literal notranslate"><span class="pre">Process</span></code> instances and calling <code class="docutils literal notranslate"><span class="pre">join</span></code> to wait for
their completion. This approach works fine when dealing with a single
subprocess but presents potential issues when dealing with multiple
processes.</p>
<p>Namely, joining processes sequentially implies they will terminate
sequentially. If they don’t, and the first process does not terminate,
the process termination will go unnoticed. Also, there are no native
facilities for error propagation.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">spawn</span></code> function below addresses these concerns and takes care
of error propagation, out of order termination, and will actively
terminate processes upon detecting an error in one of them.</p>
<dl class="function">
<dt id="torch.multiprocessing.spawn">
<code class="descclassname">torch.multiprocessing.</code><code class="descname">spawn</code><span class="sig-paren">(</span><em>fn</em>, <em>args=()</em>, <em>nprocs=1</em>, <em>join=True</em>, <em>daemon=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.multiprocessing.spawn" title="Permalink to this definition">¶</a></dt>
<dd><p>Spawns <code class="docutils literal notranslate"><span class="pre">nprocs</span></code> processes that run <code class="docutils literal notranslate"><span class="pre">fn</span></code> with <code class="docutils literal notranslate"><span class="pre">args</span></code>.</p>
<p>If one of the processes exits with a non-zero exit status, the
remaining processes are killed and an exception is raised with the
cause of termination. In the case an exception was caught in the
child process, it is forwarded and its traceback is included in
the exception raised in the parent process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> (<em>function</em>) – <p>Function is called as the entrypoint of the
spawned process. This function must be defined at the top
level of a module so it can be pickled and spawned. This
is a requirement imposed by multiprocessing.</p>
<p>The function is called as <code class="docutils literal notranslate"><span class="pre">fn(i,</span> <span class="pre">*args)</span></code>, where <code class="docutils literal notranslate"><span class="pre">i</span></code> is
the process index and <code class="docutils literal notranslate"><span class="pre">args</span></code> is the passed through tuple
of arguments.</p>
</p></li>
<li><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Arguments passed to <code class="docutils literal notranslate"><span class="pre">fn</span></code>.</p></li>
<li><p><strong>nprocs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Number of processes to spawn.</p></li>
<li><p><strong>join</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – Perform a blocking join on all processes.</p></li>
<li><p><strong>daemon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – The spawned processes’ daemon flag. If set to True,
daemonic processes will be created.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None if <code class="docutils literal notranslate"><span class="pre">join</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
<a class="reference internal" href="#torch.multiprocessing.SpawnContext" title="torch.multiprocessing.SpawnContext"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpawnContext</span></code></a> if <code class="docutils literal notranslate"><span class="pre">join</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.multiprocessing.SpawnContext">
<em class="property">class </em><code class="descclassname">torch.multiprocessing.</code><code class="descname">SpawnContext</code><a class="headerlink" href="#torch.multiprocessing.SpawnContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Returned by <a class="reference internal" href="#torch.multiprocessing.spawn" title="torch.multiprocessing.spawn"><code class="xref py py-func docutils literal notranslate"><span class="pre">spawn()</span></code></a> when called with <code class="docutils literal notranslate"><span class="pre">join=False</span></code>.</p>
<dl class="method">
<dt id="torch.multiprocessing.SpawnContext.join">
<code class="descname">join</code><span class="sig-paren">(</span><em>timeout=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.multiprocessing.SpawnContext.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Tries to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.</p>
<p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if all processes have been joined successfully,
<code class="docutils literal notranslate"><span class="pre">False</span></code> if there are more processes that need to be joined.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>timeout</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Wait this long before giving up on waiting.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<span id="document-bottleneck"></span><div class="section" id="torch-utils-bottleneck">
<h2>torch.utils.bottleneck<a class="headerlink" href="#torch-utils-bottleneck" title="Permalink to this headline">¶</a></h2>
<p><cite>torch.utils.bottleneck</cite> is a tool that can be used as an initial step for
debugging bottlenecks in your program. It summarizes runs of your script with
the Python profiler and PyTorch’s autograd profiler.</p>
<p>Run it on the command line with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">bottleneck</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">source</span><span class="o">/</span><span class="n">script</span><span class="o">.</span><span class="n">py</span> <span class="p">[</span><span class="n">args</span><span class="p">]</span>
</pre></div>
</div>
<p>where [args] are any number of arguments to <cite>script.py</cite>, or run
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">torch.utils.bottleneck</span> <span class="pre">-h</span></code> for more usage instructions.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Because your script will be profiled, please ensure that it exits in a
finite amount of time.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Due to the asynchronous nature of CUDA kernels, when running against
CUDA code, the cProfile output and CPU-mode autograd profilers may
not show correct timings: the reported CPU time reports the amount of time
used to launch the kernels but does not include the time the kernel
spent executing on a GPU unless the operation does a synchronize.
Ops that do synchronize appear to be extremely expensive under regular
CPU-mode profilers.
In these case where timings are incorrect, the CUDA-mode autograd profiler
may be helpful.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to
look at, you should first check if your script is CPU-bound
(“CPU total time is much greater than CUDA total time”).
If it is CPU-bound, looking at the results of the CPU-mode autograd
profiler will help. If on the other hand your script spends most of its
time executing on the GPU, then it makes sense to start
looking for responsible CUDA operators in the output of the CUDA-mode
autograd profiler.</p>
<p>Of course the reality is much more complicated and your script might not be
in one of those two extremes depending on the part of the model you’re
evaluating. If the profiler outputs don’t help, you could try looking at
the result of <a class="reference internal" href="index.html#torch.autograd.profiler.emit_nvtx" title="torch.autograd.profiler.emit_nvtx"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.profiler.emit_nvtx()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">nvprof</span></code>.
However, please take into account that the NVTX overhead is very high and
often gives a heavily skewed timeline.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you are profiling CUDA code, the first profiler that <code class="docutils literal notranslate"><span class="pre">bottleneck</span></code> runs
(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)
in its time reporting. This should not matter if your bottlenecks result
in code much slower than the CUDA startup time.</p>
</div>
<p>For more complicated uses of the profilers (like in a multi-GPU case),
please see <a class="reference external" href="https://docs.python.org/3/library/profile.html">https://docs.python.org/3/library/profile.html</a>
or <a class="reference internal" href="index.html#torch.autograd.profiler.profile" title="torch.autograd.profiler.profile"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.profiler.profile()</span></code></a> for more information.</p>
</div>
<span id="document-checkpoint"></span><div class="section" id="torch-utils-checkpoint">
<h2>torch.utils.checkpoint<a class="headerlink" href="#torch-utils-checkpoint" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Checkpointing is implemented by rerunning a forward-pass segment for
each checkpointed segment during backward.  This can cause persistent
states like the RNG state to be advanced than they would without
checkpointing.  By default, checkpointing includes logic to juggle
the RNG state such that checkpointed passes making use of RNG
(through dropout for example) have deterministic output as
compared to non-checkpointed passes.  The logic to stash and restore
RNG states can incur a moderate performance hit depending on the runtime
of checkpointed operations.  If deterministic output compared to
non-checkpointed passes is not required, supply <code class="docutils literal notranslate"><span class="pre">preserve_rng_state=False</span></code>
to <code class="docutils literal notranslate"><span class="pre">checkpoint</span></code> or <code class="docutils literal notranslate"><span class="pre">checkpoint_sequential</span></code> to omit stashing and
restoring the RNG state during each checkpoint.</p>
<p>The stashing logic saves and restores the RNG state for the current device
and the device of all cuda Tensor arguments to the <code class="docutils literal notranslate"><span class="pre">run_fn</span></code>.
However, the logic has no way to anticipate if the user will move
Tensors to a new device within the <code class="docutils literal notranslate"><span class="pre">run_fn</span></code> itself.  Therefore, if you move
Tensors to a new device (“new” meaning not belonging to the set of
[current device + devices of Tensor arguments]) within <code class="docutils literal notranslate"><span class="pre">run_fn</span></code>, deterministic
output compared to non-checkpointed passes is never guaranteed.</p>
</div>
<dl class="function">
<dt id="torch.utils.checkpoint.checkpoint">
<code class="descclassname">torch.utils.checkpoint.</code><code class="descname">checkpoint</code><span class="sig-paren">(</span><em>function</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.checkpoint.checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Checkpoint a model or part of the model</p>
<p>Checkpointing works by trading compute for memory. Rather than storing all
intermediate activations of the entire computation graph for computing
backward, the checkpointed part does <strong>not</strong> save intermediate activations,
and instead recomputes them in backward pass. It can be applied on any part
of a model.</p>
<p>Specifically, in the forward pass, <code class="xref py py-attr docutils literal notranslate"><span class="pre">function</span></code> will run in
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> manner, i.e., not storing the intermediate
activations. Instead, the forward pass saves the inputs tuple and the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">function</span></code> parameter. In the backwards pass, the saved inputs and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">function</span></code> is retreived, and the forward pass is computed on
<code class="xref py py-attr docutils literal notranslate"><span class="pre">function</span></code> again, now tracking the intermediate activations, and then
the gradients are calculated using these activation values.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Checkpointing doesn’t work with <a class="reference internal" href="index.html#torch.autograd.grad" title="torch.autograd.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a>, but only
with <a class="reference internal" href="index.html#torch.autograd.backward" title="torch.autograd.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.backward()</span></code></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">function</span></code> invocation during backward does anything different
than the one during forward, e.g., due to some global variable, the
checkpointed version won’t be equivalent, and unfortunately it can’t be
detected.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>function</strong> – describes what to run in the forward pass of the model or
part of the model. It should also know how to handle the inputs
passed as the tuple. For example, in LSTM, if user passes
<code class="docutils literal notranslate"><span class="pre">(activation,</span> <span class="pre">hidden)</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">function</span></code> should correctly use the
first input as <code class="docutils literal notranslate"><span class="pre">activation</span></code> and the second input as <code class="docutils literal notranslate"><span class="pre">hidden</span></code></p></li>
<li><p><strong>preserve_rng_state</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – Omit stashing and restoring
the RNG state during each checkpoint.</p></li>
<li><p><strong>args</strong> – tuple containing inputs to the <code class="xref py py-attr docutils literal notranslate"><span class="pre">function</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output of running <code class="xref py py-attr docutils literal notranslate"><span class="pre">function</span></code> on <code class="xref py py-attr docutils literal notranslate"><span class="pre">*args</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.utils.checkpoint.checkpoint_sequential">
<code class="descclassname">torch.utils.checkpoint.</code><code class="descname">checkpoint_sequential</code><span class="sig-paren">(</span><em>functions</em>, <em>segments</em>, <em>*inputs</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.checkpoint.checkpoint_sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A helper function for checkpointing sequential models.</p>
<p>Sequential models execute a list of modules/functions in order
(sequentially). Therefore, we can divide such a model in various segments
and checkpoint each segment. All segments except the last will run in
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> manner, i.e., not storing the intermediate
activations. The inputs of each checkpointed segment will be saved for
re-running the segment in the backward pass.</p>
<p>See <a class="reference internal" href="#torch.utils.checkpoint.checkpoint" title="torch.utils.checkpoint.checkpoint"><code class="xref py py-func docutils literal notranslate"><span class="pre">checkpoint()</span></code></a> on how checkpointing works.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Checkpointing doesn’t work with <a class="reference internal" href="index.html#torch.autograd.grad" title="torch.autograd.grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a>, but only
with <a class="reference internal" href="index.html#torch.autograd.backward" title="torch.autograd.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.backward()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>functions</strong> – A <a class="reference internal" href="index.html#torch.nn.Sequential" title="torch.nn.Sequential"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Sequential</span></code></a> or the list of modules or
functions (comprising the model) to run sequentially.</p></li>
<li><p><strong>segments</strong> – Number of chunks to create in the model</p></li>
<li><p><strong>inputs</strong> – tuple of Tensors that are inputs to <code class="xref py py-attr docutils literal notranslate"><span class="pre">functions</span></code></p></li>
<li><p><strong>preserve_rng_state</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – Omit stashing and restoring
the RNG state during each checkpoint.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output of running <code class="xref py py-attr docutils literal notranslate"><span class="pre">functions</span></code> sequentially on <code class="xref py py-attr docutils literal notranslate"><span class="pre">*inputs</span></code></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_var</span> <span class="o">=</span> <span class="n">checkpoint_sequential</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">chunks</span><span class="p">,</span> <span class="n">input_var</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<span id="document-cpp_extension"></span><div class="section" id="torch-utils-cpp-extension">
<h2>torch.utils.cpp_extension<a class="headerlink" href="#torch-utils-cpp-extension" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.utils.cpp_extension.CppExtension">
<code class="descclassname">torch.utils.cpp_extension.</code><code class="descname">CppExtension</code><span class="sig-paren">(</span><em>name</em>, <em>sources</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.cpp_extension.CppExtension" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <code class="xref py py-class docutils literal notranslate"><span class="pre">setuptools.Extension</span></code> for C++.</p>
<p>Convenience method that creates a <code class="xref py py-class docutils literal notranslate"><span class="pre">setuptools.Extension</span></code> with the
bare minimum (but often sufficient) arguments to build a C++ extension.</p>
<p>All arguments are forwarded to the <code class="xref py py-class docutils literal notranslate"><span class="pre">setuptools.Extension</span></code>
constructor.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">setuptools</span> <span class="k">import</span> <span class="n">setup</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.utils.cpp_extension</span> <span class="k">import</span> <span class="n">BuildExtension</span><span class="p">,</span> <span class="n">CppExtension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">setup</span><span class="p">(</span>
<span class="go">        name=&#39;extension&#39;,</span>
<span class="go">        ext_modules=[</span>
<span class="go">            CppExtension(</span>
<span class="go">                name=&#39;extension&#39;,</span>
<span class="go">                sources=[&#39;extension.cpp&#39;],</span>
<span class="go">                extra_compile_args=[&#39;-g&#39;]),</span>
<span class="go">        ],</span>
<span class="go">        cmdclass={</span>
<span class="go">            &#39;build_ext&#39;: BuildExtension</span>
<span class="go">        })</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.utils.cpp_extension.CUDAExtension">
<code class="descclassname">torch.utils.cpp_extension.</code><code class="descname">CUDAExtension</code><span class="sig-paren">(</span><em>name</em>, <em>sources</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.cpp_extension.CUDAExtension" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <code class="xref py py-class docutils literal notranslate"><span class="pre">setuptools.Extension</span></code> for CUDA/C++.</p>
<p>Convenience method that creates a <code class="xref py py-class docutils literal notranslate"><span class="pre">setuptools.Extension</span></code> with the
bare minimum (but often sufficient) arguments to build a CUDA/C++
extension. This includes the CUDA include path, library path and runtime
library.</p>
<p>All arguments are forwarded to the <code class="xref py py-class docutils literal notranslate"><span class="pre">setuptools.Extension</span></code>
constructor.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">setuptools</span> <span class="k">import</span> <span class="n">setup</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.utils.cpp_extension</span> <span class="k">import</span> <span class="n">BuildExtension</span><span class="p">,</span> <span class="n">CUDAExtension</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">setup</span><span class="p">(</span>
<span class="go">        name=&#39;cuda_extension&#39;,</span>
<span class="go">        ext_modules=[</span>
<span class="go">            CUDAExtension(</span>
<span class="go">                    name=&#39;cuda_extension&#39;,</span>
<span class="go">                    sources=[&#39;extension.cpp&#39;, &#39;extension_kernel.cu&#39;],</span>
<span class="go">                    extra_compile_args={&#39;cxx&#39;: [&#39;-g&#39;],</span>
<span class="go">                                        &#39;nvcc&#39;: [&#39;-O2&#39;]})</span>
<span class="go">        ],</span>
<span class="go">        cmdclass={</span>
<span class="go">            &#39;build_ext&#39;: BuildExtension</span>
<span class="go">        })</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.utils.cpp_extension.BuildExtension">
<code class="descclassname">torch.utils.cpp_extension.</code><code class="descname">BuildExtension</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.cpp_extension.BuildExtension" title="Permalink to this definition">¶</a></dt>
<dd><p>A custom <code class="xref py py-mod docutils literal notranslate"><span class="pre">setuptools</span></code> build extension .</p>
<p>This <code class="xref py py-class docutils literal notranslate"><span class="pre">setuptools.build_ext</span></code> subclass takes care of passing the
minimum required compiler flags (e.g. <code class="docutils literal notranslate"><span class="pre">-std=c++11</span></code>) as well as mixed
C++/CUDA compilation (and support for CUDA files in general).</p>
<p>When using <a class="reference internal" href="#torch.utils.cpp_extension.BuildExtension" title="torch.utils.cpp_extension.BuildExtension"><code class="xref py py-class docutils literal notranslate"><span class="pre">BuildExtension</span></code></a>, it is allowed to supply a dictionary
for <code class="docutils literal notranslate"><span class="pre">extra_compile_args</span></code> (rather than the usual list) that maps from
languages (<code class="docutils literal notranslate"><span class="pre">cxx</span></code> or <code class="docutils literal notranslate"><span class="pre">cuda</span></code>) to a list of additional compiler flags to
supply to the compiler. This makes it possible to supply different flags to
the C++ and CUDA compiler during mixed compilation.</p>
</dd></dl>

<dl class="function">
<dt id="torch.utils.cpp_extension.load">
<code class="descclassname">torch.utils.cpp_extension.</code><code class="descname">load</code><span class="sig-paren">(</span><em>name</em>, <em>sources</em>, <em>extra_cflags=None</em>, <em>extra_cuda_cflags=None</em>, <em>extra_ldflags=None</em>, <em>extra_include_paths=None</em>, <em>build_directory=None</em>, <em>verbose=False</em>, <em>with_cuda=None</em>, <em>is_python_module=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.cpp_extension.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a PyTorch C++ extension just-in-time (JIT).</p>
<p>To load an extension, a Ninja build file is emitted, which is used to
compile the given sources into a dynamic library. This library is
subsequently loaded into the current Python process as a module and
returned from this function, ready for use.</p>
<p>By default, the directory to which the build file is emitted and the
resulting library compiled to is <code class="docutils literal notranslate"><span class="pre">&lt;tmp&gt;/torch_extensions/&lt;name&gt;</span></code>, where
<code class="docutils literal notranslate"><span class="pre">&lt;tmp&gt;</span></code> is the temporary folder on the current platform and <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;</span></code>
the name of the extension. This location can be overridden in two ways.
First, if the <code class="docutils literal notranslate"><span class="pre">TORCH_EXTENSIONS_DIR</span></code> environment variable is set, it
replaces <code class="docutils literal notranslate"><span class="pre">&lt;tmp&gt;/torch_extensions</span></code> and all extensions will be compiled
into subfolders of this directory. Second, if the <code class="docutils literal notranslate"><span class="pre">build_directory</span></code>
argument to this function is supplied, it overrides the entire path, i.e.
the library will be compiled into that folder directly.</p>
<p>To compile the sources, the default system compiler (<code class="docutils literal notranslate"><span class="pre">c++</span></code>) is used,
which can be overridden by setting the <code class="docutils literal notranslate"><span class="pre">CXX</span></code> environment variable. To pass
additional arguments to the compilation process, <code class="docutils literal notranslate"><span class="pre">extra_cflags</span></code> or
<code class="docutils literal notranslate"><span class="pre">extra_ldflags</span></code> can be provided. For example, to compile your extension
with optimizations, pass <code class="docutils literal notranslate"><span class="pre">extra_cflags=['-O3']</span></code>. You can also use
<code class="docutils literal notranslate"><span class="pre">extra_cflags</span></code> to pass further include directories.</p>
<p>CUDA support with mixed compilation is provided. Simply pass CUDA source
files (<code class="docutils literal notranslate"><span class="pre">.cu</span></code> or <code class="docutils literal notranslate"><span class="pre">.cuh</span></code>) along with other sources. Such files will be
detected and compiled with nvcc rather than the C++ compiler. This includes
passing the CUDA lib64 directory as a library directory, and linking
<code class="docutils literal notranslate"><span class="pre">cudart</span></code>. You can pass additional flags to nvcc via
<code class="docutils literal notranslate"><span class="pre">extra_cuda_cflags</span></code>, just like with <code class="docutils literal notranslate"><span class="pre">extra_cflags</span></code> for C++. Various
heuristics for finding the CUDA install directory are used, which usually
work fine. If not, setting the <code class="docutils literal notranslate"><span class="pre">CUDA_HOME</span></code> environment variable is the
safest option.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – The name of the extension to build. This MUST be the same as the
name of the pybind11 module!</p></li>
<li><p><strong>sources</strong> – A list of relative or absolute paths to C++ source files.</p></li>
<li><p><strong>extra_cflags</strong> – optional list of compiler flags to forward to the build.</p></li>
<li><p><strong>extra_cuda_cflags</strong> – optional list of compiler flags to forward to nvcc
when building CUDA sources.</p></li>
<li><p><strong>extra_ldflags</strong> – optional list of linker flags to forward to the build.</p></li>
<li><p><strong>extra_include_paths</strong> – optional list of include directories to forward
to the build.</p></li>
<li><p><strong>build_directory</strong> – optional path to use as build workspace.</p></li>
<li><p><strong>verbose</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, turns on verbose logging of load steps.</p></li>
<li><p><strong>with_cuda</strong> – Determines whether CUDA headers and libraries are added to
the build. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), this value is
automatically determined based on the existence of <code class="docutils literal notranslate"><span class="pre">.cu</span></code> or
<code class="docutils literal notranslate"><span class="pre">.cuh</span></code> in <code class="docutils literal notranslate"><span class="pre">sources</span></code>. Set it to <cite>True`</cite> to force CUDA headers
and libraries to be included.</p></li>
<li><p><strong>is_python_module</strong> – If <code class="docutils literal notranslate"><span class="pre">True</span></code> (default), imports the produced shared
library as a Python module. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, loads it into the process
as a plain dynamic library.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>If <code class="docutils literal notranslate"><span class="pre">is_python_module</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, returns the loaded PyTorch
extension as a Python module. If <code class="docutils literal notranslate"><span class="pre">is_python_module</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>
returns nothing (the shared library is loaded into the process as a side
effect).</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.utils.cpp_extension</span> <span class="k">import</span> <span class="n">load</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span>
<span class="go">        name=&#39;extension&#39;,</span>
<span class="go">        sources=[&#39;extension.cpp&#39;, &#39;extension_kernel.cu&#39;],</span>
<span class="go">        extra_cflags=[&#39;-O2&#39;],</span>
<span class="go">        verbose=True)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.utils.cpp_extension.load_inline">
<code class="descclassname">torch.utils.cpp_extension.</code><code class="descname">load_inline</code><span class="sig-paren">(</span><em>name</em>, <em>cpp_sources</em>, <em>cuda_sources=None</em>, <em>functions=None</em>, <em>extra_cflags=None</em>, <em>extra_cuda_cflags=None</em>, <em>extra_ldflags=None</em>, <em>extra_include_paths=None</em>, <em>build_directory=None</em>, <em>verbose=False</em>, <em>with_cuda=None</em>, <em>is_python_module=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.cpp_extension.load_inline" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a PyTorch C++ extension just-in-time (JIT) from string sources.</p>
<p>This function behaves exactly like <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">load()</span></code></a>, but takes its sources as
strings rather than filenames. These strings are stored to files in the
build directory, after which the behavior of <a class="reference internal" href="#torch.utils.cpp_extension.load_inline" title="torch.utils.cpp_extension.load_inline"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_inline()</span></code></a> is
identical to <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">load()</span></code></a>.</p>
<p>See <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/test/test_cpp_extensions.py">the
tests</a>
for good examples of using this function.</p>
<p>Sources may omit two required parts of a typical non-inline C++ extension:
the necessary header includes, as well as the (pybind11) binding code. More
precisely, strings passed to <code class="docutils literal notranslate"><span class="pre">cpp_sources</span></code> are first concatenated into a
single <code class="docutils literal notranslate"><span class="pre">.cpp</span></code> file. This file is then prepended with <code class="docutils literal notranslate"><span class="pre">#include</span>
<span class="pre">&lt;torch/extension.h&gt;</span></code>.</p>
<p>Furthermore, if the <code class="docutils literal notranslate"><span class="pre">functions</span></code> argument is supplied, bindings will be
automatically generated for each function specified. <code class="docutils literal notranslate"><span class="pre">functions</span></code> can
either be a list of function names, or a dictionary mapping from function
names to docstrings. If a list is given, the name of each function is used
as its docstring.</p>
<p>The sources in <code class="docutils literal notranslate"><span class="pre">cuda_sources</span></code> are concatenated into a separate <code class="docutils literal notranslate"><span class="pre">.cu</span></code>
file and  prepended with <code class="docutils literal notranslate"><span class="pre">torch/types.h</span></code>, <code class="docutils literal notranslate"><span class="pre">cuda.h</span></code> and
<code class="docutils literal notranslate"><span class="pre">cuda_runtime.h</span></code> includes. The <code class="docutils literal notranslate"><span class="pre">.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">.cu</span></code> files are compiled
separately, but ultimately linked into a single library. Note that no
bindings are generated for functions in <code class="docutils literal notranslate"><span class="pre">cuda_sources</span></code> per  se. To bind
to a CUDA kernel, you must create a C++ function that calls it, and either
declare or define this C++ function in one of the <code class="docutils literal notranslate"><span class="pre">cpp_sources</span></code> (and
include its name in <code class="docutils literal notranslate"><span class="pre">functions</span></code>).</p>
<p>See <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code class="xref py py-func docutils literal notranslate"><span class="pre">load()</span></code></a> for a description of arguments omitted below.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cpp_sources</strong> – A string, or list of strings, containing C++ source code.</p></li>
<li><p><strong>cuda_sources</strong> – A string, or list of strings, containing CUDA source code.</p></li>
<li><p><strong>functions</strong> – A list of function names for which to generate function
bindings. If a dictionary is given, it should map function names to
docstrings (which are otherwise just the function names).</p></li>
<li><p><strong>with_cuda</strong> – Determines whether CUDA headers and libraries are added to
the build. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), this value is
automatically determined based on whether <code class="docutils literal notranslate"><span class="pre">cuda_sources</span></code> is
provided. Set it to <cite>True`</cite> to force CUDA headers
and libraries to be included.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.utils.cpp_extension</span> <span class="k">import</span> <span class="n">load_inline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">source</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="go">at::Tensor sin_add(at::Tensor x, at::Tensor y) {</span>
<span class="go">  return x.sin() + y.sin();</span>
<span class="go">}</span>
<span class="go">&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">load_inline</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;inline_extension&#39;</span><span class="p">,</span>
<span class="go">                         cpp_sources=[source],</span>
<span class="go">                         functions=[&#39;sin_add&#39;])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.utils.cpp_extension.include_paths">
<code class="descclassname">torch.utils.cpp_extension.</code><code class="descname">include_paths</code><span class="sig-paren">(</span><em>cuda=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.cpp_extension.include_paths" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the include paths required to build a C++ or CUDA extension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cuda</strong> – If <cite>True</cite>, includes CUDA-specific include paths.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of include path strings.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.utils.cpp_extension.check_compiler_abi_compatibility">
<code class="descclassname">torch.utils.cpp_extension.</code><code class="descname">check_compiler_abi_compatibility</code><span class="sig-paren">(</span><em>compiler</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.cpp_extension.check_compiler_abi_compatibility" title="Permalink to this definition">¶</a></dt>
<dd><p>Verifies that the given compiler is ABI-compatible with PyTorch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>compiler</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – The compiler executable name to check (e.g. <code class="docutils literal notranslate"><span class="pre">g++</span></code>).
Must be executable in a shell process.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>False if the compiler is (likely) ABI-incompatible with PyTorch,
else True.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.utils.cpp_extension.verify_ninja_availability">
<code class="descclassname">torch.utils.cpp_extension.</code><code class="descname">verify_ninja_availability</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.cpp_extension.verify_ninja_availability" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if the <a class="reference external" href="https://ninja-build.org/">ninja</a> build system is
available on the system.</p>
</dd></dl>

</div>
<span id="document-data"></span><div class="section" id="module-torch.utils.data">
<span id="torch-utils-data"></span><h2>torch.utils.data<a class="headerlink" href="#module-torch.utils.data" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch.utils.data.Dataset">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">Dataset</code><a class="headerlink" href="#torch.utils.data.Dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>An abstract class representing a Dataset.</p>
<p>All other datasets should subclass it. All subclasses should override
<code class="docutils literal notranslate"><span class="pre">__len__</span></code>, that provides the size of the dataset, and <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>,
supporting integer indexing in range from 0 to len(self) exclusive.</p>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.TensorDataset">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">TensorDataset</code><span class="sig-paren">(</span><em>*tensors</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.TensorDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Dataset wrapping tensors.</p>
<p>Each sample will be retrieved by indexing tensors along the first dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>*tensors</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensors that have the same size of the first dimension.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.ConcatDataset">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">ConcatDataset</code><span class="sig-paren">(</span><em>datasets</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.ConcatDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Dataset to concatenate multiple datasets.
Purpose: useful to assemble different existing datasets, possibly
large-scale datasets as the concatenation operation is done in an
on-the-fly manner.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>datasets</strong> (<em>sequence</em>) – List of datasets to be concatenated</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.Subset">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">Subset</code><span class="sig-paren">(</span><em>dataset</em>, <em>indices</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.Subset" title="Permalink to this definition">¶</a></dt>
<dd><p>Subset of a dataset at specified indices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) – The whole Dataset</p></li>
<li><p><strong>indices</strong> (<em>sequence</em>) – Indices in the whole set selected for subset</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.DataLoader">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">DataLoader</code><span class="sig-paren">(</span><em>dataset</em>, <em>batch_size=1</em>, <em>shuffle=False</em>, <em>sampler=None</em>, <em>batch_sampler=None</em>, <em>num_workers=0</em>, <em>collate_fn=&lt;function default_collate&gt;</em>, <em>pin_memory=False</em>, <em>drop_last=False</em>, <em>timeout=0</em>, <em>worker_init_fn=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.DataLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Data loader. Combines a dataset and a sampler, and provides
single- or multi-process iterators over the dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) – dataset from which to load the data.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – how many samples per batch to load
(default: <code class="docutils literal notranslate"><span class="pre">1</span></code>).</p></li>
<li><p><strong>shuffle</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to have the data reshuffled
at every epoch (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p></li>
<li><p><strong>sampler</strong> (<a class="reference internal" href="index.html#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><em>Sampler</em></a><em>, </em><em>optional</em>) – defines the strategy to draw samples from
the dataset. If specified, <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> must be False.</p></li>
<li><p><strong>batch_sampler</strong> (<a class="reference internal" href="index.html#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><em>Sampler</em></a><em>, </em><em>optional</em>) – like sampler, but returns a batch of
indices at a time. Mutually exclusive with <code class="xref py py-attr docutils literal notranslate"><span class="pre">batch_size</span></code>,
<code class="xref py py-attr docutils literal notranslate"><span class="pre">shuffle</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code>, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">drop_last</span></code>.</p></li>
<li><p><strong>num_workers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – how many subprocesses to use for data
loading. 0 means that the data will be loaded in the main process.
(default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p></li>
<li><p><strong>collate_fn</strong> (<em>callable</em><em>, </em><em>optional</em>) – merges a list of samples to form a mini-batch.</p></li>
<li><p><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data loader will copy tensors
into CUDA pinned memory before returning them.  If your data elements
are a custom type, or your <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code> returns a batch that is a custom type
see the example below.</p></li>
<li><p><strong>drop_last</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to drop the last incomplete batch,
if the dataset size is not divisible by the batch size. If <code class="docutils literal notranslate"><span class="pre">False</span></code> and
the size of dataset is not divisible by the batch size, then the last batch
will be smaller. (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>timeout</strong> (<em>numeric</em><em>, </em><em>optional</em>) – if positive, the timeout value for collecting a batch
from workers. Should always be non-negative. (default: <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p></li>
<li><p><strong>worker_init_fn</strong> (<em>callable</em><em>, </em><em>optional</em>) – If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, this will be called on each
worker subprocess with the worker id (an int in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">num_workers</span> <span class="pre">-</span> <span class="pre">1]</span></code>) as
input, after seeding and before data loading. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, each worker will have its PyTorch seed set to
<code class="docutils literal notranslate"><span class="pre">base_seed</span> <span class="pre">+</span> <span class="pre">worker_id</span></code>, where <code class="docutils literal notranslate"><span class="pre">base_seed</span></code> is a long generated
by main process using its RNG. However, seeds for other libraies
may be duplicated upon initializing workers (w.g., NumPy), causing
each worker to return identical random numbers. (See
<span class="xref std std-ref">dataloader-workers-random-seed</span> section in FAQ.) You may
use <a class="reference internal" href="index.html#torch.initial_seed" title="torch.initial_seed"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.initial_seed()</span></code></a> to access the PyTorch seed for
each worker in <code class="xref py py-attr docutils literal notranslate"><span class="pre">worker_init_fn</span></code>, and use it to set other
seeds before data loading.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If <code class="docutils literal notranslate"><span class="pre">spawn</span></code> start method is used, <code class="xref py py-attr docutils literal notranslate"><span class="pre">worker_init_fn</span></code> cannot be an
unpicklable object, e.g., a lambda function.</p>
</div>
<p>The default memory pinning logic only recognizes Tensors and maps and iterables
containg Tensors.  By default, if the pinning logic sees a batch that is a custom type
(which will occur if you have a <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code> that returns a custom batch type),
or if each element of your batch is a custom type, the pinning logic will not
recognize them, and it will return that batch (or those elements)
without pinning the memory.  To enable memory pinning for custom batch or data types,
define a <code class="docutils literal notranslate"><span class="pre">pin_memory</span></code> method on your custom type(s).</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleCustomBatch</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">transposed_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">data</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">transposed_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">transposed_data</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pin_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inp</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

<span class="k">def</span> <span class="nf">collate_wrapper</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">SimpleCustomBatch</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">tgts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">inps</span><span class="p">,</span> <span class="n">tgts</span><span class="p">)</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_wrapper</span><span class="p">,</span>
                    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">batch_ndx</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">inp</span><span class="o">.</span><span class="n">is_pinned</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">is_pinned</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.utils.data.random_split">
<code class="descclassname">torch.utils.data.</code><code class="descname">random_split</code><span class="sig-paren">(</span><em>dataset</em>, <em>lengths</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.random_split" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly split a dataset into non-overlapping new datasets of given lengths.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<a class="reference internal" href="index.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) – Dataset to be split</p></li>
<li><p><strong>lengths</strong> (<em>sequence</em>) – lengths of splits to be produced</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.Sampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">Sampler</code><span class="sig-paren">(</span><em>data_source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.Sampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all Samplers.</p>
<p>Every Sampler subclass has to provide an __iter__ method, providing a way
to iterate over indices of dataset elements, and a __len__ method that
returns the length of the returned iterators.</p>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.SequentialSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">SequentialSampler</code><span class="sig-paren">(</span><em>data_source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.SequentialSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements sequentially, always in the same order.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data_source</strong> (<a class="reference internal" href="index.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) – dataset to sample from</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.RandomSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">RandomSampler</code><span class="sig-paren">(</span><em>data_source</em>, <em>replacement=False</em>, <em>num_samples=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.RandomSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements randomly. If without replacement, then sample from a shuffled dataset.
If with replacement, then user can specify <code class="docutils literal notranslate"><span class="pre">num_samples</span></code> to draw.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_source</strong> (<a class="reference internal" href="index.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) – dataset to sample from</p></li>
<li><p><strong>num_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of samples to draw, default=len(dataset)</p></li>
<li><p><strong>replacement</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – samples are drawn with replacement if <code class="docutils literal notranslate"><span class="pre">True</span></code>, default=False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.SubsetRandomSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">SubsetRandomSampler</code><span class="sig-paren">(</span><em>indices</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.SubsetRandomSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements randomly from a given list of indices, without replacement.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>indices</strong> (<em>sequence</em>) – a sequence of indices</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.WeightedRandomSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">WeightedRandomSampler</code><span class="sig-paren">(</span><em>weights</em>, <em>num_samples</em>, <em>replacement=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.WeightedRandomSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements from [0,..,len(weights)-1] with given probabilities (weights).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weights</strong> (<em>sequence</em>) – a sequence of weights, not necessary summing up to one</p></li>
<li><p><strong>num_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – number of samples to draw</p></li>
<li><p><strong>replacement</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, samples are drawn with replacement.
If not, they are drawn without replacement, which means that when a
sample index is drawn for a row, it cannot be drawn again for that row.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">WeightedRandomSampler</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="mi">5</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">[0, 0, 0, 1, 0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">WeightedRandomSampler</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="mi">5</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="go">[0, 1, 4, 3, 2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.BatchSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">BatchSampler</code><span class="sig-paren">(</span><em>sampler</em>, <em>batch_size</em>, <em>drop_last</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.BatchSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps another sampler to yield a mini-batch of indices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sampler</strong> (<a class="reference internal" href="index.html#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><em>Sampler</em></a>) – Base sampler.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Size of mini-batch.</p></li>
<li><p><strong>drop_last</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the sampler will drop the last batch if
its size would be less than <code class="docutils literal notranslate"><span class="pre">batch_size</span></code></p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">BatchSampler</span><span class="p">(</span><span class="n">SequentialSampler</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="go">[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">BatchSampler</span><span class="p">(</span><span class="n">SequentialSampler</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">[[0, 1, 2], [3, 4, 5], [6, 7, 8]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.distributed.DistributedSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.distributed.</code><code class="descname">DistributedSampler</code><span class="sig-paren">(</span><em>dataset</em>, <em>num_replicas=None</em>, <em>rank=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.distributed.DistributedSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Sampler that restricts data loading to a subset of the dataset.</p>
<p>It is especially useful in conjunction with
<a class="reference internal" href="index.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code></a>. In such case, each
process can pass a DistributedSampler instance as a DataLoader sampler,
and load a subset of the original dataset that is exclusive to it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Dataset is assumed to be of constant size.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – Dataset used for sampling.</p></li>
<li><p><strong>num_replicas</strong> (<em>optional</em>) – Number of processes participating in
distributed training.</p></li>
<li><p><strong>rank</strong> (<em>optional</em>) – Rank of the current process within num_replicas.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<span id="document-dlpack"></span><div class="section" id="torch-utils-dlpack">
<h2>torch.utils.dlpack<a class="headerlink" href="#torch-utils-dlpack" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.utils.dlpack.from_dlpack">
<code class="descclassname">torch.utils.dlpack.</code><code class="descname">from_dlpack</code><span class="sig-paren">(</span><em>dlpack</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.utils.dlpack.from_dlpack" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes a DLPack to a tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dlpack</strong> – a PyCapsule object with the dltensor</p>
</dd>
</dl>
<p>The tensor will share the memory with the object represented
in the dlpack.
Note that each dlpack can only be consumed once.</p>
</dd></dl>

<dl class="function">
<dt id="torch.utils.dlpack.to_dlpack">
<code class="descclassname">torch.utils.dlpack.</code><code class="descname">to_dlpack</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &#x2192; PyCapsule<a class="headerlink" href="#torch.utils.dlpack.to_dlpack" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a DLPack representing the tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – a tensor to be exported</p>
</dd>
</dl>
<p>The dlpack shares the tensors memory.
Note that each dlpack can only be consumed once.</p>
</dd></dl>

</div>
<span id="document-hub"></span><div class="section" id="torch-hub">
<h2>torch.hub<a class="headerlink" href="#torch-hub" title="Permalink to this headline">¶</a></h2>
<p>Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.</p>
<div class="section" id="publishing-models">
<h3>Publishing models<a class="headerlink" href="#publishing-models" title="Permalink to this headline">¶</a></h3>
<p>Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)
to a github repository by adding a simple <code class="docutils literal notranslate"><span class="pre">hubconf.py</span></code> file;</p>
<p><code class="docutils literal notranslate"><span class="pre">hubconf.py</span></code> can have multiple entrypoints. Each entrypoint is defined as a python function with
the following signature.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">entrypoint_name</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="section" id="how-to-implement-an-entrypoint">
<h4>How to implement an entrypoint?<a class="headerlink" href="#how-to-implement-an-entrypoint" title="Permalink to this headline">¶</a></h4>
<p>Here is a code snippet from pytorch/vision repository, which specifies an entrypoint
for <code class="docutils literal notranslate"><span class="pre">resnet18</span></code> model. You can see a full script in
<a class="reference external" href="https://github.com/pytorch/vision/blob/master/hubconf.py">pytorch/vision repo</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dependencies</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;torch&#39;</span><span class="p">,</span> <span class="s1">&#39;math&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resnet18 model</span>
<span class="sd">    pretrained (bool): a recommended kwargs for all entrypoints</span>
<span class="sd">    args &amp; kwargs are arguments for the function</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">######## Call the model in the repo ###############</span>
    <span class="kn">from</span> <span class="nn">torchvision.models.resnet</span> <span class="k">import</span> <span class="n">resnet18</span> <span class="k">as</span> <span class="n">_resnet18</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">_resnet18</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1">######## End of call ##############################</span>
    <span class="c1"># The following logic is REQUIRED</span>
    <span class="k">if</span> <span class="n">pretrained</span><span class="p">:</span>
        <span class="c1"># For weights saved in local repo</span>
        <span class="c1"># model.load_state_dict(&lt;path_to_saved_file&gt;)</span>

        <span class="c1"># For weights saved elsewhere</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="s1">&#39;https://download.pytorch.org/models/resnet18-5c106cde.pth&#39;</span>
        <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">load_url</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dependencies</span></code> variable is a <strong>list</strong> of package names required to to run the model.</p></li>
<li><p>Pretrained weights can either be stored local in the github repo, or loadable by
<code class="docutils literal notranslate"><span class="pre">model_zoo.load()</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pretrained</span></code> controls whether to load the pre-trained weights provided by repo owners.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">args</span></code> and <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> are passed along to the real callable function.</p></li>
<li><p>Docstring of the function works as a help message, explaining what does the model do and what
are the allowed arguments.</p></li>
<li><p>Entrypoint function should <strong>ALWAYS</strong> return a model(nn.module).</p></li>
</ul>
</div>
<div class="section" id="important-notice">
<h4>Important Notice<a class="headerlink" href="#important-notice" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>The published models should be at least in a branch/tag. It can’t be a random commit.</p></li>
</ul>
</div>
</div>
<div class="section" id="loading-models-from-hub">
<h3>Loading models from Hub<a class="headerlink" href="#loading-models-from-hub" title="Permalink to this headline">¶</a></h3>
<p>Users can load the pre-trained models using <code class="docutils literal notranslate"><span class="pre">torch.hub.load()</span></code> API.</p>
<span class="target" id="module-torch.hub"></span><dl class="function">
<dt id="torch.hub.load">
<code class="descclassname">torch.hub.</code><code class="descname">load</code><span class="sig-paren">(</span><em>github</em>, <em>model</em>, <em>force_reload=False</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.hub.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a model from a github repo, with pretrained weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>github</strong> – Required, a string with format “repo_owner/repo_name[:tag_name]” with an optional
tag/branch. The default branch is <cite>master</cite> if not specified.
Example: ‘pytorch/vision[:hub]’</p></li>
<li><p><strong>model</strong> – Required, a string of entrypoint name defined in repo’s hubconf.py</p></li>
<li><p><strong>force_reload</strong> – Optional, whether to discard the existing cache and force a fresh download.
Default is <cite>False</cite>.</p></li>
<li><p><strong>*args</strong> – Optional, the corresponding args for callable <cite>model</cite>.</p></li>
<li><p><strong>**kwargs</strong> – Optional, the corresponding kwargs for callable <cite>model</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a single model with corresponding pretrained weights.</p>
</dd>
</dl>
</dd></dl>

<p>Here’s an example loading <code class="docutils literal notranslate"><span class="pre">resnet18</span></code> entrypoint from <code class="docutils literal notranslate"><span class="pre">pytorch/vision</span></code> repo.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hub_model</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
    <span class="s1">&#39;pytorch/vision:master&#39;</span><span class="p">,</span> <span class="c1"># repo_owner/repo_name:branch</span>
    <span class="s1">&#39;resnet18&#39;</span><span class="p">,</span> <span class="c1"># entrypoint</span>
    <span class="mi">1234</span><span class="p">,</span> <span class="c1"># args for callable [not applicable to resnet]</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># kwargs for callable</span>
</pre></div>
</div>
<div class="section" id="where-are-my-downloaded-model-weights-saved">
<h4>Where are my downloaded model &amp; weights saved?<a class="headerlink" href="#where-are-my-downloaded-model-weights-saved" title="Permalink to this headline">¶</a></h4>
<p>The locations are used in the order of</p>
<ul class="simple">
<li><p>hub_dir: user specified path. It can be set in the following ways:
- Setting the environment variable <code class="docutils literal notranslate"><span class="pre">TORCH_HUB_DIR</span></code>
- Calling <code class="docutils literal notranslate"><span class="pre">hub.set_dir(&lt;PATH_TO_HUB_DIR&gt;)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">~/.torch/hub</span></code></p></li>
</ul>
<dl class="function">
<dt id="torch.hub.set_dir">
<code class="descclassname">torch.hub.</code><code class="descname">set_dir</code><span class="sig-paren">(</span><em>d</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.hub.set_dir" title="Permalink to this definition">¶</a></dt>
<dd><p>Optionally set hub_dir to a local dir to save downloaded models &amp; weights.</p>
<p>If this argument is not set, env variable <cite>TORCH_HUB_DIR</cite> will be searched first,
<cite>~/.torch/hub</cite> will be created and used as fallback.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>d</strong> – path to a local folder to save downloaded models &amp; weights.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="caching-logic">
<h4>Caching logic<a class="headerlink" href="#caching-logic" title="Permalink to this headline">¶</a></h4>
<p>By default, we don’t clean up files after loading it. Hub uses the cache by default if it already exists in <code class="docutils literal notranslate"><span class="pre">hub_dir</span></code>.</p>
<p>Users can force a reload by calling <code class="docutils literal notranslate"><span class="pre">hub.load(...,</span> <span class="pre">force_reload=True)</span></code>. This will delete
the existing github folder and downloaded weights, reinitialize a fresh download. This is useful
when updates are published to the same branch, users can keep up with the latest release.</p>
</div>
</div>
</div>
<span id="document-model_zoo"></span><div class="section" id="module-torch.utils.model_zoo">
<span id="torch-utils-model-zoo"></span><h2>torch.utils.model_zoo<a class="headerlink" href="#module-torch.utils.model_zoo" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.utils.model_zoo.load_url">
<code class="descclassname">torch.utils.model_zoo.</code><code class="descname">load_url</code><span class="sig-paren">(</span><em>url</em>, <em>model_dir=None</em>, <em>map_location=None</em>, <em>progress=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.model_zoo.load_url" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the Torch serialized object at the given URL.</p>
<p>If the object is already present in <cite>model_dir</cite>, it’s deserialized and
returned. The filename part of the URL should follow the naming convention
<code class="docutils literal notranslate"><span class="pre">filename-&lt;sha256&gt;.ext</span></code> where <code class="docutils literal notranslate"><span class="pre">&lt;sha256&gt;</span></code> is the first eight or more
digits of the SHA256 hash of the contents of the file. The hash is used to
ensure unique names and to verify the contents of the file.</p>
<p>The default value of <cite>model_dir</cite> is <code class="docutils literal notranslate"><span class="pre">$TORCH_HOME/models</span></code> where
<code class="docutils literal notranslate"><span class="pre">$TORCH_HOME</span></code> defaults to <code class="docutils literal notranslate"><span class="pre">~/.torch</span></code>. The default directory can be
overridden with the <code class="docutils literal notranslate"><span class="pre">$TORCH_MODEL_ZOO</span></code> environment variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> (<em>string</em>) – URL of the object to download</p></li>
<li><p><strong>model_dir</strong> (<em>string</em><em>, </em><em>optional</em>) – directory in which to save the object</p></li>
<li><p><strong>map_location</strong> (<em>optional</em>) – a function or a dict specifying how to remap storage locations (see torch.load)</p></li>
<li><p><strong>progress</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – whether or not to display a progress bar to stderr</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">load_url</span><span class="p">(</span><span class="s1">&#39;https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<span id="document-onnx"></span><div class="section" id="module-torch.onnx">
<span id="torch-onnx"></span><h2>torch.onnx<a class="headerlink" href="#module-torch.onnx" title="Permalink to this headline">¶</a></h2>
<div class="section" id="example-end-to-end-alexnet-from-pytorch-to-caffe2">
<h3>Example: End-to-end AlexNet from PyTorch to Caffe2<a class="headerlink" href="#example-end-to-end-alexnet-from-pytorch-to-caffe2" title="Permalink to this headline">¶</a></h3>
<p>Here is a simple script which exports a pretrained AlexNet as defined in
torchvision into ONNX.  It runs a single round of inference and then
saves the resulting traced model to <code class="docutils literal notranslate"><span class="pre">alexnet.onnx</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>

<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">alexnet</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># Providing input and output names sets the display names for values</span>
<span class="c1"># within the model&#39;s graph. Setting these does not change the semantics</span>
<span class="c1"># of the graph; it is only for readability.</span>
<span class="c1">#</span>
<span class="c1"># The inputs to the network consist of the flat list of inputs (i.e.</span>
<span class="c1"># the values you would pass to the forward() method) followed by the</span>
<span class="c1"># flat list of parameters. You can partially specify names, i.e. provide</span>
<span class="c1"># a list here shorter than the number of inputs to the model, and we will</span>
<span class="c1"># only set that subset of names, starting from the beginning.</span>
<span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span> <span class="s2">&quot;actual_input_1&quot;</span> <span class="p">]</span> <span class="o">+</span> <span class="p">[</span> <span class="s2">&quot;learned_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span> <span class="p">]</span>
<span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span> <span class="s2">&quot;output1&quot;</span> <span class="p">]</span>

<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="s2">&quot;alexnet.onnx&quot;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">)</span>
</pre></div>
</div>
<p>The resulting <code class="docutils literal notranslate"><span class="pre">alexnet.onnx</span></code> is a binary protobuf file which contains both
the network structure and parameters of the model you exported
(in this case, AlexNet).  The keyword argument <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code> causes the
exporter to print out a human-readable representation of the network:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># These are the inputs and parameters to the network, which have taken on</span>
<span class="c1"># the names we specified earlier.</span>
<span class="n">graph</span><span class="p">(</span><span class="o">%</span><span class="n">actual_input_1</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
      <span class="o">%</span><span class="n">learned_0</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
      <span class="o">%</span><span class="n">learned_1</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
      <span class="o">%</span><span class="n">learned_2</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
      <span class="o">%</span><span class="n">learned_3</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">192</span><span class="p">)</span>
      <span class="c1"># ---- omitted for brevity ----</span>
      <span class="o">%</span><span class="n">learned_14</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>
      <span class="o">%</span><span class="n">learned_15</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span> <span class="p">{</span>
  <span class="c1"># Every statement consists of some output tensors (and their types),</span>
  <span class="c1"># the operator to be run (with its attributes, e.g., kernels, strides,</span>
  <span class="c1"># etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)</span>
  <span class="o">%</span><span class="mi">17</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">55</span><span class="p">)</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">Conv</span><span class="p">[</span><span class="n">dilations</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span> <span class="n">pads</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]](</span><span class="o">%</span><span class="n">actual_input_1</span><span class="p">,</span> <span class="o">%</span><span class="n">learned_0</span><span class="p">,</span> <span class="o">%</span><span class="n">learned_1</span><span class="p">),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span><span class="o">/</span><span class="n">Sequential</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">/</span><span class="n">Conv2d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="o">%</span><span class="mi">18</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">55</span><span class="p">)</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">Relu</span><span class="p">(</span><span class="o">%</span><span class="mi">17</span><span class="p">),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span><span class="o">/</span><span class="n">Sequential</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">/</span><span class="n">ReLU</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="o">%</span><span class="mi">19</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">27</span><span class="p">)</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">MaxPool</span><span class="p">[</span><span class="n">kernel_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">pads</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]](</span><span class="o">%</span><span class="mi">18</span><span class="p">),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span><span class="o">/</span><span class="n">Sequential</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">/</span><span class="n">MaxPool2d</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
  <span class="c1"># ---- omitted for brevity ----</span>
  <span class="o">%</span><span class="mi">29</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">MaxPool</span><span class="p">[</span><span class="n">kernel_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">pads</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]](</span><span class="o">%</span><span class="mi">28</span><span class="p">),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span><span class="o">/</span><span class="n">Sequential</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">/</span><span class="n">MaxPool2d</span><span class="p">[</span><span class="mi">12</span><span class="p">]</span>
  <span class="c1"># Dynamic means that the shape is not known. This may be because of a</span>
  <span class="c1"># limitation of our implementation (which we would like to fix in a</span>
  <span class="c1"># future release) or shapes which are truly dynamic.</span>
  <span class="o">%</span><span class="mi">30</span> <span class="p">:</span> <span class="n">Dynamic</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">Shape</span><span class="p">(</span><span class="o">%</span><span class="mi">29</span><span class="p">),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span>
  <span class="o">%</span><span class="mi">31</span> <span class="p">:</span> <span class="n">Dynamic</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">Slice</span><span class="p">[</span><span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ends</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">starts</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]](</span><span class="o">%</span><span class="mi">30</span><span class="p">),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span>
  <span class="o">%</span><span class="mi">32</span> <span class="p">:</span> <span class="n">Long</span><span class="p">()</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">Squeeze</span><span class="p">[</span><span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]](</span><span class="o">%</span><span class="mi">31</span><span class="p">),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span>
  <span class="o">%</span><span class="mi">33</span> <span class="p">:</span> <span class="n">Long</span><span class="p">()</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">Constant</span><span class="p">[</span><span class="n">value</span><span class="o">=</span><span class="p">{</span><span class="mi">9216</span><span class="p">}](),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span>
  <span class="c1"># ---- omitted for brevity ----</span>
  <span class="o">%</span><span class="n">output1</span> <span class="p">:</span> <span class="n">Float</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span> <span class="o">=</span> <span class="n">onnx</span><span class="p">::</span><span class="n">Gemm</span><span class="p">[</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">broadcast</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">transB</span><span class="o">=</span><span class="mi">1</span><span class="p">](</span><span class="o">%</span><span class="mi">45</span><span class="p">,</span> <span class="o">%</span><span class="n">learned_14</span><span class="p">,</span> <span class="o">%</span><span class="n">learned_15</span><span class="p">),</span> <span class="n">scope</span><span class="p">:</span> <span class="n">AlexNet</span><span class="o">/</span><span class="n">Sequential</span><span class="p">[</span><span class="n">classifier</span><span class="p">]</span><span class="o">/</span><span class="n">Linear</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>
  <span class="k">return</span> <span class="p">(</span><span class="o">%</span><span class="n">output1</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>You can also verify the protobuf using the <a class="reference external" href="https://github.com/onnx/onnx/">onnx</a> library.
You can install <code class="docutils literal notranslate"><span class="pre">onnx</span></code> with conda:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">onnx</span>
</pre></div>
</div>
<p>Then, you can run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>

<span class="c1"># Load the ONNX model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;alexnet.onnx&quot;</span><span class="p">)</span>

<span class="c1"># Check that the IR is well formed</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Print a human readable representation of the graph</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">printable_graph</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<p>To run the exported script with <a class="reference external" href="https://caffe2.ai/">caffe2</a>, you will need to install <cite>caffe2</cite>: If you don’t have one already, Please <a class="reference external" href="https://caffe2.ai/docs/getting-started.html">follow the install instructions</a>.</p>
<p>Once these are installed, you can use the backend for Caffe2:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ...continuing from above</span>
<span class="kn">import</span> <span class="nn">caffe2.python.onnx.backend</span> <span class="k">as</span> <span class="nn">backend</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">rep</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;CUDA:0&quot;</span><span class="p">)</span> <span class="c1"># or &quot;CPU&quot;</span>
<span class="c1"># For the Caffe2 backend:</span>
<span class="c1">#     rep.predict_net is the Caffe2 protobuf for the network</span>
<span class="c1">#     rep.workspace is the Caffe2 workspace for the network</span>
<span class="c1">#       (see the class caffe2.python.onnx.backend.Workspace)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">rep</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="c1"># To run networks with more than one input, pass a tuple</span>
<span class="c1"># rather than a single numpy ndarray.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>In the future, there will be backends for other frameworks as well.</p>
</div>
<div class="section" id="limitations">
<h3>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The ONNX exporter is a <em>trace-based</em> exporter, which means that it
operates by executing your model once, and exporting the operators which
were actually run during this run.  This means that if your model is
dynamic, e.g., changes behavior depending on input data, the export
won’t be accurate.  Similarly, a trace is likely to be valid only
for a specific input size (which is one reason why we require explicit inputs
on tracing.)  We recommend examining the model trace and making sure
the traced operators look reasonable.</p></li>
<li><p>PyTorch and Caffe2 often have implementations of operators with some
numeric differences.  Depending on model structure, these differences
may be negligible, but they can also cause major divergences in behavior
(especially on untrained models.)  In a future release, we plan to
allow Caffe2 to call directly to Torch implementations of operators, to
help you smooth over these differences when precision is important,
and to also document these differences.</p></li>
</ul>
</div>
<div class="section" id="supported-operators">
<h3>Supported operators<a class="headerlink" href="#supported-operators" title="Permalink to this headline">¶</a></h3>
<p>The following operators are supported:</p>
<ul class="simple">
<li><p>add (nonzero alpha not supported)</p></li>
<li><p>sub (nonzero alpha not supported)</p></li>
<li><p>mul</p></li>
<li><p>div</p></li>
<li><p>cat</p></li>
<li><p>mm</p></li>
<li><p>addmm</p></li>
<li><p>neg</p></li>
<li><p>sqrt</p></li>
<li><p>tanh</p></li>
<li><p>sigmoid</p></li>
<li><p>mean</p></li>
<li><p>sum</p></li>
<li><p>prod</p></li>
<li><p>t</p></li>
<li><p>expand (only when used before a broadcasting ONNX operator; e.g., add)</p></li>
<li><p>transpose</p></li>
<li><p>view</p></li>
<li><p>split</p></li>
<li><p>squeeze</p></li>
<li><p>prelu (single weight shared among input channels not supported)</p></li>
<li><p>threshold (non-zero threshold/non-zero value not supported)</p></li>
<li><p>leaky_relu</p></li>
<li><p>glu</p></li>
<li><p>softmax (only dim=-1 supported)</p></li>
<li><p>avg_pool2d (ceil_mode not supported)</p></li>
<li><p>log_softmax</p></li>
<li><p>unfold (experimental support with ATen-Caffe2 integration)</p></li>
<li><p>elu</p></li>
<li><p>concat</p></li>
<li><p>abs</p></li>
<li><p>index_select</p></li>
<li><p>pow</p></li>
<li><p>clamp</p></li>
<li><p>max</p></li>
<li><p>min</p></li>
<li><p>eq</p></li>
<li><p>gt</p></li>
<li><p>lt</p></li>
<li><p>ge</p></li>
<li><p>le</p></li>
<li><p>exp</p></li>
<li><p>sin</p></li>
<li><p>cos</p></li>
<li><p>tan</p></li>
<li><p>asin</p></li>
<li><p>acos</p></li>
<li><p>atan</p></li>
<li><p>permute</p></li>
<li><p>Conv</p></li>
<li><p>BatchNorm</p></li>
<li><p>MaxPool1d (ceil_mode not supported)</p></li>
<li><p>MaxPool2d (ceil_mode not supported)</p></li>
<li><p>MaxPool3d (ceil_mode not supported)</p></li>
<li><p>Embedding (no optional arguments supported)</p></li>
<li><p>RNN</p></li>
<li><p>ConstantPadNd</p></li>
<li><p>Dropout</p></li>
<li><p>FeatureDropout (training mode not supported)</p></li>
<li><p>Index (constant integer and tuple indices supported)</p></li>
</ul>
<p>The operator set above is sufficient to export the following models:</p>
<ul class="simple">
<li><p>AlexNet</p></li>
<li><p>DCGAN</p></li>
<li><p>DenseNet</p></li>
<li><p>Inception (warning: this model is highly sensitive to changes in operator
implementation)</p></li>
<li><p>ResNet</p></li>
<li><p>SuperResolution</p></li>
<li><p>VGG</p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/examples/tree/master/word_language_model">word_language_model</a></p></li>
</ul>
<p>Adding export support for operators is an <em>advance usage</em>.
To achieve this, developers need to touch the source code of PyTorch.
Please follow the <a class="reference external" href="https://github.com/pytorch/pytorch#from-source">instructions</a>
for installing PyTorch from source.
If the wanted operator is standardized in ONNX, it should be easy to add
support for exporting such operator (adding a symbolic function for the operator).
To confirm whether the operator is standardized or not, please check the
<a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md">ONNX operator list</a>.</p>
<p>If the operator is an ATen operator, which means you can find the declaration
of the function in <code class="docutils literal notranslate"><span class="pre">torch/csrc/autograd/generated/VariableType.h</span></code>
(available in generated code in PyTorch install dir), you should add the symbolic
function in <code class="docutils literal notranslate"><span class="pre">torch/onnx/symbolic.py</span></code> and follow the instructions listed as below:</p>
<ul class="simple">
<li><p>Define the symbolic function in
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic.py">torch/onnx/symbolic.py</a>.
Make sure the function has the same name as the ATen operator/function
defined in <code class="docutils literal notranslate"><span class="pre">VariableType.h</span></code>.</p></li>
<li><p>The first parameter is always the exported ONNX graph.
Parameter names must EXACTLY match the names in <code class="docutils literal notranslate"><span class="pre">VariableType.h</span></code>,
because dispatch is done with keyword arguments.</p></li>
<li><p>Parameter ordering does NOT necessarily match what is in <code class="docutils literal notranslate"><span class="pre">VariableType.h</span></code>,
tensors (inputs) are always first, then non-tensor arguments.</p></li>
<li><p>In the symbolic function, if the operator is already standardized in ONNX,
we only need to create a node to represent the ONNX operator in the graph.</p></li>
<li><p>If the input argument is a tensor, but ONNX asks for a scalar, we have to
explicitly do the conversion. The helper function <code class="docutils literal notranslate"><span class="pre">_scalar</span></code> can convert a
scalar tensor into a python scalar, and <code class="docutils literal notranslate"><span class="pre">_if_scalar_type_as</span></code> can turn a
Python scalar into a PyTorch tensor.</p></li>
</ul>
<p>If the operator is a non-ATen operator, the symbolic function has to be
added in the corresponding PyTorch Function class. Please read the following
instructions:</p>
<ul class="simple">
<li><p>Create a symbolic function named <code class="docutils literal notranslate"><span class="pre">symbolic</span></code> in the corresponding Function class.</p></li>
<li><p>The first parameter is always the exported ONNX graph.</p></li>
<li><p>Parameter names except the first must EXACTLY match the names in <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>The output tuple size must match the outputs of <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>In the symbolic function, if the operator is already standardized in ONNX,
we just need to create a node to represent the ONNX operator in the graph.</p></li>
</ul>
<p>Symbolic functions should be implemented in Python. All of these functions interact
with Python methods which are implemented via C++-Python bindings,
but intuitively the interface they provide looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">operator</span><span class="o">/</span><span class="n">symbolic</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Modifies Graph (e.g., using &quot;op&quot;), adding the ONNX operations representing</span>
<span class="sd">  this PyTorch function, and returning a Value or tuple of Values specifying the</span>
<span class="sd">  ONNX outputs whose values correspond to the original PyTorch return values</span>
<span class="sd">  of the autograd Function (or None if an output is not supported by ONNX).</span>

<span class="sd">  Arguments:</span>
<span class="sd">    g (Graph): graph to write the ONNX representation into</span>
<span class="sd">    inputs (Value...): list of values representing the variables which contain</span>
<span class="sd">        the inputs for this function</span>
<span class="sd">  &quot;&quot;&quot;</span>

<span class="k">class</span> <span class="nc">Value</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Represents an intermediate tensor value computed in ONNX.&quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Type of the value.&quot;&quot;&quot;</span>

<span class="k">class</span> <span class="nc">Type</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">sizes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a tuple of ints representing the shape of a tensor this describes.&quot;&quot;&quot;</span>

<span class="k">class</span> <span class="nc">Graph</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">opname</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">attrs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create an ONNX operator &#39;opname&#39;, taking &#39;args&#39; as inputs</span>
<span class="sd">    and attributes &#39;kwargs&#39; and add it as a node to the current graph,</span>
<span class="sd">    returning the value representing the single output of this</span>
<span class="sd">    operator (see the `outputs` keyword argument for multi-return</span>
<span class="sd">    nodes).</span>

<span class="sd">    The set of operators and the inputs/attributes they take</span>
<span class="sd">    is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md</span>

<span class="sd">    Arguments:</span>
<span class="sd">        opname (string): The ONNX operator name, e.g., `Abs` or `Add`.</span>
<span class="sd">        args (Value...): The inputs to the operator; usually provided</span>
<span class="sd">            as arguments to the `symbolic` definition.</span>
<span class="sd">        kwargs: The attributes of the ONNX operator, with keys named</span>
<span class="sd">            according to the following convention: `alpha_f` indicates</span>
<span class="sd">            the `alpha` attribute with type `f`.  The valid type specifiers are</span>
<span class="sd">            `f` (float), `i` (int), `s` (string) or `t` (Tensor).  An attribute</span>
<span class="sd">            specified with type float accepts either a single float, or a</span>
<span class="sd">            list of floats (e.g., you would say `dims_i` for a `dims` attribute</span>
<span class="sd">            that takes a list of integers).</span>
<span class="sd">        outputs (int, optional):  The number of outputs this operator returns;</span>
<span class="sd">            by default an operator is assumed to return a single output.</span>
<span class="sd">            If `outputs` is greater than one, this functions returns a tuple</span>
<span class="sd">            of output `Value`, representing each output of the ONNX operator</span>
<span class="sd">            in positional.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>The ONNX graph C++ definition is in <code class="docutils literal notranslate"><span class="pre">torch/csrc/jit/ir.h</span></code>.</p>
<p>Here is an example of handling missing symbolic function for <code class="docutils literal notranslate"><span class="pre">elu</span></code> operator.
We try to export the model and see the error message as below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ne">UserWarning</span><span class="p">:</span> <span class="n">ONNX</span> <span class="n">export</span> <span class="n">failed</span> <span class="n">on</span> <span class="n">elu</span> <span class="n">because</span> <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">symbolic</span><span class="o">.</span><span class="n">elu</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">exist</span>
<span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">ONNX</span> <span class="n">export</span> <span class="n">failed</span><span class="p">:</span> <span class="n">Couldn</span><span class="s1">&#39;t export operator elu</span>
</pre></div>
</div>
<p>The export fails because PyTorch does not support exporting <code class="docutils literal notranslate"><span class="pre">elu</span></code> operator.
We find <code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">Tensor</span> <span class="pre">elu(const</span> <span class="pre">Tensor</span> <span class="pre">&amp;</span> <span class="pre">input,</span> <span class="pre">Scalar</span> <span class="pre">alpha,</span> <span class="pre">bool</span> <span class="pre">inplace)</span> <span class="pre">const</span> <span class="pre">override;</span></code>
in <code class="docutils literal notranslate"><span class="pre">VariableType.h</span></code>. This means <code class="docutils literal notranslate"><span class="pre">elu</span></code> is an ATen operator.
We check the <a class="reference external" href="http://https://github.com/onnx/onnx/blob/master/docs/Operators.md">ONNX operator list</a>,
and confirm that <code class="docutils literal notranslate"><span class="pre">Elu</span></code> is standardized in ONNX.
We add the following lines to <code class="docutils literal notranslate"><span class="pre">symbolic.py</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">g</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="s2">&quot;Elu&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">alpha_f</span><span class="o">=</span><span class="n">_scalar</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
</pre></div>
</div>
<p>Now PyTorch is able to export <code class="docutils literal notranslate"><span class="pre">elu</span></code> operator.</p>
<p>There are more examples in
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic.py">symbolic.py</a>,
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/99037d627da68cdf53d3d0315deceddfadf03bba/torch/autograd/_functions/tensor.py#L24">tensor.py</a>,
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/99037d627da68cdf53d3d0315deceddfadf03bba/torch/nn/_functions/padding.py#L8">padding.py</a>.</p>
<p>The interface for specifying operator definitions is experimental;
adventurous users should note that the APIs will probably
change in a future interface.</p>
</div>
<div class="section" id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.onnx.export">
<code class="descclassname">torch.onnx.</code><code class="descname">export</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.onnx.export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<span id="document-distributed_deprecated"></span><div class="section" id="distributed-communication-package-deprecated-torch-distributed-deprecated">
<h2>Distributed communication package (deprecated) - torch.distributed.deprecated<a class="headerlink" href="#distributed-communication-package-deprecated-torch-distributed-deprecated" title="Permalink to this headline">¶</a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>torch.distributed.deprecated is the older version of torch.distributed and
currently deprecated. It will be removed soon. Please use and refer the doc
for torch.distributed, which is the latest distributed communication
package for PyTorch</p>
</div>
<span class="target" id="module-torch.distributed.deprecated"></span><p>torch.distributed.deprecated provides an MPI-like interface for exchanging tensor
data across multi-machine networks. It supports a few different backends
and initialization methods.</p>
<p>Currently torch.distributed.deprecated supports four backends, each with
different capabilities. The table below shows which functions are available
for use with CPU / CUDA tensors.
MPI supports cuda only if the implementation used to build PyTorch supports it.</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 23%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Backend</p></th>
<th class="head" colspan="2"><p><code class="docutils literal notranslate"><span class="pre">tcp</span></code></p></th>
<th class="head" colspan="2"><p><code class="docutils literal notranslate"><span class="pre">gloo</span></code></p></th>
<th class="head" colspan="2"><p><code class="docutils literal notranslate"><span class="pre">mpi</span></code></p></th>
<th class="head" colspan="2"><p><code class="docutils literal notranslate"><span class="pre">nccl</span></code></p></th>
</tr>
<tr class="row-even"><th class="head"><p>Device</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
<th class="head"><p>CPU</p></th>
<th class="head"><p>GPU</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>send</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
<tr class="row-even"><td><p>recv</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
<tr class="row-odd"><td><p>broadcast</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>all_reduce</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>reduce</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-even"><td><p>all_gather</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
</tr>
<tr class="row-odd"><td><p>gather</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
<tr class="row-even"><td><p>scatter</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
<tr class="row-odd"><td><p>barrier</p></td>
<td><p>✓</p></td>
<td><p>✘</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>✓</p></td>
<td><p>?</p></td>
<td><p>✘</p></td>
<td><p>✘</p></td>
</tr>
</tbody>
</table>
<div class="section" id="basics">
<span id="distributed-deprecated-basics"></span><h3>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h3>
<p>The <cite>torch.distributed.deprecated</cite> package provides PyTorch support and communication primitives
for multiprocess parallelism across several computation nodes running on one or more
machines. The class <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.parallel.deprecated.DistributedDataParallel()</span></code> builds on this
functionality to provide synchronous distributed training as a wrapper around any
PyTorch model. This differs from the kinds of parallelism provided by
<a class="reference internal" href="index.html#document-multiprocessing"><span class="doc">Multiprocessing package - torch.multiprocessing</span></a> and <a class="reference internal" href="index.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.DataParallel()</span></code></a> in that it supports
multiple network-connected machines and in that the user must explicitly launch a separate
copy of the main training script for each process.</p>
<p>In the single-machine synchronous case, <cite>torch.distributed.deprecated</cite> or the
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.parallel.deprecated.DistributedDataParallel()</span></code> wrapper may still have advantages over other
approaches to data-parallelism, including <a class="reference internal" href="index.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.DataParallel()</span></code></a>:</p>
<ul class="simple">
<li><p>Each process maintains its own optimizer and performs a complete optimization step with each
iteration. While this may appear redundant, since the gradients have already been gathered
together and averaged across processes and are thus the same for every process, this means
that no parameter broadcast step is needed, reducing time spent transferring tensors between
nodes.</p></li>
<li><p>Each process contains an independent Python interpreter, eliminating the extra interpreter
overhead and “GIL-thrashing” that comes from driving several execution threads, model
replicas, or GPUs from a single Python process. This is especially important for models that
make heavy use of the Python runtime, including models with recurrent layers or many small
components.</p></li>
</ul>
</div>
<div class="section" id="initialization">
<h3>Initialization<a class="headerlink" href="#initialization" title="Permalink to this headline">¶</a></h3>
<p>The package needs to be initialized using the <a class="reference internal" href="#torch.distributed.deprecated.init_process_group" title="torch.distributed.deprecated.init_process_group"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.deprecated.init_process_group()</span></code></a>
function before calling any other methods. This blocks until all processes have
joined.</p>
<dl class="function">
<dt id="torch.distributed.deprecated.init_process_group">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">init_process_group</code><span class="sig-paren">(</span><em>backend</em>, <em>init_method='env://'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.init_process_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Initializes the distributed package.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backend</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – Name of the backend to use. Depending on build-time configuration
valid values include: <code class="docutils literal notranslate"><span class="pre">tcp</span></code>, <code class="docutils literal notranslate"><span class="pre">mpi</span></code>, <code class="docutils literal notranslate"><span class="pre">gloo</span></code> and <code class="docutils literal notranslate"><span class="pre">nccl</span></code>.</p></li>
<li><p><strong>init_method</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – URL specifying how to initialize the package.</p></li>
<li><p><strong>world_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Number of processes participating in the job.</p></li>
<li><p><strong>rank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Rank of the current process.</p></li>
<li><p><strong>group_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>, </em><em>optional</em>) – Group name. See description of init methods.</p></li>
</ul>
</dd>
</dl>
<p>To enable <code class="docutils literal notranslate"><span class="pre">backend</span> <span class="pre">==</span> <span class="pre">mpi</span></code>, PyTorch needs to built from source on a system that
supports MPI. If you want to use Open MPI with CUDA-aware support, please use
Open MPI major version 2 and above.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method initializes CUDA context. Therefore, if multiple processes
run on a single machine but use different GPUs, make sure to use
<a class="reference internal" href="index.html#torch.cuda.set_device" title="torch.cuda.set_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code></a> before this method to avoid unnecessarily
creating context on the first visible device.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.get_rank">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">get_rank</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.get_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the rank of current process.</p>
<p>Rank is a unique identifier assigned to each process within a distributed
group. They are always consecutive integers ranging from <code class="docutils literal notranslate"><span class="pre">0</span></code> to
<code class="docutils literal notranslate"><span class="pre">world_size</span> <span class="pre">-</span> <span class="pre">1</span></code> (inclusive).</p>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.get_world_size">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">get_world_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.get_world_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of processes in the distributed group.</p>
</dd></dl>

<hr class="docutils" />
<p>Currently three initialization methods are supported:</p>
<div class="section" id="tcp-initialization">
<h4>TCP initialization<a class="headerlink" href="#tcp-initialization" title="Permalink to this headline">¶</a></h4>
<p>There are two ways to initialize using TCP, both requiring a network address
reachable from all processes and a desired <code class="docutils literal notranslate"><span class="pre">world_size</span></code>. The first way
requires specifying an address that belongs to the rank 0 process. This
initialization method requires that all processes have manually specified ranks.</p>
<p>Alternatively, the address has to be a valid IP multicast address, in which case
ranks can be assigned automatically. Multicast initialization also supports
a <code class="docutils literal notranslate"><span class="pre">group_name</span></code> argument, which allows you to use the same address for multiple
jobs, as long as they use different group names.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.distributed.deprecated</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="c1"># Use address of one of the machines</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;tcp://10.1.1.20:23456&#39;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># or a multicast address - rank will be assigned automatically if unspecified</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456&#39;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="shared-file-system-initialization">
<h4>Shared file-system initialization<a class="headerlink" href="#shared-file-system-initialization" title="Permalink to this headline">¶</a></h4>
<p>Another initialization method makes use of a file system that is shared and
visible from all machines in a group, along with a desired <code class="docutils literal notranslate"><span class="pre">world_size</span></code>. The URL should start
with <code class="docutils literal notranslate"><span class="pre">file://</span></code> and contain a path to a non-existent file (in an existing
directory) on a shared file system. This initialization method also supports a
<code class="docutils literal notranslate"><span class="pre">group_name</span></code> argument, which allows you to use the same shared file path for
multiple jobs, as long as they use different group names.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This method assumes that the file system supports locking using <code class="docutils literal notranslate"><span class="pre">fcntl</span></code> - most
local systems and NFS support it.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.distributed.deprecated</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="c1"># Rank will be assigned automatically if unspecified</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;file:///mnt/nfs/sharedfile&#39;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">group_name</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="environment-variable-initialization">
<h4>Environment variable initialization<a class="headerlink" href="#environment-variable-initialization" title="Permalink to this headline">¶</a></h4>
<p>This method will read the configuration from environment variables, allowing
one to fully customize how the information is obtained. The variables to be set
are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code> - required; has to be a free port on machine with rank 0</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> - required (except for rank 0); address of rank 0 node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WORLD_SIZE</span></code> - required; can be set either here, or in a call to init function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RANK</span></code> - required; can be set either here, or in a call to init function</p></li>
</ul>
<p>The machine with rank 0 will be used to set up all connections.</p>
<p>This is the default method, meaning that <code class="docutils literal notranslate"><span class="pre">init_method</span></code> does not have to be specified (or
can be <code class="docutils literal notranslate"><span class="pre">env://</span></code>).</p>
</div>
</div>
<div class="section" id="groups">
<h3>Groups<a class="headerlink" href="#groups" title="Permalink to this headline">¶</a></h3>
<p>By default collectives operate on the default group (also called the world) and
require all processes to enter the distributed function call. However, some workloads can benefit
from more fine-grained communication. This is where distributed groups come
into play. <a class="reference internal" href="#torch.distributed.deprecated.new_group" title="torch.distributed.deprecated.new_group"><code class="xref py py-func docutils literal notranslate"><span class="pre">new_group()</span></code></a> function can be
used to create new groups, with arbitrary subsets of all processes. It returns
an opaque group handle that can be given as a <code class="docutils literal notranslate"><span class="pre">group</span></code> argument to all collectives
(collectives are distributed functions to exchange information in certain well-known programming patterns).</p>
<dl class="function">
<dt id="torch.distributed.deprecated.new_group">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">new_group</code><span class="sig-paren">(</span><em>ranks=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.new_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new distributed group.</p>
<p>This function requires that all processes in the main group (i.e., all
processes that are part of the distributed job) enter this function, even
if they are not going to be members of the group. Additionally, groups
should be created in the same order in all processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ranks</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – List of ranks of group members.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A handle of distributed group that can be given to collective calls.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="point-to-point-communication">
<h3>Point-to-point communication<a class="headerlink" href="#point-to-point-communication" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.distributed.deprecated.send">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">send</code><span class="sig-paren">(</span><em>tensor</em>, <em>dst</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.send" title="Permalink to this definition">¶</a></dt>
<dd><p>Sends a tensor synchronously.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor to send.</p></li>
<li><p><strong>dst</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Destination rank.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.recv">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">recv</code><span class="sig-paren">(</span><em>tensor</em>, <em>src=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.recv" title="Permalink to this definition">¶</a></dt>
<dd><p>Receives a tensor synchronously.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor to fill with received data.</p></li>
<li><p><strong>src</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Source rank. Will receive from any
process if unspecified.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Sender rank.</p>
</dd>
</dl>
</dd></dl>

<p><a class="reference internal" href="#torch.distributed.deprecated.isend" title="torch.distributed.deprecated.isend"><code class="xref py py-func docutils literal notranslate"><span class="pre">isend()</span></code></a> and <a class="reference internal" href="#torch.distributed.deprecated.irecv" title="torch.distributed.deprecated.irecv"><code class="xref py py-func docutils literal notranslate"><span class="pre">irecv()</span></code></a>
return distributed request objects when used. In general, the type of this object is unspecified
as they should never be created manually, but they are guaranteed to support two methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">is_completed()</span></code> - returns True if the operation has finished</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wait()</span></code> - will block the process until the operation is finished.
<code class="docutils literal notranslate"><span class="pre">is_completed()</span></code> is guaranteed to return True once it returns.</p></li>
</ul>
<p>When using the MPI backend, <a class="reference internal" href="#torch.distributed.deprecated.isend" title="torch.distributed.deprecated.isend"><code class="xref py py-func docutils literal notranslate"><span class="pre">isend()</span></code></a> and <a class="reference internal" href="#torch.distributed.deprecated.irecv" title="torch.distributed.deprecated.irecv"><code class="xref py py-func docutils literal notranslate"><span class="pre">irecv()</span></code></a>
support non-overtaking, which has some guarantees on supporting message order. For more detail, see
<a class="reference external" href="http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54">http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54</a></p>
<dl class="function">
<dt id="torch.distributed.deprecated.isend">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">isend</code><span class="sig-paren">(</span><em>tensor</em>, <em>dst</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.isend" title="Permalink to this definition">¶</a></dt>
<dd><p>Sends a tensor asynchronously.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor to send.</p></li>
<li><p><strong>dst</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Destination rank.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A distributed request object.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.irecv">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">irecv</code><span class="sig-paren">(</span><em>tensor</em>, <em>src</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.irecv" title="Permalink to this definition">¶</a></dt>
<dd><p>Receives a tensor asynchronously.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor to fill with received data.</p></li>
<li><p><strong>src</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Source rank.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A distributed request object.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="collective-functions">
<h3>Collective functions<a class="headerlink" href="#collective-functions" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.distributed.deprecated.broadcast">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">broadcast</code><span class="sig-paren">(</span><em>tensor</em>, <em>src</em>, <em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts the tensor to the whole group.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> must have the same number of elements in all processes
participating in the collective.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Data to be sent if <code class="xref py py-attr docutils literal notranslate"><span class="pre">src</span></code> is the rank of
current process, and tensor to be used to save received data
otherwise.</p></li>
<li><p><strong>src</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Source rank.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.all_reduce">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">all_reduce</code><span class="sig-paren">(</span><em>tensor</em>, <em>op=&lt;object object&gt;</em>, <em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.all_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduces the tensor data across all machines in such a way that all get
the final result.</p>
<p>After the call <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> will be bitwise identical in all processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Input and output of the collective. The function
operates in-place.</p></li>
<li><p><strong>op</strong> (<em>optional</em>) – One of the values from <code class="docutils literal notranslate"><span class="pre">torch.distributed.deprecated.reduce_op</span></code>
enum.  Specifies an operation used for element-wise reductions.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.reduce">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">reduce</code><span class="sig-paren">(</span><em>tensor</em>, <em>dst</em>, <em>op=&lt;object object&gt;</em>, <em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduces the tensor data across all machines.</p>
<p>Only the process with rank <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst</span></code> is going to receive the final result.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Input and output of the collective. The function
operates in-place.</p></li>
<li><p><strong>dst</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Destination rank</p></li>
<li><p><strong>op</strong> (<em>optional</em>) – One of the values from <code class="docutils literal notranslate"><span class="pre">torch.distributed.deprecated.reduce_op</span></code>
enum.  Specifies an operation used for element-wise reductions.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.all_gather">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">all_gather</code><span class="sig-paren">(</span><em>tensor_list</em>, <em>tensor</em>, <em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers tensors from the whole group in a list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_list</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – Output list. It should contain
correctly-sized tensors to be used for output of the collective.</p></li>
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Tensor to be broadcast from current process.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.gather">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">gather</code><span class="sig-paren">(</span><em>tensor</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers a list of tensors in a single process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Input tensor.</p></li>
<li><p><strong>dst</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Destination rank. Required in all processes except the one that
is receiveing the data.</p></li>
<li><p><strong>gather_list</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – List of appropriately-sized tensors to
use for received data. Required only in the receiving process.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.scatter">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">scatter</code><span class="sig-paren">(</span><em>tensor</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatters a list of tensors to all processes in a group.</p>
<p>Each process will receive exactly one tensor and store its data in the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Output tensor.</p></li>
<li><p><strong>src</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Source rank. Required in all processes except the one that
is sending the data.</p></li>
<li><p><strong>scatter_list</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – List of tensors to scatter. Required only
in the process that is sending the data.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.barrier">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">barrier</code><span class="sig-paren">(</span><em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.barrier" title="Permalink to this definition">¶</a></dt>
<dd><p>Synchronizes all processes.</p>
<p>This collective blocks processes until the whole group enters this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="multi-gpu-collective-functions">
<h3>Multi-GPU collective functions<a class="headerlink" href="#multi-gpu-collective-functions" title="Permalink to this headline">¶</a></h3>
<p>If you have more than one GPU on each node, when using the NCCL backend,
<a class="reference internal" href="#torch.distributed.deprecated.broadcast_multigpu" title="torch.distributed.deprecated.broadcast_multigpu"><code class="xref py py-func docutils literal notranslate"><span class="pre">broadcast_multigpu()</span></code></a>
<a class="reference internal" href="#torch.distributed.deprecated.all_reduce_multigpu" title="torch.distributed.deprecated.all_reduce_multigpu"><code class="xref py py-func docutils literal notranslate"><span class="pre">all_reduce_multigpu()</span></code></a>
<a class="reference internal" href="#torch.distributed.deprecated.reduce_multigpu" title="torch.distributed.deprecated.reduce_multigpu"><code class="xref py py-func docutils literal notranslate"><span class="pre">reduce_multigpu()</span></code></a> and
<a class="reference internal" href="#torch.distributed.deprecated.all_gather_multigpu" title="torch.distributed.deprecated.all_gather_multigpu"><code class="xref py py-func docutils literal notranslate"><span class="pre">all_gather_multigpu()</span></code></a> support distributed collective
operations among multiple GPUs within each node. These functions can potentially
improve the overall distributed training performance and be easily used by
passing a list of tensors. Each Tensor in the passed tensor list needs
to be on a separate GPU device of the host where the function is called. Note
that the length of the tensor list needs to be identical among all the
distributed processes. Also note that currently the multi-GPU collective
functions are only supported by the NCCL backend.</p>
<p>For example, if the system we use for distributed training has 2 nodes, each
of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would
like to all-reduce. The following code can serve as a reference:</p>
<p>Code running on Node 0</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed.deprecated</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
                        <span class="n">init_method</span><span class="o">=</span><span class="s2">&quot;file:///distributed_test&quot;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">dev_idx</span><span class="p">))</span>

<span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</pre></div>
</div>
<p>Code running on Node 1</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed.deprecated</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
                        <span class="n">init_method</span><span class="o">=</span><span class="s2">&quot;file:///distributed_test&quot;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">dev_idx</span><span class="p">))</span>

<span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>
</pre></div>
</div>
<p>After the call, all 16 tensors on the two nodes will have the all-reduced value
of 16</p>
<dl class="function">
<dt id="torch.distributed.deprecated.broadcast_multigpu">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">broadcast_multigpu</code><span class="sig-paren">(</span><em>tensor_list</em>, <em>src</em>, <em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.broadcast_multigpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts the tensor to the whole group with multiple GPU tensors
per node.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> must have the same number of elements in all the GPUs from
all processes participating in the collective. each tensor in the list must
be on a different GPU.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only NCCL backend is currently supported. <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor_list</span></code> should only
contain GPU tensors.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_list</strong> (<em>List</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – Tensors that participate in the collective
operation. if <code class="docutils literal notranslate"><span class="pre">src</span></code> is the rank, then the first element of
<code class="docutils literal notranslate"><span class="pre">tensor_list</span></code> (<code class="docutils literal notranslate"><span class="pre">tensor_list[0]</span></code>) will be broadcasted to all
other tensors (on different GPUs) in the src process and all tensors
in <code class="docutils literal notranslate"><span class="pre">tensor_list</span></code> of other non-src processes. You also need to make
sure that <code class="docutils literal notranslate"><span class="pre">len(tensor_list)</span></code> is the same for all the distributed
processes calling this function.</p></li>
<li><p><strong>src</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Source rank.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.all_reduce_multigpu">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">all_reduce_multigpu</code><span class="sig-paren">(</span><em>tensor_list</em>, <em>op=&lt;object object&gt;</em>, <em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.all_reduce_multigpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduces the tensor data across all machines in such a way that all get
the final result. This function reduces a number of tensors on every node,
while each tensor resides on a different GPU.
Therefore, the input tensor in the tensor list needs to be GPU tensors.
Also, each tensor in the tensor list needs to reside on a different GPU.</p>
<p>After the call, all tensors in <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor_list</span></code> will be bitwise identical
in all processes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only NCCL backend is currently supported. <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor_list</span></code> should only
contain GPU tensors.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_list</strong> (<em>List</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – List of input and output tensors of
the collective. The function operates in-place and requires that
each tensor to be a GPU tensor on different GPUs.
You also need to make sure that <code class="docutils literal notranslate"><span class="pre">len(tensor_list)</span></code> is the same for
all the distributed processes calling this function.</p></li>
<li><p><strong>op</strong> (<em>optional</em>) – One of the values from <code class="docutils literal notranslate"><span class="pre">torch.distributed.deprecated.reduce_op</span></code>
enum.  Specifies an operation used for element-wise reductions.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.reduce_multigpu">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">reduce_multigpu</code><span class="sig-paren">(</span><em>tensor_list</em>, <em>dst</em>, <em>op=&lt;object object&gt;</em>, <em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.reduce_multigpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduces the tensor data on multiple GPUs across all machines. Each tensor
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor_list</span></code> should reside on a separate GPU.</p>
<p>Only the GPU of <code class="docutils literal notranslate"><span class="pre">tensor_list[0]</span></code> on the process with rank <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst</span></code> is
going to receive the final result.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only NCCL backend is currently supported. <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor_list</span></code> should only
contain GPU tensors.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_list</strong> (<em>List</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – Input and output GPU tensors of the
collective. The function operates in-place.
You also need to make sure that <code class="docutils literal notranslate"><span class="pre">len(tensor_list)</span></code> is the same for
all the distributed processes calling this function.</p></li>
<li><p><strong>dst</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Destination rank</p></li>
<li><p><strong>op</strong> (<em>optional</em>) – One of the values from <code class="docutils literal notranslate"><span class="pre">torch.distributed.deprecated.reduce_op</span></code>
enum.  Specifies an operation used for element-wise reductions.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.distributed.deprecated.all_gather_multigpu">
<code class="descclassname">torch.distributed.deprecated.</code><code class="descname">all_gather_multigpu</code><span class="sig-paren">(</span><em>output_tensor_lists</em>, <em>input_tensor_list</em>, <em>group=&lt;object object&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.distributed.deprecated.all_gather_multigpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers tensors from the whole group in a list.
Each tensor in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input_tensor_list</span></code> should reside on a separate GPU.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only NCCL backend is currently supported. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_tensor_lists</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input_tensor_list</span></code> should only contain GPU tensors.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_tensor_lists</strong> (<em>List</em><em>[</em><em>List</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em><em>]</em>) – Output lists. It should
contain correctly-sized tensors on each GPU to be used for output of
the collective.
e.g. <code class="docutils literal notranslate"><span class="pre">output_tensor_lists[i]</span></code> contains the all_gather
result that resides on the GPU of <code class="docutils literal notranslate"><span class="pre">input_tensor_list[i]</span></code>.
Note that each element of <code class="docutils literal notranslate"><span class="pre">output_tensor_lists[i]</span></code> has the size of
<code class="docutils literal notranslate"><span class="pre">world_size</span> <span class="pre">*</span> <span class="pre">len(input_tensor_list)</span></code>, since the function all
gathers the result from every single GPU in the group. To interpret
each element of <code class="docutils literal notranslate"><span class="pre">output_tensor_list[i]</span></code>, note that
<code class="docutils literal notranslate"><span class="pre">input_tensor_list[j]</span></code> of rank k will be appear in
<code class="docutils literal notranslate"><span class="pre">output_tensor_list[i][rank</span> <span class="pre">*</span> <span class="pre">world_size</span> <span class="pre">+</span> <span class="pre">j]</span></code>
Also note that <code class="docutils literal notranslate"><span class="pre">len(output_tensor_lists)</span></code>, and the size of each
element in <code class="docutils literal notranslate"><span class="pre">output_tensor_lists</span></code> (each element is a list,
therefore <code class="docutils literal notranslate"><span class="pre">len(output_tensor_lists[i])</span></code>) need to be the same
for all the distributed processes calling this function.</p></li>
<li><p><strong>input_tensor_list</strong> (<em>List</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – List of tensors (on different GPUs) to
be broadcast from current process.
Note that <code class="docutils literal notranslate"><span class="pre">len(input_tensor_list)</span></code> needs to be the same for
all the distributed processes calling this function.</p></li>
<li><p><strong>group</strong> (<em>optional</em>) – Group of the collective.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="launch-utility">
<h3>Launch utility<a class="headerlink" href="#launch-utility" title="Permalink to this headline">¶</a></h3>
<p>The <cite>torch.distributed.deprecated</cite> package also provides a launch utility in
<cite>torch.distributed.deprecated.launch</cite>.</p>
<span class="target" id="module-torch.distributed.launch"></span><p><cite>torch.distributed.launch</cite> is a module that spawns up multiple distributed
training processes on each of the training nodes.</p>
<p>The utility can be used for single-node distributed training, in which one or
more processes per node will be spawned. The utility can be used for either
CPU training or GPU training. If the utility is used for GPU training,
each distributed process will be operating on a single GPU. This can achieve
well-improved single-node training performance. It can also be used in
multi-node distributed training, by spawning up multiple processes on each node
for well-improved multi-node distributed training performance as well.
This will especially be benefitial for systems with multiple Infiniband
interfaces that have direct-GPU support, since all of them can be utilized for
aggregated communication bandwidth.</p>
<p>In both cases of single-node distributed training or multi-node distributed
training, this utility will launch the given number of processes per node
(<code class="docutils literal notranslate"><span class="pre">--nproc_per_node</span></code>). If used for GPU training, this number needs to be less
or euqal to the number of GPUs on the current system (<code class="docutils literal notranslate"><span class="pre">nproc_per_node</span></code>),
and each process will be operating on a single GPU from <em>GPU 0 to
GPU (nproc_per_node - 1)</em>.</p>
<p><strong>How to use this module:</strong></p>
<ol class="arabic simple">
<li><p>Single-Node multi-process distributed training</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
<span class="go">           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other</span>
<span class="go">           arguments of your training script)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Multi-Node multi-process distributed training: (e.g. two nodes)</p></li>
</ol>
<p>Node 1: <em>(IP: 192.168.1.1, and has a free port: 1234)</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
<span class="go">           --nnodes=2 --node_rank=0 --master_addr=&quot;192.168.1.1&quot;</span>
<span class="go">           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3</span>
<span class="go">           and all other arguments of your training script)</span>
</pre></div>
</div>
<p>Node 2:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span>
<span class="go">           --nnodes=2 --node_rank=1 --master_addr=&quot;192.168.1.1&quot;</span>
<span class="go">           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3</span>
<span class="go">           and all other arguments of your training script)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>To look up what optional arguments this module offers:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">help</span>
</pre></div>
</div>
<p><strong>Important Notices:</strong></p>
<p>1. This utilty and multi-process distributed (single-node or
multi-node) GPU training currently only achieves the best performance using
the NCCL distributed backend. Thus NCCL backend is the recommended backend to
use for GPU training.</p>
<p>2. In your training program, you must parse the command-line argument:
<code class="docutils literal notranslate"><span class="pre">--local_rank=LOCAL_PROCESS_RANK</span></code>, which will be provided by this module.
If your training program uses GPUs, you should ensure that your code only
runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:</p>
<p>Parsing the local_rank argument</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--local_rank&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</pre></div>
</div>
<p>Set your device to local rank using either</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>  <span class="c1"># before your code runs</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">local_rank</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="c1"># your code to run</span>
</pre></div>
</div>
<p>3. In your training program, you are supposed to call the following function
at the beginning to start the distributed backend. You need to make sure that
the init_method uses <code class="docutils literal notranslate"><span class="pre">env://</span></code>, which is the only supported <code class="docutils literal notranslate"><span class="pre">init_method</span></code>
by this module.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;YOUR BACKEND&#39;</span><span class="p">,</span>
                                     <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>4. In your training program, you can either use regular distributed functions
or use <a class="reference internal" href="index.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code></a> module. If your
training program uses GPUs for training and you would like to use
<a class="reference internal" href="index.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel()</span></code></a> module,
here is how to configure it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                                                  <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">arg</span><span class="o">.</span><span class="n">local_rank</span><span class="p">],</span>
                                                  <span class="n">output_device</span><span class="o">=</span><span class="n">arg</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>
</pre></div>
</div>
<p>Please ensure that <code class="docutils literal notranslate"><span class="pre">device_ids</span></code> argument is set to be the only GPU device id
that your code will be operating on. This is generally the local rank of the
process. In other words, the <code class="docutils literal notranslate"><span class="pre">device_ids</span></code> needs to be <code class="docutils literal notranslate"><span class="pre">[args.local_rank]</span></code>,
and <code class="docutils literal notranslate"><span class="pre">output_device</span></code> needs to be <code class="docutils literal notranslate"><span class="pre">args.local_rank</span></code> in order to use this
utility</p>
<p>5. Another way to pass <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> to the subprocesses via environment variable
<code class="docutils literal notranslate"><span class="pre">LOCAL_RANK</span></code>. This behavior is enabled when you launch the script with
<code class="docutils literal notranslate"><span class="pre">--use_env=True</span></code>. You must adjust the subprocess example above to replace
<code class="docutils literal notranslate"><span class="pre">args.local_rank</span></code> with <code class="docutils literal notranslate"><span class="pre">os.environ['LOCAL_RANK']</span></code>; the launcher
will not pass <code class="docutils literal notranslate"><span class="pre">--local_rank</span></code> when you specify this flag.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">local_rank</span></code> is NOT globally unique: it is only unique per process
on a machine.  Thus, don’t use it to decide if you should, e.g.,
write to a networked filesystem.  See
<a class="reference external" href="https://github.com/pytorch/pytorch/issues/12042">https://github.com/pytorch/pytorch/issues/12042</a> for an example of
how things can go wrong if you don’t do this correctly.</p>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
</ul>
</div>


          </div>
          
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, Torch Contributors.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.0.0b2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>